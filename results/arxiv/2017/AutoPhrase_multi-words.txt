0.9678062294	markov chain
0.9661744878	support vector machines
0.9652613568	gradient descent
0.9642033886	natural language
0.9641045385	artificial intelligence
0.9639066337	genetic programming
0.9636042916	compressed sensing
0.9629544944	principal component analysis
0.9621840742	dimensionality reduction
0.9618176738	sentiment analysis
0.9612188347	point cloud
0.9609473738	logistic regression
0.9604919996	machine translation
0.9594883477	latent variable
0.9585538016	speech recognition
0.9583897649	random forest
0.9583084198	social media
0.9573245012	machine learning
0.9571735676	breast cancer
0.9571423648	facial expression
0.9560375936	anomaly detection
0.9548764622	knowledge base
0.9536668185	optical flow
0.9531973097	support vector machine
0.9526426415	stochastic gradient descent
0.9518777756	maximum likelihood
0.9513619312	total variation
0.9511088542	neural networks
0.9509906942	question answering
0.9467958513	convex optimization
0.9438144303	remote sensing
0.9427409402	big data
0.9425694060	feature extraction
0.9425018603	neural network
0.9423249346	receptive field
0.9413064881	reinforcement learning
0.9405736826	magnetic resonance
0.9401555642	stochastic optimization
0.9400437556	feature selection
0.9385322436	matrix factorization
0.9372278022	reading comprehension
0.9370100894	deep learning
0.9365693211	point clouds
0.9362608676	autonomous driving
0.9360160987	monte carlo
0.9346189130	natural language processing
0.9331477720	automatic speech recognition
0.9328598517	mutual information
0.9326461641	conditional random field
0.9324780385	open source
0.9293391964	markov chain monte carlo
0.9292358262	computed tomography
0.9290161352	active learning
0.9287652948	brain tumor
0.9280938856	knowledge transfer
0.9278454705	compressive sensing
0.9260541213	gesture recognition
0.9258936405	sparse coding
0.9253665083	named entity recognition
0.9252246594	bayesian inference
0.9241483800	eye tracking
0.9232235722	supervised learning
0.9230927375	light field
0.9226838108	statistical machine translation
0.9225425730	random forests
0.9221348427	differential equations
0.9212270380	medical imaging
0.9195532325	skin lesion
0.9192081069	mobile devices
0.9189305871	visual odometry
0.9181457353	decision tree
0.9175604615	genetic algorithm
0.9173960795	source code
0.9173419432	data mining
0.9173413442	dynamical systems
0.9172533096	covariance matrix
0.9171444234	image processing
0.9169064406	linear regression
0.9162218141	reinforcement learning rl
0.9153078804	face detection
0.9152976107	genetic algorithms
0.9150934618	unsupervised learning
0.9147604251	ct scans
0.9147131870	coordinate descent
0.9144395589	gaussian process
0.9137241397	thompson sampling
0.9134837984	local minima
0.9132451557	information extraction
0.9124851443	magnetic resonance imaging
0.9110157461	artificial neural networks
0.9106155067	evolutionary algorithms
0.9105969043	skip connections
0.9105617344	fourier transform
0.9100879022	alzheimers disease
0.9094320421	random variables
0.9087880894	stochastic gradient
0.9073089613	region proposal
0.9067339539	state space
0.9066533847	activity recognition
0.9062148923	weakly supervised
0.9055891936	differential privacy
0.9041940252	word embeddings
0.9038441183	gaussian mixture
0.9035316625	signal processing
0.9027287537	graphical models
0.9027056397	pattern recognition
0.9024622346	nearest neighbors
0.9017858966	language modeling
0.9014811467	pascal voc
0.9002990244	loss function
0.9002309994	image registration
0.8996348532	autonomous vehicles
0.8990516872	style transfer
0.8982052705	adversarial examples
0.8967279042	data augmentation
0.8960842458	pose estimation
0.8958641711	boltzmann machines
0.8957509801	density estimation
0.8952827500	image segmentation
0.8952771160	motion capture
0.8948412576	dependency parsing
0.8947397065	face recognition
0.8944654358	image compression
0.8942967533	gaussian processes
0.8928686105	activation functions
0.8928027259	information retrieval
0.8914252084	transfer learning
0.8913707422	decision trees
0.8906086042	fake news
0.8904790203	path planning
0.8904721445	variational inference
0.8904252180	generative models
0.8897953851	mixture model
0.8896185046	semantic segmentation
0.8895536372	neural machine translation nmt
0.8890815529	neural nets
0.8876486833	sample complexity
0.8875794866	beam search
0.8872172678	emotion recognition
0.8871408984	pos tagging
0.8871152223	conditional random fields
0.8868870962	evaluation metrics
0.8860876782	upper bound
0.8856660477	semantic web
0.8855625814	crowd counting
0.8855542422	object tracking
0.8843523406	ad hoc
0.8842751509	residual network
0.8842115770	convolutional neuralnetworks
0.8839125843	domain adaptation
0.8821237296	deep neural networks
0.8820624411	domain specific
0.8815710234	bounding box
0.8815084385	nearest neighbor
0.8807511562	vector space
0.8799365070	latent variables
0.8794454082	generative adversarial networks gans
0.8792359556	bayesian nonparametric
0.8791429077	image retrieval
0.8787897105	recurrent neural
0.8778422611	image denoising
0.8777487322	risk minimization
0.8776747630	receptive fields
0.8775455354	deep neural networks dnns
0.8775050914	object detectors
0.8774485009	topic modeling
0.8768281198	policy gradient
0.8767076154	sparse representation
0.8765394851	adversarial attacks
0.8761276413	wasserstein distance
0.8760289979	vector machine svm
0.8758777725	head pose
0.8757886654	image enhancement
0.8755914447	batch normalization
0.8754515947	social networks
0.8748859208	optimization problem
0.8748358373	auto encoders
0.8743445347	nuclear norm
0.8741842130	gold standard
0.8736130340	dialogue systems
0.8734027285	face verification
0.8732929842	knowledge graph
0.8727395471	image captioning
0.8725921702	adversarial perturbations
0.8725266303	image restoration
0.8716375836	facial expressions
0.8714703745	object detection
0.8714583210	max pooling
0.8714314811	knowledge bases
0.8709880764	fully convolutional
0.8707837772	facial landmark
0.8701987180	fully connected
0.8701940859	object recognition
0.8700409202	deep neural network
0.8699961582	feature maps
0.8695636752	scene understanding
0.8694932559	attention mechanism
0.8694836990	closed form
0.8692463707	error rate
0.8690889638	convolutional neural networks cnns
0.8690350663	convolutional neural networkcnn
0.8689957127	instance segmentation
0.8682714550	auto encoder
0.8678409956	semantic parsing
0.8677328168	spectral clustering
0.8671024539	video captioning
0.8668700102	lower bound
0.8668047161	information theoretic
0.8664009747	bayesian optimization
0.8663204949	decision making
0.8661713510	quality assessment
0.8661016585	higher order
0.8654632384	black box
0.8649970475	real valued
0.8646060842	f1 score
0.8644678963	generative adversarial
0.8640868118	recurrent neural network rnn
0.8637340170	medical image
0.8634940398	encoder decoder
0.8621067886	medical images
0.8619382023	convolutional neural
0.8615564681	action recognition
0.8612854337	salient object detection
0.8598119435	data set
0.8592258752	multi armed bandit
0.8588409779	gated recurrent
0.8586978775	super resolution
0.8585945145	actor critic
0.8580489179	imitation learning
0.8576626365	ms coco
0.8575377692	depth estimation
0.8572069247	shared task
0.8570463130	low resource
0.8567685615	latent space
0.8565826902	restricted boltzmann
0.8564099355	topic models
0.8562554117	distance metric
0.8560360480	adversarial training
0.8557852962	hand pose estimation
0.8544228886	matrix completion
0.8539465779	knowledge graphs
0.8537656995	false positive
0.8537450064	word embedding
0.8534656370	recurrent neural networks
0.8534274597	human action recognition
0.8528083957	face alignment
0.8520082661	convolutional neural network cnn
0.8515258869	scene text
0.8513011945	low dose
0.8510214690	feature engineering
0.8509569542	convolutional neural network
0.8506157477	cross lingual
0.8499226250	tensor completion
0.8495140587	mini batch
0.8495098971	x ray
0.8491161434	convolutional networks
0.8490589905	cross validation
0.8486931274	expectation maximization
0.8480794867	long short term memory
0.8477904393	e commerce
0.8475274283	probability distribution
0.8469171576	human pose estimation
0.8467605934	outlier detection
0.8466835208	convolutional neural networks
0.8466512971	high resolution
0.8463136803	convolutional neuralnetwork cnn
0.8460261415	visual tracking
0.8451058932	visual recognition
0.8447810577	echo state
0.8446275327	feed forward
0.8436763486	tree search
0.8436590117	class imbalance
0.8435668530	general purpose
0.8432412477	graphical model
0.8428245054	deep networks
0.8425758217	binary classification
0.8425039322	long short term memory lstm
0.8421024540	low rank
0.8417956941	sequence labeling
0.8414697346	variational autoencoder
0.8413500287	variational autoencoders
0.8411370883	natural languageprocessing
0.8409688692	generative adversarial networks
0.8406416360	image classification
0.8403612095	unsupervised domain adaptation
0.8400704848	subspace clustering
0.8397463699	representation learning
0.8380210772	dictionary learning
0.8380196678	spatio temporal
0.8375581982	recurrent neural networks rnns
0.8367802293	long short term
0.8366941049	relation extraction
0.8363973615	saliency maps
0.8362573461	deep rl
0.8358515502	recurrent neuralnetworks
0.8352002974	computational complexity
0.8351933827	saliency detection
0.8348224746	loss functions
0.8343943751	video surveillance
0.8334089651	video frames
0.8332169183	human pose
0.8330078446	reward function
0.8317150426	neural machine translation
0.8312868608	np hard
0.8312656374	fully automatic
0.8309726912	language processing nlp
0.8308346250	recommender systems
0.8306862396	spiking neural networks
0.8304254007	visual question answering
0.8298387088	fine grained
0.8297323959	mr images
0.8297240051	word vectors
0.8272289784	recurrent neural network
0.8272201553	lower bounds
0.8261558405	image reconstruction
0.8257308506	fully connected layers
0.8256267091	convolutionalneural network
0.8253623629	face detector
0.8246622503	cross entropy
0.8241614799	saliency map
0.8236574506	short term memory
0.8230370684	fine tuning
0.8230004552	language models
0.8228447667	semi supervised
0.8222660472	comparative study
0.8221455144	convergence rates
0.8217892938	large scale
0.8217395258	markov decision
0.8213550663	low resolution
0.8211084292	semantic similarity
0.8206867605	convolutional layers
0.8191035427	conditional generative adversarial
0.8183810086	parameter tuning
0.8182958252	k means clustering
0.8178366982	feature space
0.8169959822	residual networks
0.8163579971	multi modal
0.8153202482	deep reinforcement learning
0.8149812961	cross modal
0.8147105131	single shot
0.8145174401	photo realistic
0.8141343913	multitask learning
0.8140117385	batch size
0.8138116327	feature fusion
0.8132556960	deep convolutional neural networks
0.8130003412	generative modeling
0.8128589680	low cost
0.8120538987	convolutionalneural networks
0.8118821793	long term
0.8117730206	lesion segmentation
0.8115101405	low rank matrix
0.8106592533	language understanding
0.8105532015	inverse problems
0.8100613388	short term
0.8099445668	alternating direction method
0.8094080823	pixel wise
0.8088022214	term memory lstm
0.8087825328	single image super resolution
0.8080918261	deep generative models
0.8078729961	support vector
0.8072166424	fully automated
0.8066753852	probability distributions
0.8065646957	multi armed
0.8065210338	fully convolutional networks
0.8063520704	ground truth
0.8057995225	triplet loss
0.8039263730	convergence rate
0.8036704457	neural network architecture
0.8035529866	long range
0.8034215849	hand pose
0.8026765410	component analysis
0.8021141074	multi agent
0.8017804936	character level
0.8011515493	intra class
0.8005598531	post processing
0.8001313130	medical image analysis
0.7996793045	answer set
0.7987886237	multi view
0.7974435046	deep convolutional neural network
0.7962263881	log likelihood
0.7957519751	low dimensional
0.7954125478	generative adversarial network
0.7946842670	depth map
0.7939891091	moving objects
0.7937707429	convolutional network
0.7937029250	context aware
0.7935209453	hyperspectral image
0.7931401726	convolutional neuralnetwork
0.7930680675	computationally efficient
0.7907492461	deep convolutional
0.7905292208	faster r cnn
0.7902942227	label noise
0.7901553560	object localization
0.7901150971	optimization problems
0.7898218980	closely related
0.7892500043	image generation
0.7889658352	human brain
0.7859671249	camera pose
0.7853908702	high dimensional
0.7852187422	person re identification
0.7850223078	n gram
0.7830041888	automatic segmentation
0.7822174800	named entity
0.7818955043	cross domain
0.7816043060	multi class
0.7815789595	metric learning
0.7810733333	multi label
0.7795106129	multi layer
0.7786710822	neural network architectures
0.7775301748	generative model
0.7751759047	shot learning
0.7736694638	mixture models
0.7725899786	hidden layer
0.7725874208	activation function
0.7721016597	random fields
0.7719171776	data streams
0.7712270121	visual attention
0.7709274743	multiple instance
0.7699140037	data driven
0.7698732690	empirical study
0.7687331635	data analysis
0.7687103233	rgb images
0.7683619988	cost function
0.7676009976	fully convolutional neural network
0.7665433683	multi task learning
0.7663388807	adversarial networks
0.7649729271	human action
0.7648446924	object detector
0.7643176727	feature learning
0.7633038913	deep convolutional networks
0.7624285819	image analysis
0.7616869579	pixel level
0.7613124927	generativeadversarial networks
0.7609728881	multi objective
0.7607261972	bounding boxes
0.7593577659	energy efficiency
0.7570401130	zero shot
0.7565842151	semi supervised learning
0.7562522459	high order
0.7556525884	image synthesis
0.7555966077	temporal dynamics
0.7553869122	language processing
0.7548496246	task oriented
0.7548485357	unlabeled data
0.7547408231	labeled data
0.7547199926	multi scale
0.7541422022	online learning
0.7535107897	face images
0.7533838110	meta learning
0.7532207853	deep architectures
0.7528663203	synthetic data
0.7521251655	visual search
0.7519091077	model selection
0.7510527970	multi channel
0.7509279708	deep neuralnetworks
0.7505455892	vector machines
0.7495856118	image super resolution
0.7495208731	video frame
0.7493726971	high quality
0.7493051752	short term memory lstm
0.7479500455	u net
0.7476418357	particle swarm
0.7450928694	hand crafted features
0.7444751039	structured prediction
0.7442699230	rule based
0.7442333083	pre processing
0.7437607857	multi stage
0.7433744258	weakly supervised object
0.7422999239	multi person
0.7418292677	high level
0.7409442098	multi task
0.7408501821	single view
0.7390909138	fully convolutional network
0.7378541977	network architecture
0.7368320222	training data
0.7358124258	search algorithm
0.7355779519	text classification
0.7343114412	worst case
0.7342366815	data sets
0.7336168866	link prediction
0.7324192614	pre trained
0.7323710300	theoretical analysis
0.7322154945	generalization error
0.7300617305	deepneural networks
0.7293055232	human robot
0.7292220473	human body
0.7281477579	deep cnns
0.7278963949	classification accuracy
0.7271709965	convolutional neural networks cnn
0.7269509196	building blocks
0.7266504243	deep neural
0.7257463781	long short
0.7252816217	objective function
0.7251865328	deep features
0.7245884911	prior knowledge
0.7235854686	false positives
0.7217744530	linear models
0.7209786403	fixed point
0.7209459769	latent representations
0.7207976701	language model
0.7207273553	missing data
0.7188101294	adversarial networks gans
0.7187893697	feature vectors
0.7182847713	recent advances
0.7171255746	tomography ct
0.7150554162	human motion
0.7136005143	principal component
0.7130460543	least squares
0.7111919223	hand crafted
0.7111729806	spatial temporal
0.7109452717	fully convolutional neural networks
0.7092465108	recurrent networks
0.7068511518	training samples
0.7066699232	depth maps
0.7059579572	fine tuned
0.7057839891	cnn based
0.7049168472	salient object
0.7048469101	open domain
0.7023815332	action detection
0.7023393439	empirical risk
0.6983666659	neural networks cnns
0.6974027899	armed bandit
0.6969775744	fine tune
0.6960219363	similarity measure
0.6947016857	graph embedding
0.6943316615	trade offs
0.6932053394	remote sensing image
0.6930101206	youtube 8m
0.6919098884	spatial resolution
0.6912896074	computational cost
0.6902052260	back propagation
0.6898833157	deep network
0.6895024853	video sequences
0.6891210573	text generation
0.6883152615	image patches
0.6872750781	high probability
0.6867572677	theoretical guarantees
0.6859478406	local search
0.6846076070	hierarchical structure
0.6825834219	attention based
0.6816284187	case study
0.6809875521	multi camera
0.6805330406	source domain
0.6799087298	pre training
0.6785197086	deep neuralnetwork
0.6781230949	recent progress
0.6770267367	sample size
0.6765936691	natural images
0.6756535310	few shot
0.6746953339	synthetic images
0.6726049376	real life
0.6720957163	video classification
0.6719654972	word representations
0.6706502839	word segmentation
0.6700660520	high accuracy
0.6697632494	ct images
0.6696362408	text detection
0.6678043377	object segmentation
0.6668355664	multi level
0.6663395215	hyper parameters
0.6654098981	significant improvements
0.6639818015	color images
0.6634853810	language generation
0.6624196402	real world
0.6620392648	objective functions
0.6600943833	image recognition
0.6598467638	embedding space
0.6568752511	graph based
0.6556963660	expression recognition
0.6550467490	zero shot learning
0.6543453936	machine learning techniques
0.6526342999	computationally expensive
0.6521521093	visual features
0.6514482707	contextual information
0.6484802531	tumor segmentation
0.6478775399	low level
0.6465946898	convolutional layer
0.6464026252	open problem
0.6461905956	off policy
0.6443882563	vector representations
0.6428536147	frame level
0.6427447079	deep cnn
0.6427399072	domain knowledge
0.6424149641	patch based
0.6419734498	class labels
0.6415999067	bag of words
0.6412735103	empirical evaluation
0.6412075215	error rates
0.6406023875	computer aided
0.6397924655	sensor data
0.6396063650	temporal action
0.6391054006	multi dimensional
0.6390848496	lstm networks
0.6386903384	k means
0.6386254012	neural net
0.6376928560	cnn architectures
0.6362428846	human actions
0.6352365749	image quality
0.6351619479	camera motion
0.6336146234	re id
0.6331114690	prediction accuracy
0.6306377397	training examples
0.6304207735	vision tasks
0.6284432706	high precision
0.6278866025	optimal solution
0.6248248563	vision based
0.6242032163	rgb d
0.6218823926	successfully applied
0.6210436901	performance improvement
0.6209376628	region based
0.6190376614	topic model
0.6176375885	significant improvement
0.6176249691	feature map
0.6164717661	depth images
0.6160413772	non stationary
0.6159252939	test set
0.6155623113	single image
0.6151308730	data collection
0.6141398138	computer vision
0.6133527725	machine learning algorithms
0.6133151858	deep learning models
0.6131665197	phrase based
0.6130896432	character recognition
0.6123081672	computational efficiency
0.6122568213	image fusion
0.6113424369	local feature
0.6103988542	human activity
0.6094544620	significantly improves
0.6082880402	r cnn
0.6061099029	special cases
0.6038104163	class specific
0.6033450799	time series
0.6027149131	deep learning based
0.6026680034	previous works
0.6022642640	great success
0.5998656388	aconvolutional neural network
0.5994086835	self supervised
0.5984616744	hashing methods
0.5981399936	non convex
0.5964612793	paper investigates
0.5963435911	special case
0.5957709029	recent works
0.5934416168	mean squared
0.5928941206	multi label learning
0.5928291525	training set
0.5925827728	key idea
0.5922846436	optimal policy
0.5915780032	network structure
0.5912987551	feature representations
0.5905737936	optimization algorithm
0.5899308700	neural network based
0.5891640089	cnn architecture
0.5891373513	numerical results
0.5885556852	graph structure
0.5885112850	random field
0.5876905135	existing methods
0.5860490893	theproposed algorithm
0.5837282840	neural networkcnn
0.5825690305	significantly outperforms
0.5823524182	benchmark dataset
0.5810961006	nlp tasks
0.5798466213	non rigid
0.5798125533	deep recurrent
0.5796766606	linear convergence
0.5795032645	text recognition
0.5776117768	facial images
0.5774219180	cifar 100
0.5766701141	computer science
0.5750635372	word level
0.5730640517	learning algorithms
0.5725773021	an open source
0.5725538553	conditional generative
0.5691371948	deep learning framework
0.5661322457	feature representation
0.5650172134	numerical experiments
0.5648262957	data points
0.5645866431	real world applications
0.5643008549	model based
0.5635799686	means clustering
0.5635042060	future research
0.5629243329	experimental results
0.5627092928	neural architectures
0.5612523937	markov chain monte
0.5606815358	re identification
0.5596251644	training procedure
0.5592428828	feature vector
0.5582274112	theexperimental results
0.5577417537	reconstruction error
0.5569280204	neural network cnn
0.5550680249	times faster
0.5545648456	related tasks
0.5544684870	kernel methods
0.5535747778	based onthe
0.5535190710	top down
0.5516536077	deep reinforcement
0.5513940746	convolutional features
0.5509204282	residual learning
0.5507603979	gradient based
0.5497371514	large margin
0.5490691073	near optimal
0.5473208321	prior information
0.5460702958	superior performance
0.5446648353	high frequency
0.5431845814	second order
0.5425235899	higher accuracy
0.5406742179	network architectures
0.5401959901	target domain
0.5396011648	bandit problem
0.5392515065	deep learning architecture
0.5378407522	inthis paper
0.5367738605	co occurrence
0.5357889186	image pairs
0.5348298927	taking into account
0.5344946225	end to end trainable
0.5343201019	benchmark datasets
0.5332631860	flow estimation
0.5324408559	term memory
0.5319405956	adversarial nets
0.5316372694	language pairs
0.5304948252	hidden markov
0.5303681349	spiking neural
0.5274603655	high performance
0.5267910788	theoretical results
0.5249220798	object classification
0.5248445890	recent years
0.5248053711	extensive experiments
0.5246883555	multi step
0.5246523184	mean square
0.5238581162	adversarial network
0.5222201184	alternating direction
0.5217895383	squared error
0.5217577444	improved performance
0.5212370957	deep learning techniques
0.5200663965	proposed method outperforms
0.5191619657	significantly improve
0.5175606057	higher level
0.5171873289	temporal information
0.5167678881	machine learning models
0.5149952530	task specific
0.5144013065	kernel learning
0.5136021318	probabilistic model
0.5131947421	unified framework
0.5122853315	theproposed method
0.5122727800	adversarial learning
0.5118202151	direction method of multipliers
0.5101129282	cifar 10
0.5100186828	recently proposed
0.5097706132	model free
0.5085288825	voc 2007
0.5084756522	single image super
0.5083991193	real world scenarios
0.5080667313	vision applications
0.5066865488	non parametric
0.5053016443	experimental results demonstrate
0.5049917191	low power
0.5037516393	proposed method
0.5037205883	deep learning approach
0.5032208293	number of clusters
0.5019047825	promising results
0.4995705366	thatthe proposed
0.4986549528	existing approaches
0.4977959089	learning process
0.4975943446	based approaches
0.4964737166	end to end
0.4962878666	ill posed
0.4955554666	significantly improved
0.4949156312	previous approaches
0.4934276098	test sets
0.4919416952	attention network
0.4905670524	theproposed approach
0.4904467601	face image
0.4899758729	existing works
0.4892244235	neural network rnn
0.4891995328	dimensional space
0.4888590433	one shot
0.4880429489	k nearest
0.4880428158	learned features
0.4875081115	bottom up
0.4871990028	anend to end
0.4855590849	easy to implement
0.4807984156	orders of magnitude
0.4805570789	real world datasets
0.4797801321	f measure
0.4787529304	polynomial time
0.4784722887	search space
0.4755020687	translation nmt
0.4748282181	classification performance
0.4736832399	real time
0.4735352683	generalization performance
0.4730199496	major challenge
0.4722310907	paper describes
0.4721367740	extensive experimental
0.4715320688	input space
0.4714961674	paper proposes
0.4679230093	coarse to fine
0.4676970443	algorithm called
0.4663922922	attention models
0.4652905050	deep q
0.4639910382	image to image translation
0.4625471707	entity recognition
0.4616849561	recently introduced
0.4611463733	world applications
0.4598601943	sequence to sequence
0.4596116800	previous studies
0.4595131187	a unified framework
0.4578103896	previous methods
0.4576478148	paper presents
0.4569916850	classification task
0.4566234423	part of speech
0.4560774487	kernel based
0.4554987701	two stream
0.4549583388	first person
0.4538312050	conditional random
0.4535708026	experiment results
0.4533710434	results suggest
0.4531977057	machine svm
0.4523445844	deep learning methods
0.4507684117	competitive results
0.4486069086	propose anovel
0.4475423543	spatial information
0.4464568954	deep residual
0.4456568023	a case study
0.4453052232	semantic information
0.4450778603	annotated data
0.4446142012	state of art
0.4434746879	side information
0.4403827674	traditional approaches
0.4402630978	level labels
0.4400358842	tasks including
0.4397031952	et al
0.4396781746	large amounts
0.4386008305	empirical results
0.4384179347	semantic space
0.4380203574	connected layers
0.4374568565	traditional methods
0.4370324162	practical applications
0.4357597708	deep generative
0.4356604708	real data
0.4354752214	source and target
0.4352304586	state of theart
0.4344323533	non linear
0.4343468546	large datasets
0.4340376079	learning rate
0.4340042768	classification tasks
0.4338268833	deep architecture
0.4337170193	using deep learning
0.4336049434	based approach
0.4335386755	recent studies
0.4332434765	voc 2012
0.4326035807	spatial and temporal
0.4324273270	results showthat
0.4321497549	two stage
0.4314250595	public datasets
0.4309665597	sequential data
0.4294434739	generated images
0.4283634951	classification problems
0.4257134433	recent research
0.4254322379	q learning
0.4252833354	memory networks
0.4248160705	synthetic and real world
0.4245666744	resonance imaging
0.4231163731	baseline methods
0.4228431026	based methods
0.4216614812	recurrent network
0.4210685528	competitive performance
0.4208358276	image representation
0.4208314008	attention model
0.4190905591	using convolutional neural networks
0.4185557417	artificial neural
0.4170246987	learning algorithm
0.4163109616	time series data
0.4162074692	method outperforms
0.4135454900	detection and tracking
0.4135278794	field of view
0.4122241289	recognition accuracy
0.4091379826	et al 2016
0.4090994650	real world data
0.4061550636	fast and accurate
0.4053860726	image to image
0.4051564899	experimental evaluation
0.4047286710	simulated and real
0.4039475544	training and testing
0.4037294540	input image
0.4023102331	conduct experiments
0.4020742089	non negative
0.4007442167	inference algorithm
0.4007050991	content based
0.4006127482	order of magnitude
0.3995584174	neural network model
0.3989780332	local features
0.3983362256	machine translation nmt
0.3976531963	first order
0.3973167926	method achieves
0.3971381727	noisy data
0.3966782522	point of view
0.3959929866	success of deep
0.3944585675	results obtained
0.3938855647	synthetic and real
0.3935394472	et al 2015
0.3929474637	neural machine
0.3924605776	demonstrate theeffectiveness
0.3911707777	simulated data
0.3908026124	detection methods
0.3906316408	optimization algorithms
0.3905373085	large scale datasets
0.3901169902	trained end to end
0.3893693659	image and video
0.3890455944	applications including
0.3873646042	important step
0.3872475233	deep convolutional neural
0.3850607855	datasets including
0.3849159174	state ofthe
0.3840505151	important task
0.3817822592	experiments on real
0.3817002035	number of samples
0.3808056361	automatic speech
0.3804001344	training of deep
0.3797487864	to end trainable
0.3795413226	a generative model
0.3794386008	reasoning about
0.3790830263	recognition performance
0.3789335253	model parameters
0.3785606274	challenging problem
0.3783779653	video object
0.3782580335	trade off
0.3780688924	proposed technique
0.3777728169	human computer
0.3774676921	processing nlp
0.3764398430	cnn models
0.3755094347	input images
0.3753760066	large number
0.3718521065	three dimensional
0.3697062573	paper introduces
0.3675835124	neural network models
0.3661964380	significant performance
0.3650396294	approach outperforms
0.3641326851	a general framework
0.3636621245	real images
0.3626667560	cnn model
0.3623456483	neural models
0.3608690156	human visual
0.3608429741	challenging task
0.3601524472	sequence to sequence models
0.3601044300	deep feature
0.3594945862	learning based approach
0.3593598443	data sources
0.3554658153	method of multipliers
0.3551520406	high dimensional data
0.3517123955	machine learning methods
0.3498771723	the art methods
0.3497068791	theproposed model
0.3495806278	cnn features
0.3493333240	faster r
0.3484108299	features extracted
0.3476380417	large scale image
0.3467118217	decision process
0.3463385588	existing algorithms
0.3442974215	rnn based
0.3437030522	data collected
0.3428389839	neural networks cnn
0.3416022901	image regions
0.3396025944	the proposed approach
0.3389254532	proposed algorithm
0.3385509432	a unified
0.3383955867	vector machine
0.3380731251	policy learning
0.3375568457	results demonstrate
0.3372324726	statistical machine
0.3357657578	this paper presents
0.3353826861	area under
0.3348798707	learning based
0.3348379687	end to end learning
0.3342939373	non local
0.3329537176	the proposed method
0.3325825293	small number
0.3317042552	end to end training
0.3315349949	an adaptive
0.3309866504	image features
0.3288720290	an efficient
0.3284435356	this paper proposes
0.3273474556	proposed model
0.3266747403	well suited
0.3262220491	experiments demonstrate
0.3262159294	gap between
0.3246789385	real datasets
0.3238420624	an improved
0.3222910795	graph convolutional
0.3207220898	approach achieves
0.3197717612	process models
0.3175960081	proposed approach
0.3161656141	ofthe proposed
0.3148946842	in recent years
0.3119672841	time consuming
0.3102213067	learning rl
0.3072817434	paper aims
0.3068020736	class classification
0.3052879510	person re
0.3036367868	clustering algorithm
0.3017742953	datasets demonstrate
0.3004317530	network learns
0.3002013576	chain monte carlo
0.2989762086	neural model
0.2974427790	data distribution
0.2971783289	achieves state of
0.2951173547	wide variety
0.2941006648	this paper describes
0.2940577566	memory network
0.2938972591	multi object
0.2931528786	relations between
0.2930618306	rank matrix
0.2926319166	current methods
0.2918176483	a wide variety
0.2901877938	non trivial
0.2899694176	this paper introduces
0.2893012178	the art performance
0.2891469438	recent advances in
0.2890482386	gradient method
0.2885162058	the art
0.2884500324	publicly available
0.2882925000	the proposed algorithm
0.2866499142	gradient methods
0.2850347949	two dimensional
0.2849726973	features extracted from
0.2844965872	term dependencies
0.2837314826	a deep neural network
0.2829997021	world datasets
0.2827192074	large amounts of
0.2826906821	analysis pca
0.2818655619	crafted features
0.2813559402	image translation
0.2810362091	an unsupervised
0.2799214429	wide range
0.2794493998	large scale dataset
0.2789230590	experimental results show
0.2769020768	outperforms thestate of
0.2765087140	from scratch
0.2747808478	based method
0.2747585291	large dataset
0.2740854130	neural networks rnns
0.2737632152	not necessarily
0.2734047988	visual question
0.2730858096	optimization methods
0.2728517907	video data
0.2719182552	well established
0.2705268293	image dataset
0.2695494698	computer vision tasks
0.2688703344	an ensemble
0.2687990005	training process
0.2687286606	visual object
0.2659197768	trade off between
0.2652110913	while maintaining
0.2629267941	human performance
0.2621032539	the art approaches
0.2598212742	reason about
0.2592933543	a comprehensive
0.2590322687	learning framework
0.2586892658	3d reconstruction
0.2577948877	structure learning
0.2570043984	3d shapes
0.2565484607	3 d
0.2561367324	an effective
0.2558058772	using deep neural networks
0.2556692378	few years
0.2554201046	second stage
0.2553212582	visual data
0.2550228652	based models
0.2543057547	large number of
0.2539086326	the proposed model
0.2534100625	an end to end
0.2531142768	outperforms state of
0.2523075662	faster than
0.2520114860	speed up
0.2519714150	so called
0.2510979003	a wide range
0.2493938359	paper addresses
0.2489559717	number of parameters
0.2489002001	3d shape
0.2486659155	neural networks dnns
0.2484028113	the proposed framework
0.2480144186	a deep network
0.2471488081	al 2015
0.2471146757	art performance on
0.2470303519	al 2016
0.2463685484	of theart
0.2461070634	the art results
0.2457140163	classification problem
0.2436838337	segmentation methods
0.2435056920	improve performance
0.2434134927	a survey
0.2431638597	experimental results on
0.2428355351	extensive experiments on
0.2426372247	art performance in
0.2423047603	run time
0.2410782054	each iteration
0.2407969943	unsupervised domain
0.2394476710	an open
0.2393924687	a single image
0.2389569683	networks rnns
0.2388082797	this paper addresses
0.2385103959	this paper
0.2378128330	classification models
0.2376073863	previous state of
0.2360427121	into account
0.2357518978	a large scale
0.2356362384	a convolutional neural network
0.2348907831	a large margin
0.2327606747	markov random
0.2314726539	a neural network
0.2304323040	methods in terms
0.2291825534	recognition tasks
0.2288543654	one class
0.2284181104	commonly used
0.2280770956	memory lstm
0.2279514144	recognition systems
0.2275966720	method called
0.2275361678	using deep convolutional
0.2270567826	without requiring
0.2269320705	demonstrate theeffectiveness of
0.2267662297	current state of
0.2265880729	the artperformance
0.2263692354	an interactive
0.2257731702	color image
0.2257689939	an alternative
0.2256211372	well studied
0.2249975286	wide range of
0.2237224159	3d facial
0.2227927441	3d face
0.2220664287	an application
0.2216580954	more importantly
0.2211762942	sequence learning
0.2205728719	state of
0.2202145324	task learning
0.2197391875	2 d
0.2193202077	training dataset
0.2178153992	paper explores
0.2178092994	this paper wepresent
0.2176321959	learning approach
0.2174992085	2d and 3d
0.2173250908	to noise ratio
0.2156419619	more accurate
0.2155819124	regression model
0.2151205348	achieve state of
0.2149687422	widely used
0.2148078975	based on
0.2138917176	the shelf
0.2124239524	very large
0.2113777124	this paperwe
0.2111738298	a challenging task
0.2111478546	efficient algorithm
0.2110668063	image datasets
0.2109343381	during training
0.2105262775	connections between
0.2085001147	network gan
0.2078341884	does not require
0.2077362357	wide variety of
0.2076167276	able to learn
0.2069398230	this paper wepropose
0.2061315070	test data
0.2044495788	as wellas
0.2011874037	detection accuracy
0.2010021238	an online
0.1994422415	a machine learning
0.1992684241	an important problem
0.1989035923	take advantage
0.1985303538	proposed framework
0.1974190710	3d pose
0.1973561858	sequence models
0.1967566751	network rnn
0.1961106840	feature based
0.1948389152	a new
0.1948088231	the internet
0.1945837366	model achieves
0.1933034769	so far
0.1918329772	divided into
0.1913360724	running time
0.1903324999	classification using
0.1895972029	aimed at
0.1894999819	an iterative
0.1892957727	convolution neural
0.1892034267	there exist
0.1889525806	different modalities
0.1885532256	learning methods
0.1880423196	achievesstate of
0.1877640096	training images
0.1863571299	the other hand
0.1863185484	a widerange
0.1846456597	an important role
0.1846145033	inthis work
0.1833805730	a small number
0.1833599280	recognition task
0.1831936867	different scales
0.1831566319	method based on
0.1831230522	instance learning
0.1821043850	image based
0.1820936297	in many cases
0.1818881316	fully convolutional neural
0.1815236015	this problem
0.1801958742	top 1
0.1800667729	by incorporating
0.1797673855	entropy loss
0.1790459583	supervised training
0.1783067061	level features
0.1781238896	region of interest
0.1776399186	art methods
0.1775738627	sequence model
0.1773846194	image representations
0.1763053120	important role
0.1762885049	existing models
0.1757927333	thestate of
0.1750880039	neural language
0.1748073570	over fitting
0.1745759660	prediction models
0.1741797526	prediction model
0.1736072056	model size
0.1735425394	to sequence model
0.1733586949	step towards
0.1729085536	model outperforms
0.1728618078	an overview
0.1726552572	text data
0.1724749526	paper wepresent
0.1724277385	aswell as
0.1721330736	referred to as
0.1720412676	number of
0.1718316528	in addition
0.1717862058	different levels
0.1716022394	a deep neural
0.1712988091	similarity between
0.1707339486	a deep
0.1702713678	important role in
0.1700992103	fed into
0.1696295784	rely on
0.1695192172	small number of
0.1694985662	previous work
0.1694636796	caused by
0.1685861395	recently deep
0.1684739188	a large number
0.1675559750	models trained on
0.1674145389	more efficient
0.1667912105	best performing
0.1658625733	noise ratio
0.1658553140	rather than
0.1651988191	a single
0.1648116866	the original
0.1644703692	even though
0.1642206774	a novel
0.1634000974	series data
0.1630689753	detection performance
0.1628146921	as opposed
0.1621063739	serve as
0.1620474131	a challenging problem
0.1615740542	general framework
0.1613147396	this end
0.1612308611	training deep
0.1605350465	existing state of
0.1604309743	regarded as
0.1598949539	an empirical
0.1594600484	insight into
0.1592413026	the proposed
0.1590520681	value function
0.1587279627	astate of
0.1584471943	focus on
0.1584158579	compared to
0.1580784638	detection algorithms
0.1578856230	deep models
0.1577978002	experiments on two
0.1575859621	learning agents
0.1575718529	of things
0.1572225442	isbased on
0.1568624680	resolution images
0.1565083746	a fast
0.1563927088	both synthetic and real
0.1563534021	art results
0.1562961647	suffers from
0.1559921706	an explicit
0.1553839202	time steps
0.1548907143	the target domain
0.1540055117	the proposed methods
0.1537513872	recent work
0.1536776166	this article
0.1534425991	the art algorithms
0.1534187137	ranging from
0.1529740143	a large
0.1522547552	image level
0.1513853361	time complexity
0.1509671903	thenumber of
0.1504842032	segmentation results
0.1504831113	affected by
0.1500948483	an optimal
0.1496167366	a convolutional neural network cnn
0.1495934823	model called
0.1495256252	information about
0.1493246245	currentstate of
0.1493230494	an important
0.1490069317	the ground truth
0.1486915145	insights into
0.1486909760	evaluation results
0.1486774118	did not
0.1483800776	this issue
0.1483032376	segmentation task
0.1479805051	the art techniques
0.1479273199	by introducing
0.1476229628	a simple
0.1473559211	for multi label
0.1466183564	serves as
0.1463749322	viewed as
0.1461803663	an essential
0.1460105022	small amount of
0.1459067805	relationships between
0.1450604827	at test time
0.1449491308	inspired by
0.1445047967	makes use of
0.1439785869	network based
0.1437252170	an image
0.1436553202	the first stage
0.1433856170	neuralnetwork cnn
0.1427388836	compared with
0.1426461435	to end training
0.1424566455	trained end to
0.1420337810	does not
0.1419899022	challenging due to
0.1417748245	much faster
0.1416505483	label classification
0.1416036962	detection task
0.1414601211	by proposing
0.1408046933	the effectivenessof
0.1405822556	deep recurrent neural
0.1404642418	depending on
0.1401756067	represented by
0.1390166093	performance compared to
0.1388345563	suffer from
0.1384030789	this workwe
0.1380585124	problem in computer
0.1379212972	a new dataset
0.1377286835	the proposed architecture
0.1376775972	prior work
0.1369686059	imaging data
0.1363755772	in thispaper
0.1361410493	aims at
0.1354218809	interpreted as
0.1348360437	interactions between
0.1346556539	each frame
0.1346522011	to sequence models
0.1341302462	to end manner
0.1339456059	results on two
0.1333184326	the input image
0.1332866267	well defined
0.1330167975	applied to
0.1324789767	well known
0.1324366254	derived from
0.1320853315	anumber of
0.1320616080	3d object
0.1319952018	distance between
0.1318138128	extracted from
0.1315192531	a probabilistic
0.1311952410	current state
0.1310853315	avariety of
0.1303474283	segmentation using
0.1302756912	the effects of
0.1299673464	deals with
0.1299296169	an adversarial
0.1298887026	motivated by
0.1296847392	an agent
0.1294878585	focuses on
0.1292628210	propose to use
0.1292122402	inorder to
0.1290189389	the most common
0.1284062885	an automatic
0.1283471402	a review
0.1283453467	capable of
0.1283365055	image super
0.1280214929	a novel approach
0.1279593139	3d hand
0.1279135845	relies on
0.1279128928	recognition system
0.1278309562	relying on
0.1276163669	the proposedalgorithm
0.1275292829	set of
0.1274213190	an active
0.1273084487	trained on
0.1266274553	the proposedmethod
0.1266224589	human like
0.1265809733	generated by
0.1261044402	knowledge about
0.1259287976	algorithm based on
0.1255683904	two step
0.1255652905	based approach to
0.1252810794	connection between
0.1249845390	more accurately
0.1248978978	a generalized
0.1248751864	very high
0.1247346192	trained network
0.1243884257	of up to
0.1243204846	do not
0.1243039743	many real world
0.1240286111	optimization method
0.1235908530	test time
0.1235774236	models trained
0.1234973556	more complex
0.1234482195	based classification
0.1233336449	thecontext of
0.1233276548	aset of
0.1233180299	an automated
0.1231012750	while preserving
0.1230709658	resolution image
0.1228977535	first step
0.1220242837	difference between
0.1218204669	this study
0.1216323310	various types
0.1214973335	formulated as
0.1214590532	in practice
0.1214242991	depend on
0.1214105641	consists of
0.1212740799	a deep convolutional
0.1210067320	3d cnn
0.1209499610	a low dimensional
0.1208854768	such as
0.1208676675	proposed architecture
0.1205662624	dealing with
0.1204614742	produced by
0.1195431930	used to train
0.1191827269	a priori
0.1187029434	show thatthe
0.1184010632	obtained from
0.1178866615	to image translation
0.1178121551	focused on
0.1175818936	correlation between
0.1174096017	show thatour
0.1172585235	smaller than
0.1171230148	model based on
0.1170187461	trained models
0.1169893909	the same
0.1166309673	approach based on
0.1166183602	aim at
0.1164274278	depends on
0.1160644565	much attention
0.1156080997	alternating direction method of
0.1155770453	relationship between
0.1153608677	the use of
0.1152792589	the input data
0.1152480777	recognition using
0.1149887723	at least
0.1149241521	learning tasks
0.1149066358	these issues
0.1148301978	processing tasks
0.1147782955	time step
0.1147575989	the proposed network
0.1147460536	evaluated on
0.1145863783	mapping between
0.1144309697	the training data
0.1144263566	using deep convolutional neural
0.1139968345	emerged as
0.1139468620	at hand
0.1139371922	in thiswork
0.1137955079	the search space
0.1137849585	one hand
0.1137102264	results show
0.1136841300	differences between
0.1132316744	obtained by
0.1131400074	by up to
0.1128646171	approach to
0.1128328318	tasks such as
0.1128307651	on line
0.1128289796	more robust
0.1127801024	method for
0.1127223632	combined with
0.1117801856	version of
0.1117458141	captured by
0.1116528663	relation between
0.1115849525	an extensive
0.1109562979	significantly better
0.1108196325	large amount of
0.1107657184	as well as
0.1106814224	improvement over
0.1106113307	treated as
0.1103290578	each pixel
0.1102930711	the form of
0.1102098823	across multiple
0.1101947410	input data
0.1100950038	this work
0.1100258864	a wide range of
0.1099327942	performance compared
0.1096388361	better than
0.1093428856	three main
0.1085649631	by combining
0.1081967191	fraction of
0.1080655571	more effective
0.1080645772	achieved by
0.1080286078	provided by
0.1078441346	focusing on
0.1074909110	the most important
0.1067269166	a semi supervised
0.1066863743	proposed methods
0.1065812849	learning techniques
0.1063968280	make use of
0.1061704021	each individual
0.1061600759	the art accuracy
0.1059600868	characterized by
0.1059398931	art results on
0.1058247464	variable models
0.1055458630	perform well
0.1054556866	in order to
0.1052667608	by leveraging
0.1050071277	less than
0.1049904893	world scenarios
0.1048398147	some cases
0.1048065997	a widerange of
0.1043245884	2d image
0.1041622036	a lot
0.1040513529	tested on
0.1039788412	the training process
0.1039679906	a comparative
0.1036898326	a form of
0.1036602509	the state of
0.1034346976	effective approach
0.1032625559	the training set
0.1028455893	starting from
0.1027483979	notion of
0.1025629825	in orderto
0.1024135926	two steps
0.1023744557	learned from
0.1023205506	act as
0.1021798987	the art models
0.1021680711	the recognition of
0.1021174682	trained model
0.1020581374	framework for
0.1019791682	each step
0.1018535361	in terms of
0.1018442404	art approaches
0.1017464167	an initial
0.1016680711	the meaning of
0.1016676296	an object
0.1012906729	2d images
0.1012673177	the art results on
0.1010562265	direction method
0.1010041170	resulted in
0.1008557254	distribution over
0.1005771782	detection using
0.1002511905	each layer
0.0998183691	in conjunction with
0.0997558715	more difficult
0.0996937829	considered as
0.0994322253	consist of
0.0994213077	the proposedapproach
0.0992895197	by means of
0.0985417477	an average
0.0984067364	modeled as
0.0983768353	theperformance of
0.0981947937	integrated into
0.0978674896	more efficiently
0.0978280229	performs well
0.0977883784	theeffectiveness of
0.0976692728	also discuss
0.0976061518	future work
0.0975558043	this paper aims
0.0973785373	interested in
0.0973547816	approach for
0.0970549034	learning models
0.0970281925	absence of
0.0969672187	more specifically
0.0969294051	based method for
0.0969248803	consists of two
0.0967317412	with respect to
0.0963701612	by employing
0.0963194783	coming from
0.0962967294	the art performance on
0.0961569333	led to
0.0960978895	learning problems
0.0960494169	a systematic
0.0959834028	benefit from
0.0957846497	drawn from
0.0957576455	our approach
0.0955416186	algorithm for
0.0955232056	induced by
0.0951680711	the challenge of
0.0950907876	the effectiveness of
0.0947718602	art algorithms
0.0946763578	experiments show
0.0945430711	the detection of
0.0944053148	achieves better
0.0943764044	the capability of
0.0942653026	two main
0.0941890223	an extremely
0.0940659205	the benefit of
0.0940111330	further research
0.0939994726	better accuracy
0.0938221747	by adding
0.0937267131	a small set
0.0936375019	outperforms existing
0.0936262441	do so
0.0934820367	results indicate
0.0934341410	portion of
0.0933957076	to end learning
0.0933764044	the degree of
0.0933347377	the contribution of
0.0931985749	a pre trained
0.0931680711	the risk of
0.0927297619	presence of
0.0927196352	different domains
0.0925861434	variety of
0.0925126889	by exploiting
0.0924642254	a simple yet
0.0923866216	very small
0.0922992835	per second
0.0920430711	the core of
0.0919360677	segmentation method
0.0918825658	estimation using
0.0918776578	higher than
0.0918271216	represented as
0.0918190693	more than
0.0917287048	equipped with
0.0915430711	the identification of
0.0915426778	improvements over
0.0909652010	conditioned on
0.0909644381	new class
0.0909465203	not only
0.0909044840	very challenging
0.0905430711	the probability of
0.0904524650	recent deep
0.0904343843	yet effective
0.0901190236	learning approaches
0.0897884678	ability to
0.0895430711	the network to
0.0893764044	the help of
0.0893601321	but also
0.0893383507	very important
0.0892851209	the fly
0.0891306215	over time
0.0890871356	most existing
0.0890430711	the issue of
0.0887742539	the majority of
0.0887451356	problem of learning
0.0886125350	access to
0.0885454921	this purpose
0.0885430711	the analysis of
0.0885430711	the level of
0.0884657999	simple yet
0.0883540596	with respect
0.0883166855	further improve
0.0883130018	used to
0.0881788971	deal with
0.0880430711	the usage of
0.0880112735	measured by
0.0879884115	look at
0.0879405014	followed by
0.0878232504	an extension
0.0877006429	composed of
0.0876814006	the problem of
0.0875728996	an end to
0.0875454921	this gap
0.0875430711	the accuracy of
0.0875430711	the quality of
0.0873543749	the performance of
0.0873089908	methods based on
0.0871726642	various applications
0.0870989869	widely used in
0.0870324952	on par with
0.0869973779	by comparing
0.0869168589	cope with
0.0869032632	performs better
0.0866645072	the impact of
0.0866053150	each class
0.0865865892	this work presents
0.0865456980	superior performance of
0.0864409205	the applicability of
0.0863674250	a variety of
0.0863209352	an additional
0.0862719297	nature of
0.0862549555	determined by
0.0862230830	even if
0.0860825857	suited for
0.0860430711	the importance of
0.0859846730	a large number of
0.0858292113	the objective function
0.0855570846	better performance than
0.0855430711	the evaluation of
0.0855430711	the task of
0.0855430711	the generation of
0.0855430711	the geometry of
0.0855430711	the context of
0.0855430711	the robustness of
0.0854565869	comparable or
0.0854133759	the test set
0.0854118842	in computer vision
0.0851680711	a model for
0.0851484037	suitable for
0.0850430711	the idea of
0.0850430711	the development of
0.0850430711	the concept of
0.0850430711	the success of
0.0850430711	the field of
0.0850430711	the power of
0.0849950133	the same time
0.0847742539	a collection of
0.0846901631	previous state
0.0846601581	applications such as
0.0846081788	an algorithm
0.0845430711	the complexity of
0.0845430711	the efficiency of
0.0843764044	the utility of
0.0843390993	a consequence
0.0841887326	the current state
0.0840509072	single model
0.0840430711	the choice of
0.0840430711	the ability of
0.0840430711	the goal of
0.0840194197	a multi task
0.0840130822	the wild
0.0839553151	small set of
0.0839056527	shown to
0.0837742539	a pair of
0.0837742539	a series of
0.0835430711	the size of
0.0835430711	the training of
0.0835430711	the application of
0.0833764044	the difficulty of
0.0833295121	different types
0.0832358525	the case
0.0830430711	a sequence of
0.0830430711	the influence of
0.0829848689	each other
0.0829398427	by applying
0.0828217962	more general
0.0828134690	conducted on
0.0827616039	advantage of
0.0827343592	an interesting
0.0826075697	dimensional data
0.0825430711	the behavior of
0.0825430711	the distribution of
0.0825430711	the process of
0.0825430711	the design of
0.0825430711	the output of
0.0824867285	art models
0.0824409205	a family of
0.0824409205	the first time
0.0824368154	of freedom
0.0822949373	role in
0.0822785420	to end deep
0.0820430711	the potential of
0.0817742539	a solution to
0.0816699858	good performance
0.0815430711	a method to
0.0815430711	the structure of
0.0813764044	a result of
0.0812090180	availability of
0.0811461413	bag of
0.0810430711	the purpose of
0.0809740895	a formal
0.0807982874	the efficacy of
0.0806443961	the presence of
0.0805430711	the role of
0.0805430711	the value of
0.0805430711	the cost of
0.0803764044	a class of
0.0803058196	lack of
0.0801882821	defined as
0.0801244421	a general
0.0800430711	a dataset of
0.0800430711	the construction of
0.0800430711	the aim of
0.0797742539	in comparison to
0.0797603827	directly from
0.0795430711	a mixture of
0.0795430711	the framework of
0.0793690866	performance of
0.0790858477	interact with
0.0790369658	designed to
0.0785743790	end to
0.0785430711	the study of
0.0785293946	a broad
0.0785114349	scale dataset
0.0783764044	the basis of
0.0783275643	a wide variety of
0.0783078992	novel method
0.0780430711	the potential to
0.0780430711	the need to
0.0778351959	a joint
0.0775430711	the space of
0.0773543749	a set of
0.0772167509	model trained
0.0771037147	small amount
0.0770430711	a function of
0.0769857923	on top of
0.0765887628	solved by
0.0765561860	an unknown
0.0765015599	based image
0.0763764044	of interest in
0.0763764044	an accuracy of
0.0763757257	important problem
0.0760948310	amount of
0.0760387243	by minimizing
0.0759965654	an arbitrary
0.0759857923	two types of
0.0758412607	trained with
0.0757173539	trained end
0.0756799475	the number of
0.0754010671	good results
0.0753090469	tool for
0.0752616110	a convolutional neuralnetwork
0.0746796182	guided by
0.0746199885	a new model
0.0744509952	used to generate
0.0743906408	correlated with
0.0743897873	the problem
0.0743406662	sampled from
0.0743356274	to end
0.0743225224	the feature space
0.0741216585	a novel deep
0.0740545204	learning framework for
0.0737742539	a way to
0.0733359070	more likely
0.0732933190	collected from
0.0730126296	respect to
0.0727742539	so as to
0.0723286312	a principled
0.0720124823	the experimental results
0.0719973780	proven to
0.0717653742	related to
0.0716867331	network trained
0.0716791568	take advantage of
0.0715725987	range of
0.0714947173	the effect of
0.0714108710	scale image
0.0712343478	other state of
0.0709012969	defined by
0.0706565516	associated with
0.0704879737	a hierarchical
0.0703478613	as part of
0.0699665336	method uses
0.0696721662	coupled with
0.0695411431	proposed network
0.0695354571	the problem of learning
0.0692588482	performed on
0.0692177640	lies in
0.0688915721	by integrating
0.0687326090	guaranteed to
0.0686472854	an approximate
0.0686407414	a new approach
0.0682521732	computed from
0.0682389731	a deep convolutional neural
0.0681852271	large amount
0.0678364755	a large set of
0.0677191409	more challenging
0.0676208254	the number
0.0675097661	more and more
0.0672899387	a variety
0.0669508267	regardless of
0.0668121980	large set of
0.0666887646	by taking
0.0664034068	suited to
0.0662364415	the feasibility of
0.0660206034	attempt to
0.0659009916	a small set of
0.0658952301	by utilizing
0.0658593234	scale datasets
0.0656799475	a number of
0.0655237931	not always
0.0655075038	prediction using
0.0651180208	a minimal
0.0649067213	3d point
0.0644833128	a generic
0.0638540951	a small
0.0638317607	world data
0.0634962849	rise to
0.0632769627	the superiority of
0.0632410766	in many applications
0.0631777901	used for
0.0626039866	by analyzing
0.0623358279	a recurrent neural network
0.0623083828	our experimental results
0.0619202964	a large set
0.0618905297	used as
0.0616776512	learning approach to
0.0615606033	on par
0.0613392274	based framework
0.0610905584	a hybrid
0.0610498516	used in
0.0610331630	present results
0.0604512890	as opposed to
0.0597657643	large set
0.0597348405	proposed algorithms
0.0596320643	an important role in
0.0595755149	subset of
0.0593733790	an approach
0.0591502298	in contrast
0.0588457151	the expected
0.0586978494	a lot of
0.0586412209	driven by
0.0584749763	different types of
0.0583963202	variant of
0.0582332375	added to
0.0580464848	learned by
0.0579259659	the presence
0.0579071121	even more
0.0577188835	a given
0.0576098435	taking into
0.0575470916	new dataset
0.0571474239	performed by
0.0570807537	also known as
0.0569701739	a novel method
0.0569293496	account for
0.0568577649	a method for
0.0568309209	involved in
0.0565214821	as well
0.0558620544	problems such as
0.0558495348	the most popular
0.0556785114	better performance
0.0553919690	techniques such as
0.0553356305	an overview of
0.0550297787	together with
0.0549982310	adapted to
0.0547439135	a novel framework
0.0547271419	consistent with
0.0545887342	the need for
0.0544150308	efficient than
0.0541715864	dependent on
0.0540862175	many applications
0.0540826988	an input
0.0540471954	the number of parameters
0.0535236114	much more
0.0534666326	best known
0.0533852091	extended to
0.0529711231	crucial for
0.0528384664	the art on
0.0525041602	a fixed
0.0523693629	the existence of
0.0522542590	amount of data
0.0520162292	a flexible
0.0517983736	the case of
0.0515628823	an extension of
0.0515072939	an appropriate
0.0512915998	in order
0.0512084285	to deal with
0.0510610852	competitive with
0.0510148030	a combination of
0.0508790089	a small number of
0.0507423519	employed to
0.0505823944	emergence of
0.0497919739	by showing
0.0497532503	in contrast to
0.0493843078	the emergence of
0.0483891383	effectiveness of
0.0480034689	other approaches
0.0479127627	most popular
0.0475997133	in addition to
0.0473835533	the current state of
0.0470102812	known as
0.0469924274	needed to
0.0465143241	3d human
0.0464090887	a range of
0.0459952886	our experiments show
0.0458203990	opposed to
0.0458171253	in terms
0.0457569148	a subset of
0.0457016580	generated from
0.0454868145	par with
0.0453090792	framework based on
0.0451057789	existing work
0.0450346604	much better
0.0447076590	to cope with
0.0444531830	by providing
0.0437087869	the arts
0.0431573790	an approach to
0.0431464410	a set
0.0429829868	the ability to
0.0424764547	combination of
0.0421072449	a new algorithm
0.0420607399	the combination of
0.0418439226	described by
0.0413979630	seen as
0.0410390697	an analysis of
0.0409648249	a wide
0.0404100865	introduced by
0.0402332793	widely used for
0.0393086501	a thorough
0.0382688515	written in
0.0378265608	an improvement
0.0370407836	the lack of
0.0370212000	over state of
0.0369621304	usefulness of
0.0369196858	the amount of
0.0367620446	this report
0.0365473807	very deep
0.0362405427	to do so
0.0359184554	to date
0.0359030191	a framework for
0.0355852580	the possibility of
0.0352542320	adopted to
0.0351277227	taken from
0.0351063835	a novel approach for
0.0350535540	the possibility
0.0347941625	proved to
0.0343623810	limited by
0.0342291469	a new method for
0.0341351955	referred to
0.0337941625	utilized to
0.0337695322	the absence of
0.0337251653	visual system
0.0336142760	the relationship between
0.0335061945	the notion of
0.0334211858	our results show
0.0334158257	built on
0.0334005549	done by
0.0332692194	the proposed system
0.0330214571	the well known
0.0326909116	the usefulness of
0.0323673934	the effectiveness
0.0323440149	several state of
0.0322626764	new state of
0.0322019247	a large amount of
0.0319083159	first stage
0.0318055690	a novel approach to
0.0315411271	an ensemble of
0.0314876964	than state of
0.0312730896	in particular
0.0310029806	in comparison with
0.0309389023	a variant of
0.0305421031	far from
0.0303716210	most important
0.0301600478	available at
0.0300724866	an encoder
0.0300145035	other hand
0.0297118129	an example
0.0295908855	a multi
0.0295647093	superiority of
0.0294342330	available datasets
0.0294331470	to account for
0.0291578090	improved by
0.0291071625	a novel deep learning
0.0290500877	the availability of
0.0289534400	the art performance in
0.0286062736	conjunction with
0.0282136706	the distance between
0.0277727581	the gap between
0.0271834612	both synthetic
0.0270855716	a new approach to
0.0268723265	comparison with
0.0263823492	an overall
0.0262434591	while most
0.0262152580	a novel method for
0.0262073854	then used to
0.0261985063	withrespect to
0.0254664804	in conjunction
0.0251133356	an approach for
0.0250548081	off between
0.0248496366	both synthetic and
0.0243295204	existence of
0.0241223582	many real
0.0240074496	the emergence
0.0236074940	a novel method to
0.0235902679	a new method
0.0232612199	a large amount
0.0227253840	same time
0.0226633795	many computer
0.0219409128	novel technique
0.0213217443	a novel algorithm
0.0212574496	the absence
0.0211540349	experiment with
0.0207060972	well as
0.0206840804	an alternative to
0.0203820114	the nature of
0.0193820114	the advantage of
0.0190955269	this problem by
0.0184994797	close to
0.0176773584	the existence
0.0176074940	an algorithm for
0.0169067140	novel method to
0.0160884238	a new framework
0.0154597246	novel approach
0.0142885773	made by
0.0140312756	need for
0.0139757036	a subset
0.0137061821	as compared to
0.0123673105	bound on
0.0122234645	take into
0.0119043579	novel approach to
0.0100480369	novel framework for
