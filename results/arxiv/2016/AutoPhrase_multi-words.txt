0.9753747931	collaborative filtering
0.9700668753	artificial intelligence
0.9700451152	logistic regression
0.9697720095	natural language
0.9697385578	social media
0.9658812545	gradient descent
0.9658627638	speech recognition
0.9646159879	dimensionality reduction
0.9643873174	stochastic gradient descent
0.9638629519	reinforcement learning
0.9627525064	question answering
0.9623740604	knowledge base
0.9618230376	neural network
0.9616415496	monte carlo
0.9604795697	sentiment analysis
0.9602356848	compressed sensing
0.9586615739	big data
0.9579787658	random forest
0.9567565613	optical flow
0.9545269192	convex optimization
0.9543728731	machine translation
0.9534996759	neural networks
0.9531169263	pattern recognition
0.9525003292	feature selection
0.9520163001	support vector machines
0.9516951147	supervised learning
0.9513046233	latent variable
0.9510339499	deep learning
0.9500041096	gaussian process
0.9497474743	anomaly detection
0.9492355394	natural language processing
0.9487174283	machine learning
0.9460703611	feature extraction
0.9446776590	light field
0.9411907812	mutual information
0.9400680398	information extraction
0.9374867684	image processing
0.9355994970	artificial neural networks
0.9349905394	latent dirichlet allocation
0.9282765991	linear regression
0.9270799162	markov chain
0.9226105374	named entity
0.9195477648	matrix factorization
0.9190098702	hilbert space
0.9187468416	graphical models
0.9170326080	loss function
0.9164213441	unsupervised learning
0.9157843655	genetic algorithm
0.9151279144	restricted boltzmann
0.9144618201	word embeddings
0.9128410280	nearest neighbor
0.9118474205	data mining
0.9116653494	active learning
0.9112693472	evolutionary algorithms
0.9110360289	recurrent neural network
0.9109613131	open source
0.9099351891	state space
0.9081708904	weakly supervised
0.9066582677	bayesian inference
0.9056775494	image segmentation
0.9019007992	vector space
0.9006102280	knowledge bases
0.8997978720	deep neural networks
0.8997332588	variational inference
0.8987389581	fully convolutional
0.8985614171	action recognition
0.8965750659	lower bounds
0.8959330644	domain adaptation
0.8923260140	pose estimation
0.8908511353	graphical model
0.8888225657	face detection
0.8884384346	autonomous driving
0.8879802387	residual networks
0.8856384453	gaussian mixture
0.8854095982	upper bound
0.8849438402	matrix completion
0.8847604022	computational complexity
0.8799557153	signal processing
0.8770705822	language modeling
0.8763174364	transfer learning
0.8747161272	prior knowledge
0.8714541627	domain specific
0.8708185641	maximum likelihood
0.8707426014	convolutional neural network
0.8681393042	convolutional neural network cnn
0.8658876908	decision making
0.8651335190	image retrieval
0.8648398247	neural machine translation
0.8640225759	semantic segmentation
0.8637302498	stochastic gradient
0.8629326763	generative models
0.8623119758	recurrent neural networks
0.8608776060	shared task
0.8605049564	data set
0.8603600229	object detection
0.8577562218	image captioning
0.8564916068	short term memory
0.8563881769	object recognition
0.8563565756	high resolution
0.8539416717	attention mechanism
0.8526786760	generative adversarial networks
0.8521791783	convolutional neural networks
0.8520428459	information theoretic
0.8515102336	convolutional neural networks cnns
0.8495759469	markov decision
0.8489363021	word embedding
0.8479009926	relation extraction
0.8446808661	recommender systems
0.8430468663	topic models
0.8429765407	bayesian optimization
0.8406596452	recurrent neural network rnn
0.8396494630	spatio temporal
0.8393844197	encoder decoder
0.8360628354	higher order
0.8346557753	structured prediction
0.8334579068	optimization problems
0.8330844593	answer set
0.8327759857	x ray
0.8326817073	random fields
0.8319383485	generative adversarial
0.8307792537	dictionary learning
0.8305437495	semantic similarity
0.8304647528	deep convolutional
0.8295392051	gaussian processes
0.8281663504	super resolution
0.8274343689	cross validation
0.8266876914	term memory lstm
0.8265754214	image denoising
0.8250166734	black box
0.8249744794	recurrent neural
0.8249568293	long short term memory
0.8238507263	sequence labeling
0.8230953734	mixture model
0.8227915677	subspace clustering
0.8227376441	data augmentation
0.8219123230	missing data
0.8213397053	face recognition
0.8211483197	semi supervised
0.8207309315	deep neural network
0.8205928015	convolutional neuralnetworks
0.8191459909	sparse coding
0.8182582893	deep networks
0.8160146747	lower bound
0.8128287797	low rank
0.8124441473	multi view
0.8116445170	quality assessment
0.8115275955	topic modeling
0.8098555516	word representations
0.8087382005	cross lingual
0.8086541663	sparse representation
0.8060950345	model selection
0.8011906644	adversarial examples
0.8005090816	unified framework
0.7989634268	high dimensional
0.7981360163	link prediction
0.7961929474	object tracking
0.7950988050	optimization problem
0.7949117501	f1 score
0.7936122956	fully connected
0.7933174756	multi scale
0.7932476767	character level
0.7931734997	principal component
0.7919362220	multi modal
0.7911490430	sample complexity
0.7905723751	support vector
0.7892822006	ground truth
0.7877334201	large scale
0.7875623250	multi agent
0.7873235779	data analysis
0.7871245159	probabilistic programming
0.7866688753	image classification
0.7863780799	convolutional networks
0.7834902121	discriminant analysis
0.7823715281	low dimensional
0.7801715535	image analysis
0.7798358553	deep reinforcement learning
0.7791870108	long term
0.7713722141	convolutional neural
0.7675337323	visual question answering
0.7618350768	data sets
0.7600168538	rule based
0.7586215093	multi objective
0.7578982540	deep residual
0.7572785379	sentence level
0.7558997496	domain knowledge
0.7529322782	metric learning
0.7503422061	representation learning
0.7486880597	semi supervised learning
0.7441146755	zero shot
0.7425775999	text classification
0.7399329352	convergence rate
0.7386436567	deep convolutional neural networks
0.7379664799	deep neural
0.7374736871	deep convolutional neural network
0.7349600771	topic model
0.7325510361	fine grained
0.7304412133	feature maps
0.7297421412	convolutional layers
0.7291863643	generative model
0.7281999725	latent variables
0.7240564085	multi class
0.7232562835	multi label
0.7223302839	multi task learning
0.7222723561	natural images
0.7162087654	online learning
0.7160242918	language model
0.7144350163	high quality
0.7140630075	least squares
0.7120264322	n gram
0.7119941062	multi task
0.7111302641	feed forward
0.7110495890	single image
0.7081501758	pre trained
0.7076073862	language models
0.7059463014	attention model
0.7042891879	probabilistic models
0.7032073181	co occurrence
0.7021508613	computational cost
0.7015596874	loss functions
0.7010627920	optimization algorithms
0.6980704712	data driven
0.6965108130	long short term
0.6964875990	feature learning
0.6940671911	convolutional network
0.6922782667	cost function
0.6891509030	unlabeled data
0.6870701306	computationally efficient
0.6858162655	feature engineering
0.6843778881	high level
0.6835211659	feature space
0.6825676543	network architecture
0.6825564247	dirichlet allocation
0.6815924929	vector machine
0.6801084184	sample size
0.6798760324	neural network cnn
0.6796113837	short term
0.6791517367	labeled data
0.6768362315	word level
0.6757245390	graph based
0.6710913793	hand crafted
0.6686101877	back propagation
0.6678122437	classification accuracy
0.6671800272	recent advances
0.6667433529	training data
0.6628745190	label classification
0.6553921768	visual features
0.6518560632	based approach
0.6513550447	k means
0.6474334203	k nearest
0.6468964522	low level
0.6440056149	time series
0.6429517658	np hard
0.6420542996	fine tuning
0.6410410575	training set
0.6409208577	rgb d
0.6407701388	adversarial networks
0.6388118433	cifar 10
0.6382710643	task specific
0.6370404862	contextual information
0.6361006532	image quality
0.6347848169	phrase based
0.6297218302	real life
0.6279244962	machine learning models
0.6261832565	training samples
0.6243204508	real world
0.6233665177	test set
0.6041195729	error rate
0.6035249553	classification task
0.6005849879	general purpose
0.5979448166	search space
0.5947391993	objective function
0.5926656937	attention based
0.5910651447	higher level
0.5872696830	component analysis
0.5846874141	synthetic data
0.5845255270	content based
0.5840112008	term memory
0.5802343095	existing methods
0.5800408544	learning algorithms
0.5786888070	spiking neural
0.5779125932	computer vision
0.5770069158	neural network architecture
0.5674434535	target domain
0.5668586479	human pose
0.5655699167	data points
0.5654548492	deep convolutional neural
0.5595623489	long short
0.5577187547	non negative
0.5567724344	high dimensional data
0.5520789553	input image
0.5516629636	deep network
0.5501090864	order of magnitude
0.5446790829	latent dirichlet
0.5416559084	non parametric
0.5392309675	classification problems
0.5278232213	vector machines
0.5272470479	random field
0.5266731739	linear models
0.5216566072	inthis paper
0.5215459862	orders of magnitude
0.5205517133	second order
0.5200065506	one shot
0.5177526065	neural machine
0.5176517654	special case
0.5125383891	non convex
0.5120520549	proposed method
0.5119108346	visual question
0.5098216077	artificial neural
0.5092169521	end to end
0.5089392654	machine learning algorithms
0.5064823440	deep models
0.5059568003	sequence to sequence
0.5036904852	statistical machine
0.5033243865	language processing
0.5027348988	theproposed method
0.5001552789	high accuracy
0.4986682240	neural networks cnns
0.4964359373	bottom up
0.4959377455	deep generative
0.4921012164	extensive experiments
0.4891762868	neural network based
0.4843675120	a case study
0.4790203890	real time
0.4785634063	et al
0.4772893577	time series data
0.4760608469	et al 2015
0.4748632372	hidden markov
0.4729685726	markov model
0.4689935379	first person
0.4678009684	memory lstm
0.4668460310	trained end to end
0.4652891595	experimental results
0.4640334228	classification tasks
0.4627162876	based approaches
0.4610309832	synthetic and real
0.4601670816	side information
0.4537166692	image based
0.4517302769	trade off
0.4478005147	real world data
0.4463906829	entity recognition
0.4452080875	previous works
0.4423161930	learning algorithm
0.4415610781	method outperforms
0.4328712633	non linear
0.4268340735	existing approaches
0.4254424649	neural models
0.4246334957	input data
0.4225776648	recognition tasks
0.4214316699	benchmark datasets
0.4206200303	polynomial time
0.4183919709	significant improvement
0.4164573137	networks cnns
0.4157075923	top k
0.4156993315	non trivial
0.4143970086	significantly outperforms
0.4135307802	experiments demonstrate
0.4098088247	real world datasets
0.4059216501	significantly improves
0.4049656537	neural networks cnn
0.4031112310	real datasets
0.4023841817	superior performance
0.3980566407	theoretical results
0.3971350557	method achieves
0.3958580418	challenging task
0.3943812764	based methods
0.3934153687	number of parameters
0.3908893511	crafted features
0.3871821046	cnn based
0.3853156737	part of speech
0.3830782056	challenging problem
0.3821950896	numerical experiments
0.3807132246	empirical results
0.3800372141	a wide range
0.3784379606	experimental evaluation
0.3761613781	conditional random
0.3761276115	recent years
0.3721264479	recently proposed
0.3703809329	learning framework
0.3691865903	model based
0.3690751324	promising results
0.3688645241	a large scale
0.3686197871	case study
0.3677536618	recognition performance
0.3673337317	vision tasks
0.3608481229	time consuming
0.3602629045	one class
0.3581118980	wide variety
0.3547800539	network cnn
0.3528783745	model parameters
0.3488508615	a general framework
0.3479482569	an important role
0.3453508287	a small number
0.3434220241	well suited
0.3415186012	a unified
0.3411056222	results suggest
0.3405188869	paper presents
0.3345305679	networks cnn
0.3324415034	results demonstrate
0.3303499937	real data
0.3233653458	learning problems
0.3194982743	the proposed method
0.3194445829	paper proposes
0.3188486189	the art methods
0.3169330533	approach achieves
0.3166858368	publicly available
0.3163583125	deep reinforcement
0.3125918078	probabilistic model
0.3096964688	network based
0.3079540758	the proposed approach
0.3069339880	very deep
0.3059448969	this paper presents
0.3026151777	a wide variety
0.3010090062	paper introduces
0.3002712959	in recent years
0.2988479922	task learning
0.2966229200	network models
0.2958506383	approach outperforms
0.2923009455	learning based
0.2912650422	current state of
0.2896943432	this paper proposes
0.2894051112	wide range of
0.2868440828	paper describes
0.2842277714	an ensemble
0.2841939665	large amounts
0.2816874853	first order
0.2770075015	achieves state of
0.2762164807	learning problem
0.2751603016	the art
0.2734934160	based method
0.2732348737	large amounts of
0.2728687018	this paper
0.2666716059	test data
0.2655803020	proposed approach
0.2636021152	a comprehensive
0.2635643959	learning tasks
0.2610710422	a survey
0.2609883456	does not require
0.2609520240	al 2015
0.2601519098	large number of
0.2599890801	learning approach
0.2596435978	an empirical
0.2569499021	an application
0.2566229010	this paper describes
0.2564164386	an end to end
0.2536555425	so called
0.2526826827	achieve state of
0.2521146804	paper addresses
0.2509900847	learning method
0.2493134350	experimental results show
0.2488307115	learning methods
0.2484861104	relationships between
0.2477892815	the art approaches
0.2416992280	recent advances in
0.2413770846	a single image
0.2409741360	into account
0.2400867640	an efficient
0.2384878347	the art results
0.2374063709	so far
0.2353403942	an adaptive
0.2327918897	a deep neural network
0.2298150691	dimensional data
0.2276182158	the proposed algorithm
0.2264783027	small number
0.2259455343	the shelf
0.2254921036	as opposed
0.2249676767	neural network rnn
0.2227454104	the art performance
0.2201908828	commonly used
0.2200528724	this paperwe
0.2182136285	the art algorithms
0.2159437242	this paper introduces
0.2159102282	general framework
0.2156176832	important role
0.2152351962	widely used
0.2146418655	rather than
0.2138669030	outperforms state of
0.2120280973	based on
0.2118025799	proposed framework
0.2115150404	current state
0.2107560187	large amount
0.2100323348	thestate of
0.2099563284	each iteration
0.2096096217	gap between
0.2093371285	the proposed framework
0.2076960182	faster than
0.2073907669	depending on
0.2054516371	a convolutional neural network
0.2047442062	world applications
0.2038914854	trained end to
0.2027280996	3d object
0.2022429668	a low dimensional
0.2011870920	state of
0.2003735802	information about
0.1985889343	a priori
0.1985816817	the proposed model
0.1981505305	a deep learning
0.1980296447	a novel approach
0.1965922131	rank matrix
0.1956991918	at least
0.1938062708	previous work
0.1923181727	during training
0.1915704776	learning models
0.1905480221	well known
0.1898856486	proposed algorithm
0.1875211968	experimental results on
0.1861510423	does not
0.1857959965	extensive experiments on
0.1852129488	relationship between
0.1828273500	the objective function
0.1823021417	a large number
0.1821626151	aswell as
0.1803828050	a novel
0.1756203814	the training set
0.1737880449	in addition
0.1730898187	followed by
0.1727873918	this article
0.1722607377	the other hand
0.1704162482	propose to use
0.1700383354	test time
0.1689402374	number of
0.1688961334	recognition using
0.1677151466	an iterative
0.1666518286	distance between
0.1664238416	model achieves
0.1663130966	an important
0.1644125427	this paper addresses
0.1636768794	this paper wepropose
0.1632816222	problem of learning
0.1614331401	an agent
0.1613873346	world datasets
0.1612129415	the proposedmethod
0.1603353206	more accurate
0.1594712406	small number of
0.1590692224	the effects of
0.1587457393	do not
0.1570894249	dealing with
0.1560964330	network rnn
0.1560290446	series data
0.1554390943	method based on
0.1547559131	learning approaches
0.1546955529	algorithm based on
0.1544566615	a simple
0.1544238247	a new approach
0.1535726691	depend on
0.1530748406	a neural network
0.1524986808	art algorithms
0.1515659909	focus on
0.1513753176	rely on
0.1510306871	based model
0.1508026424	depends on
0.1485750001	focuses on
0.1481237472	recent work
0.1470646648	art results on
0.1470064504	viewed as
0.1466382609	the robustness of
0.1459088990	drawn from
0.1456283981	first step
0.1453073096	serve as
0.1451202713	an alternative
0.1440953070	proposed model
0.1437901753	a single
0.1426853960	relies on
0.1419162319	the original
0.1410450160	this problem
0.1401801269	based image
0.1396382609	the impact of
0.1391779376	these issues
0.1390792305	an order of
0.1388110793	network model
0.1385965942	the help of
0.1382820833	bag of
0.1366563303	characterized by
0.1355682428	this work
0.1354659603	large number
0.1350892413	the cost of
0.1345965942	the behavior of
0.1340892413	the design of
0.1336540171	tend to
0.1336382609	the success of
0.1335243259	a novel method
0.1323081847	inorder to
0.1320140181	inspired by
0.1318016789	knowledge about
0.1306382609	the choice of
0.1303703763	make use of
0.1299715942	the efficiency of
0.1297892019	by means of
0.1297339945	theeffectiveness of
0.1294155864	thenumber of
0.1293239787	art performance
0.1292016738	more than
0.1290424187	training time
0.1288836041	an unsupervised
0.1286382609	the purpose of
0.1284931658	on top of
0.1280892413	the space of
0.1275965942	the concept of
0.1275603616	an average
0.1275071580	a new method
0.1274208035	such as
0.1272725775	as well as
0.1267490105	with respect to
0.1263047166	art methods
0.1261426724	the proposed
0.1260892413	the application of
0.1257550449	capable of
0.1254707425	paper wepropose
0.1247673331	world data
0.1246382609	the importance of
0.1245829404	novel framework
0.1243313048	the use of
0.1241186530	the accuracy of
0.1241186530	the case of
0.1240892413	the complexity of
0.1240892413	a method to
0.1239385675	ranging from
0.1236849758	this end
0.1234133696	differences between
0.1231186530	the field of
0.1231186530	the quality of
0.1231186530	the task of
0.1230892413	the process of
0.1230892413	the power of
0.1222759921	a new
0.1221186530	the form of
0.1221186530	the size of
0.1220892413	the output of
0.1220892413	the analysis of
0.1217394766	to end
0.1213151232	more efficient
0.1210892413	the structure of
0.1201186530	the context of
0.1201101868	the same
0.1200892413	a dataset of
0.1193631310	a real world
0.1187948947	a hierarchical
0.1181848574	an algorithm
0.1168430341	a systematic
0.1161621627	a series of
0.1160892413	the ability of
0.1154987845	the art performance on
0.1150892413	a class of
0.1150892413	a sequence of
0.1147370666	in computer vision
0.1144007339	consists of
0.1142139945	represented by
0.1139491107	in practice
0.1130512062	in order to
0.1130024304	art performance on
0.1124437197	the training data
0.1121186530	the goal of
0.1114323996	proposed methods
0.1112920500	art approaches
0.1109309967	an image
0.1103069691	derived from
0.1101592302	emergence of
0.1100328198	suffer from
0.1094478270	the entire
0.1092798097	the need for
0.1084492988	theperformance of
0.1082498380	extracted from
0.1078060449	advantage of
0.1062786596	a large
0.1062473378	more robust
0.1060599602	the possibility
0.1060512062	in terms of
0.1054768123	set of
0.1039802093	the state of
0.1038698278	learning techniques
0.1037744606	fraction of
0.1035740819	better than
0.1035145173	family of
0.1034744555	overview of
0.1032950213	an end to
0.1027162389	the effectiveness of
0.1019061004	in thispaper
0.1003365278	not only
0.0999071112	more complex
0.0998753935	less than
0.0998260898	captured by
0.0997052155	the latter
0.0995230765	the superiority
0.0992187197	existence of
0.0989724077	a broad
0.0978033784	applications such as
0.0968622019	tasks such as
0.0966724982	prior work
0.0962132113	by introducing
0.0961575455	this task
0.0955421521	the art results on
0.0951716851	kind of
0.0949068074	a variety of
0.0939186056	availability of
0.0935179437	version of
0.0934358670	an approach
0.0932178051	most existing
0.0928930288	many applications
0.0924575455	this approach
0.0924557888	aims at
0.0923903605	type of
0.0922493108	even if
0.0908179566	relying on
0.0897787794	the final
0.0891422428	access to
0.0885997757	method for
0.0884043225	degree of
0.0883009188	this study
0.0881569287	results indicate
0.0877161915	this issue
0.0875627014	generated by
0.0875082529	represented as
0.0870505782	better performance
0.0868385217	art results
0.0865960151	correspond to
0.0865380561	a deep neural
0.0864766104	focused on
0.0862768121	but also
0.0859047508	approach to
0.0857036501	a fast
0.0854575455	our approach
0.0852534866	notion of
0.0841120903	widely used in
0.0840913255	algorithm for
0.0839123724	a convolutional neural
0.0836572694	the first time
0.0835717655	the main
0.0832015122	compared with
0.0829300883	impact on
0.0828709316	the wild
0.0826809169	trained on
0.0826196928	a deep
0.0825805135	combined with
0.0825789110	variety of
0.0824123724	the experimental results
0.0823864758	the role of
0.0823730262	an automatic
0.0819308707	provided by
0.0817767900	other state of
0.0813867897	motivated by
0.0809444556	experiments show
0.0808492439	obtained by
0.0805322310	framework for
0.0799396179	information from
0.0798445918	results show
0.0795031050	the problem of
0.0795031050	a set of
0.0793092447	different types of
0.0793005878	a probabilistic
0.0786568294	associated with
0.0780391133	in thiswork
0.0779044828	absence of
0.0776145431	attempt to
0.0775763086	a principled
0.0775725112	at hand
0.0775031050	the performance of
0.0768106805	the utility
0.0765423479	a wide range of
0.0764762144	suitable for
0.0762994378	improvement over
0.0762793102	by applying
0.0759395911	refer to
0.0757394899	role in
0.0754025411	composed of
0.0744693705	the effect of
0.0742821079	learned from
0.0740825315	a fixed
0.0740274734	a joint
0.0739516357	the literature
0.0736011008	a fundamental
0.0734074741	an open
0.0723769401	achieved by
0.0720449457	by incorporating
0.0718256619	collected from
0.0713470089	performs better
0.0706280300	over time
0.0706242663	by utilizing
0.0705621147	ability to
0.0705360655	an effective
0.0704849766	the same time
0.0704356832	obtained from
0.0702021548	a variety
0.0701521939	a lot
0.0700881732	an online
0.0697094613	a lot of
0.0696649034	by exploiting
0.0695254055	by adding
0.0692375857	the problem
0.0685687604	amount of
0.0681956386	considered as
0.0680650026	a general
0.0672432321	the former
0.0666660272	a strong
0.0666425060	progress in
0.0662586015	defined by
0.0658397107	performed on
0.0656561466	produced by
0.0652374645	a powerful
0.0647751941	a large number of
0.0647751800	the number of
0.0645707978	a major
0.0640127878	by combining
0.0638829697	tool for
0.0621081075	the efficacy of
0.0616360610	lack of
0.0613848706	part of
0.0610587914	an example
0.0609403006	deal with
0.0609111734	evaluated on
0.0608230572	a set
0.0607733657	an extensive
0.0604444234	a small
0.0603376490	tested on
0.0591324898	the presence of
0.0591240718	the existence of
0.0590816790	a subset of
0.0590744948	conducted on
0.0589453994	much more
0.0587176280	each other
0.0586632392	combination of
0.0584223880	an optimal
0.0576944519	a number of
0.0571994115	generated from
0.0570842968	an extension of
0.0566678545	new state of
0.0566379308	to date
0.0564429783	in contrast
0.0563081398	lot of
0.0547656322	the art on
0.0544450471	directly from
0.0542761341	benefit from
0.0539211205	lead to
0.0532563773	competitive with
0.0529521193	subset of
0.0521303756	fail to
0.0521206293	learned by
0.0520037635	a state of
0.0513943431	respect to
0.0513265051	comparison with
0.0511674256	the ability to
0.0511496684	our results show
0.0506012491	a method for
0.0505624398	aim to
0.0502663443	range of
0.0502260874	suited for
0.0494705697	performance of
0.0493505219	with respect
0.0487102804	a combination of
0.0486873519	to deal with
0.0486311670	the development of
0.0486187578	an extension
0.0475787396	the notion of
0.0473890557	try to
0.0458454864	end to
0.0457839881	crucial for
0.0457711957	different types
0.0447191077	a framework for
0.0444451557	the lack of
0.0443931078	a compact
0.0440680818	an approach to
0.0436392517	in terms
0.0436229791	in addition to
0.0429574575	essential for
0.0425624601	in contrast to
0.0424076594	a range of
0.0423687848	then used
0.0422124367	presence of
0.0420452001	the number
0.0419057327	known as
0.0415868721	both synthetic and
0.0412791606	as well
0.0404912576	in order
0.0404243984	the set of
0.0404098369	development of
0.0396458494	a generic
0.0392955380	both synthetic
0.0390809838	efficacy of
0.0388631523	used as
0.0373643096	a novel approach for
0.0367908423	the amount of
0.0367369636	a given
0.0362033008	together with
0.0355126432	used for
0.0348352605	a semi
0.0345457465	not require
0.0344401645	bound on
0.0340980047	need to
0.0317973376	effectiveness of
0.0303263406	an input
0.0301080886	the effectiveness
0.0298520171	the existence
0.0289907103	other hand
0.0279913718	the presence
0.0276809843	in particular
0.0259090118	many real
0.0256621510	new method
0.0254414568	the efficacy
0.0245268291	our main
0.0220580412	a wide
0.0211429089	well as
0.0205774829	a hybrid
0.0201548354	a special
0.0196795155	useful for
0.0196003664	converge to
0.0193398495	superiority of
0.0186003664	close to
0.0142353680	same time
0.0142107083	novel approach to
0.0140401840	novel approach
0.0121203596	a subset
0.0112062905	an end
0.0103380494	novel method for
