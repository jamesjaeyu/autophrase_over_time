0.9729069174	markov chain
0.9707489447	compressed sensing
0.9669212027	knowledge base
0.9663519049	collaborative filtering
0.9639748726	dimensionality reduction
0.9636940033	random forest
0.9600528952	belief propagation
0.9598956861	principal component analysis
0.9590720365	genetic programming
0.9585126202	dynamic programming
0.9584873020	resource allocation
0.9583305677	logistic regression
0.9581323660	simulated annealing
0.9578011369	maximum likelihood
0.9572926348	compressive sensing
0.9554205078	artificial intelligence
0.9548954469	gibbs sampling
0.9542362607	optical flow
0.9540612192	total variation
0.9532666069	semidefinite programming
0.9530776812	bayesian inference
0.9530607597	exponential family
0.9528314502	social media
0.9522040482	hidden markov models
0.9517772499	logic programming
0.9508059050	support vector machine
0.9502973030	random walk
0.9501938392	stochastic gradient descent
0.9500997349	kalman filter
0.9500134739	stochastic optimization
0.9499865152	genetic algorithm
0.9497643005	particle swarm optimization
0.9497189218	constraint satisfaction
0.9497005966	decision tree
0.9493925641	virtual reality
0.9492535714	nearest neighbors
0.9490913037	differential equations
0.9490457844	partially observable
0.9489532376	autonomous driving
0.9484098897	mutual information
0.9482947160	magnetic resonance imaging mri
0.9482368510	question answering
0.9481569632	fuzzy logic
0.9478555081	white matter
0.9478514983	markov chains
0.9476216431	big data
0.9475720937	random forests
0.9475022375	point cloud
0.9474784807	anomaly detection
0.9469638041	turing test
0.9468708645	matrix factorization
0.9467888518	augmented reality
0.9467034146	coordinate descent
0.9466431226	machine translation
0.9465117358	differential privacy
0.9459636747	feature extraction
0.9458829955	gene expression
0.9456368537	convex optimization
0.9455997294	breast cancer
0.9455041214	speech recognition
0.9450064594	lung cancer
0.9445713029	gradient descent
0.9445183187	optical coherence tomography
0.9443352552	support vector machines
0.9441708741	swarm intelligence
0.9440405291	genetic algorithms
0.9440174595	graphical models
0.9440001890	conditional independence
0.9438693175	canonical correlation analysis
0.9437443525	importance sampling
0.9437040691	reading comprehension
0.9433809054	distant supervision
0.9431496888	gaussian process
0.9430692391	knowledge bases
0.9428028223	point clouds
0.9427097270	stock market
0.9422353269	data mining
0.9422296230	motion capture
0.9420782361	influence diagrams
0.9417962144	dynamical systems
0.9416988866	contextual bandit
0.9416970698	word sense disambiguation
0.9410166955	autonomous vehicles
0.9410123160	covariance matrix
0.9410056310	evolutionary algorithms
0.9409800520	nash equilibrium
0.9407988879	mobile devices
0.9407880004	euclidean distance
0.9406730897	dependency parsing
0.9406645761	particle filter
0.9406472798	dimension reduction
0.9403014630	maximum entropy
0.9401188379	integer programming
0.9400377259	message passing
0.9398096043	mahalanobis distance
0.9393920204	optical character recognition
0.9389137477	restricted boltzmann machines
0.9387168677	radial basis function
0.9384821253	information retrieval
0.9384796559	random walks
0.9383423254	shortest path
0.9383302286	local search
0.9382324762	naive bayes
0.9381001211	natural language processing
0.9380440706	reproducing kernel hilbert
0.9378877437	loopy belief propagation
0.9377156098	phase transition
0.9375982861	gesture recognition
0.9375835579	intrusion detection
0.9375430870	convex relaxations
0.9373775704	upper bound
0.9372320467	recurrent neural network
0.9371977168	reinforcement learning
0.9370469740	decision trees
0.9369681008	batch normalization
0.9368289223	fourier transform
0.9368277583	computed tomography ct
0.9366885044	stochastic gradient
0.9365820572	information extraction
0.9363920594	restricted boltzmann machine
0.9363166326	wavelet transform
0.9361973438	natural language
0.9360307451	evolutionary computation
0.9356774020	markov decision processes
0.9356506240	empirical risk minimization
0.9353555811	mobile phone
0.9353436949	alternating minimization
0.9352977051	ridge regression
0.9352932624	weakly supervised
0.9352857776	subspace clustering
0.9352768297	skip connections
0.9351702175	elastic net
0.9348175238	pattern recognition
0.9346113687	latent dirichlet allocation
0.9345644180	fractal dimension
0.9345493090	latent dirichlet allocation lda
0.9345088571	strongly convex
0.9343670299	cellular automata
0.9343155911	stochastic gradient descent sgd
0.9341804375	electronic health records
0.9341135187	electron microscopy
0.9340436442	gaussian processes
0.9337632319	convex relaxation
0.9337341156	image processing
0.9336240992	facial expression
0.9332068903	convex hull
0.9330564885	riemannian manifold
0.9329333279	markov random fields
0.9329265604	activation functions
0.9329261523	hidden markov model hmm
0.9326307056	satellite imagery
0.9322666456	principal component analysis pca
0.9320963420	generative adversarial networks gans
0.9320617986	brain tumor
0.9318265887	light field
0.9317726247	keyword spotting
0.9313674491	artificial neural networks
0.9311398092	energy minimization
0.9310335227	densely connected
0.9310079911	dirichlet process
0.9309815005	recurrent neural networks rnns
0.9309121002	markov chain monte carlo mcmc
0.9308721072	social networks
0.9307715673	hilbert space
0.9305792220	beam search
0.9305622922	neural network
0.9305307793	belief revision
0.9303155436	block coordinate descent
0.9302627647	tensor decomposition
0.9301394126	action recognition
0.9300638218	variational inference
0.9300403465	piecewise linear
0.9299676977	square root
0.9298958826	rademacher complexity
0.9298531749	vector quantization
0.9297044676	medical diagnosis
0.9296609658	text mining
0.9296341581	sparse coding
0.9295032937	particle swarm
0.9294828681	generative models
0.9294484260	feature selection
0.9294220418	reproducing kernel hilbert space
0.9293872693	ct scans
0.9292935760	handwriting recognition
0.9292775162	matrix completion
0.9292614484	visual odometry
0.9291470803	covariate shift
0.9288836495	recommender systems
0.9288281926	tabu search
0.9286185364	local minima
0.9283906300	edit distance
0.9281046657	semantic segmentation
0.9280905510	density estimation
0.9280333952	variance reduction
0.9279655163	blind deconvolution
0.9279176033	latent variable
0.9277084007	relation extraction
0.9272407935	health care
0.9271946801	directed acyclic graphs
0.9271509356	symmetry breaking
0.9270150557	active contour
0.9270117572	alzheimers disease
0.9267141737	random projections
0.9266772252	lower bounds
0.9264661351	data augmentation
0.9263031173	remote sensing
0.9261759142	named entity recognition
0.9261660166	boltzmann machine
0.9260738112	synthetic aperture radar
0.9260635324	positive definite
0.9259766428	background subtraction
0.9256129511	influence diagram
0.9255241794	computed tomography
0.9254617664	diabetic retinopathy
0.9249741768	mirror descent
0.9249632277	artificial intelligence ai
0.9249566201	hand gesture
0.9249550506	receptive field
0.9249467272	sentiment analysis
0.9247893713	multilayer perceptron
0.9247381311	proximal gradient
0.9245889615	catastrophic forgetting
0.9245424395	video surveillance
0.9245245465	active learning
0.9244095991	contrastive divergence
0.9243093538	saddle points
0.9242408182	reinforcement learning rl
0.9239870732	linear regression
0.9239780275	domain adaptation
0.9238721815	named entities
0.9238010117	dot product
0.9236725943	tensor factorization
0.9236619374	boltzmann machines
0.9234158523	spiking neurons
0.9233832634	markov random field
0.9230815839	genetic algorithm ga
0.9230210771	indoor scenes
0.9228552035	transfer learning
0.9226237706	nearest neighbor
0.9224713542	topic modeling
0.9224654144	machine learning
0.9223888505	authorship attribution
0.9222669214	nuclear norm
0.9222668340	markov decision processes mdps
0.9222440620	majority voting
0.9222413701	word embeddings
0.9221607926	deep learning
0.9220750488	automatic speech recognition asr
0.9219799160	steady state
0.9219034143	outlier detection
0.9218759170	image captioning
0.9218111790	monte carlo
0.9216284529	spoken language
0.9215888675	generative adversarial networks
0.9215842942	nearest neighbour
0.9215495160	image retrieval
0.9214391852	pos tagging
0.9213740825	total variation tv
0.9213077423	nonnegative matrix factorization
0.9210946889	land cover
0.9210733703	cognitive science
0.9210169248	variational autoencoders
0.9210141686	hypothesis testing
0.9209748512	probabilistic inference
0.9207677872	decision maker
0.9206358400	excess risk
0.9206348634	artificial immune
0.9205501224	support vector machine svm
0.9205403292	head pose
0.9205158945	bayesian nonparametric
0.9204166890	generative adversarial nets
0.9203907189	emotion recognition
0.9202401959	skin lesion
0.9200847944	logic programs
0.9199883555	formal concept analysis
0.9199312128	supply chain
0.9199190794	shared task
0.9198674233	group lasso
0.9198132677	missing entries
0.9198114556	medical imaging
0.9197618987	differential evolution
0.9193792879	markov chain monte carlo
0.9193222786	fully connected
0.9191542283	bayesian optimization
0.9189695345	word embedding
0.9188990963	deep neural networks dnns
0.9185539999	pose estimation
0.9183791305	pairwise comparisons
0.9183414815	atari games
0.9183181762	unsupervised learning
0.9182803732	blind source separation
0.9182790970	linear programming
0.9182578300	dynamic range
0.9178838974	face recognition
0.9178026192	nonmonotonic reasoning
0.9177952843	confidence intervals
0.9173333736	variational bayes
0.9172624310	positive semidefinite
0.9172536974	max pooling
0.9169911084	object recognition
0.9167804188	conditional random field crf
0.9167507513	tensor completion
0.9167343971	hate speech
0.9166662674	game theoretic
0.9166598919	phase transitions
0.9165260782	web services
0.9164250606	image denoising
0.9163450743	deep reinforcement learning
0.9163035684	software engineering
0.9162532357	adversarial examples
0.9162520837	wasserstein distance
0.9161835019	standard deviation
0.9159266727	dictionary learning
0.9158132826	roc curve
0.9157709603	graph cuts
0.9157184139	bidirectional lstm
0.9156907298	adversarial perturbations
0.9156880167	bayesian networks
0.9156280886	determinantal point processes
0.9156280000	weakly labeled
0.9153473146	error rate
0.9150303244	decision making
0.9150185804	statistical mechanics
0.9150027377	style transfer
0.9149823264	gaussian process gp
0.9148786932	convolutional neural networks cnns
0.9148081818	spike timing
0.9147090804	optimal transport
0.9146101989	gated recurrent
0.9145705453	markov decision process mdp
0.9144390309	forward backward
0.9143986963	convolutional neural network cnn
0.9143163878	expert systems
0.9139124299	inverse covariance
0.9138249482	speaker verification
0.9137735715	saddle point
0.9135846648	textual entailment
0.9135813459	structured prediction
0.9134900304	query answering
0.9134822258	single shot
0.9134686291	noun phrases
0.9134496992	sequence labeling
0.9133415108	associative memory
0.9133168167	hamming distance
0.9131052824	contextual bandits
0.9130197521	coreference resolution
0.9129043559	turing machine
0.9128151384	image restoration
0.9128063472	principal components
0.9127842300	class imbalance
0.9126839989	face detection
0.9125005489	topic models
0.9122638281	hill climbing
0.9121843233	visual question answering vqa
0.9121538326	fully convolutional
0.9121394945	local binary pattern
0.9119922577	description logic
0.9119616999	filter bank
0.9119610487	opinion mining
0.9118911757	soft computing
0.9118406984	cosine similarity
0.9118139984	constraint programming
0.9117659395	partially observable markov decision
0.9115791850	false alarm
0.9115171114	graph laplacian
0.9115154256	link prediction
0.9115107020	spectral clustering
0.9114497619	bandit feedback
0.9114251458	maximum margin
0.9113609943	statistical machine translation
0.9112365368	recurrent neural network rnn
0.9111482502	activity recognition
0.9111390624	ant colony
0.9111307204	spanning tree
0.9109801779	conditional random fields
0.9109187806	experience replay
0.9108549033	linear discriminant analysis
0.9107836994	answer set programming
0.9105986929	risk minimization
0.9104534409	object detection
0.9103403337	vector space
0.9103002501	principal component
0.9102903104	decision theory
0.9100821103	feature maps
0.9100744152	artificial neural network
0.9100679621	nuclear norm minimization
0.9099908311	policy gradient
0.9096268329	color constancy
0.9095984246	expectation propagation
0.9093869115	indian languages
0.9093868101	global optimization
0.9092894359	approximate inference
0.9092821119	differentially private
0.9092440569	dempsters rule
0.9092007825	facial landmark
0.9089851544	adversarial attacks
0.9089571017	theorem proving
0.9089030165	edge detection
0.9080349533	f1 score
0.9080283833	recurrent neural networks
0.9079618054	thompson sampling
0.9077955493	gibbs sampler
0.9077857653	pedestrian detection
0.9076877548	hamiltonian monte carlo
0.9075523252	game playing
0.9071560222	description logics
0.9071085225	inverse reinforcement learning
0.9069686506	generative adversarial
0.9067974088	artificial neural networks anns
0.9066156018	privacy preserving
0.9064926164	conditional random field
0.9064144021	higher order
0.9064104715	histogram equalization
0.9063350355	monte carlo tree search
0.9060831959	path planning
0.9060594862	abstractive summarization
0.9060148721	function approximation
0.9058959740	maximum likelihood estimation
0.9058782032	lip reading
0.9058263593	named entity
0.9054542197	face verification
0.9053189270	slot filling
0.9052352138	video captioning
0.9052187771	image segmentation
0.9051467669	supervised learning
0.9050353094	probability distributions
0.9048244887	frequency domain
0.9046428108	fake news
0.9045383409	semantic relatedness
0.9043865960	multilayer perceptrons
0.9038942598	gray level
0.9038778665	reservoir computing
0.9037583836	image registration
0.9037389644	policy iteration
0.9036780981	latent space
0.9036529351	hash codes
0.9036253634	keyphrase extraction
0.9034065328	discriminant analysis
0.9033703599	trace norm
0.9032481121	source separation
0.9032188562	matrix multiplication
0.9031958620	source code
0.9030710292	wireless sensor networks
0.9030608220	mixture model gmm
0.9030172076	web search
0.9029311041	local optima
0.9028376934	propositional logic
0.9028067989	cold start
0.9027706073	signal processing
0.9027056756	morphologically rich
0.9026781129	kolmogorov complexity
0.9025677116	turing machines
0.9024619766	exponential families
0.9023273076	search engine
0.9023161678	variational autoencoder
0.9021756235	strong convexity
0.9020845945	radial distortion
0.9020562024	face alignment
0.9019844413	dependent plasticity
0.9019002057	factor analysis
0.9014829237	false positive
0.9014733461	kl divergence
0.9014182650	anisotropic diffusion
0.9011923725	energy consumption
0.9011616638	entity linking
0.9011008860	facial landmarks
0.9010831363	photometric stereo
0.9010224461	decision theoretic
0.9010176699	nash equilibria
0.9008786951	sat solvers
0.9008555800	programming language
0.9008362778	image compression
0.9006129096	object tracking
0.9006014146	convolutional networks
0.9003285151	magnetic resonance imaging
0.9002988558	receptive fields
0.9002554532	deep neural network
0.9001973901	bounding box
0.9001508054	chinese characters
0.9001185895	partial differential equations
0.9000765394	traveling salesman problem
0.9000043358	cross entropy
0.8999349030	armed bandit
0.8996824848	l1 regularized
0.8996478785	high resolution
0.8994970213	finite state
0.8994831816	matching pursuit
0.8994265454	augmented lagrangian
0.8993036462	automatic speech recognition
0.8992916414	crowd counting
0.8992582299	loss functions
0.8992418058	license plate
0.8991504245	pascal voc
0.8991140353	nonnegative matrix factorization nmf
0.8989019528	contrast enhancement
0.8987364332	black box
0.8986607493	concept drift
0.8984768430	long short term memory lstm
0.8984226572	mobile robots
0.8983586012	super resolution
0.8982926375	answer set programming asp
0.8981350762	denoising autoencoders
0.8981229063	spatial transformer
0.8979404754	convergence rate
0.8979267160	object detectors
0.8978996378	variable length
0.8977862462	high throughput
0.8974490332	l1 norm
0.8973637865	visual recognition
0.8973400457	belief networks
0.8973106258	minimum description length
0.8971876814	scene understanding
0.8971398670	machine comprehension
0.8970433699	arc consistency
0.8969885508	fitness landscapes
0.8969754300	salient object detection
0.8968374652	probabilistic graphical models
0.8966473239	similarity measure
0.8966197265	fisher vector
0.8965586892	independent component analysis
0.8965195056	evolutionary algorithm
0.8964894471	symmetric positive definite
0.8964746414	particle filtering
0.8963654505	sensitivity analysis
0.8962955840	expected utility
0.8960333738	facial expressions
0.8959956549	association rule
0.8959809636	neural networks
0.8958362217	abstract argumentation
0.8957904444	graphical model
0.8957556742	block coordinate
0.8956025864	change detection
0.8955476924	motion blur
0.8954108694	hidden markov model
0.8953494779	hough transform
0.8952763326	language modeling
0.8951892777	eye tracking
0.8951850375	mental health
0.8951423166	mixture models
0.8951087471	long short term memory
0.8950474195	combinatorial optimization
0.8950231702	convolutional neural network
0.8949541443	sliding window
0.8949072530	inverse problems
0.8948898360	kernel ridge regression
0.8948475631	magnetic resonance
0.8948433632	decision support
0.8948420856	latent variable models
0.8945787487	document summarization
0.8945307986	spoken dialogue
0.8944572499	max margin
0.8941572560	component analysis
0.8939264754	human activity
0.8939071796	ant colony optimization
0.8937300049	game theory
0.8936567743	stable model semantics
0.8936350004	human brain
0.8932873662	power law
0.8932851465	encoder decoder
0.8932697460	feed forward
0.8932260920	mr images
0.8931819687	latent variables
0.8930157527	evidential reasoning
0.8929745268	region proposal
0.8928214937	bayesian network
0.8927792725	machine learning ml
0.8927753693	subset selection
0.8926794266	lower bound
0.8924605010	mobile device
0.8922793163	information theoretic
0.8922556502	image reconstruction
0.8922188972	rectified linear units
0.8921843269	association rules
0.8921485112	expectation maximization
0.8921435539	neural machine translation nmt
0.8915372191	natural language processing nlp
0.8915039199	generalization error
0.8915011280	cloud computing
0.8914306767	context aware
0.8910069684	echo state networks
0.8906802775	web service
0.8906735377	neural nets
0.8906622082	answer set
0.8904558677	weak supervision
0.8903806604	support vector
0.8901618907	bio inspired
0.8901297520	long term
0.8900008022	generative adversarialnetworks
0.8898948373	generative modeling
0.8898598352	marginal likelihood
0.8898234575	closed loop
0.8897984661	cross validation
0.8897326519	modal logic
0.8897301282	piecewise constant
0.8897005215	label propagation
0.8894684446	intelligent systems
0.8893671371	document classification
0.8892544416	deep neural networks
0.8892279480	upper confidence
0.8891182548	metric learning
0.8891180038	missing data
0.8890975889	sequential decision making
0.8890730551	neural machine translation
0.8890424216	visual attention
0.8890362594	distributed representations
0.8889321942	geodesic distance
0.8888880064	expectation maximization em
0.8887856391	distributional semantics
0.8887484515	gaussian process regression
0.8887297476	blood vessels
0.8887125712	causal inference
0.8887083750	gaussian mixture models
0.8886795797	medical records
0.8884543068	template matching
0.8883489644	recurrent neuralnetworks
0.8882745292	indian buffet
0.8878450868	floating point
0.8877235152	knowledge transfer
0.8875933158	rough set
0.8873602322	semi supervised
0.8873286349	domain specific
0.8870818746	fisher vectors
0.8869251190	mobile robot
0.8869171853	large vocabulary
0.8869166103	deep neural network dnn
0.8864200947	temporal difference
0.8864199569	continuous control
0.8862833101	error correction
0.8862643475	building blocks
0.8861511093	adversarial training
0.8859452781	stick breaking
0.8858918906	fuzzy set
0.8858347293	zipfs law
0.8854152690	state space
0.8853426997	convolutional neural networks
0.8852698364	graph cut
0.8851395139	optimal control
0.8851147754	deep cnns
0.8850969846	object oriented
0.8847570966	recurrent neural
0.8846488012	submodular functions
0.8844045348	generative adversarial network
0.8840864392	constraint satisfaction problems
0.8840859524	open source
0.8840301598	community detection
0.8837983837	integer linear programming
0.8835651310	quadratic programming
0.8834980269	constraint propagation
0.8834461489	feature extractor
0.8833071239	eye movement
0.8831729309	junction tree
0.8830421315	binary classification
0.8828874250	low rank
0.8828696755	gabor filter
0.8828148695	vector spaces
0.8827986507	model selection
0.8826838718	constrained optimization
0.8826457895	reproducing kernel
0.8826437650	directed acyclic graph
0.8826182931	auto encoder
0.8825168158	convergence rates
0.8824827848	convolutionalneural network cnn
0.8824670363	search engines
0.8824465926	gaussian mixture
0.8822431779	cross lingual
0.8822159008	stereo matching
0.8821456409	wikipedia articles
0.8821321936	sign language
0.8821120214	gabor filters
0.8820024539	user interface
0.8819397103	multi armed bandit
0.8818615778	rectified linear
0.8818214041	trust region
0.8817529048	faster rcnn
0.8816918964	biologically inspired
0.8816801024	vector representations
0.8815522371	variable selection
0.8814255002	conditional probability
0.8813638499	neural net
0.8812888663	building block
0.8811249317	semi definite
0.8811078685	variance reduced
0.8810928915	web page
0.8809707136	visual cortex
0.8808379677	minimax optimal
0.8808186509	word representations
0.8807847763	feature fusion
0.8807300325	nonconvex optimization
0.8807118568	map inference
0.8805886576	attention mechanism
0.8805669124	probabilistic programming
0.8805060149	active contours
0.8804944690	machine vision
0.8803530643	hierarchical clustering
0.8803468086	hinge loss
0.8801350663	eeg signals
0.8799865451	random field
0.8799285957	sample complexity
0.8799155452	parameter estimation
0.8798453054	knowledge discovery
0.8797975266	deep convolutional neural networks
0.8797110913	language processing
0.8796386741	saliency maps
0.8796270625	named entity recognition ner
0.8796181687	optimization problems
0.8795752976	residual networks
0.8793122307	global convergence
0.8791778053	sparse representation
0.8788899283	saliency detection
0.8788023142	early stopping
0.8784265272	dempster shafer theory
0.8782121935	graph partitioning
0.8780028637	auto encoders
0.8779904388	pattern matching
0.8778777393	evaluation metrics
0.8777567857	multi armed bandits
0.8776588494	pattern mining
0.8776559822	digital image
0.8775864194	motion planning
0.8775851572	content based image retrieval
0.8774771033	handwritten character
0.8774628349	human pose
0.8774453863	moving objects
0.8774395852	nonparametric bayesian
0.8773716280	spatio temporal
0.8771964965	human pose estimation
0.8771503568	text categorization
0.8771336281	free grammars
0.8769456877	mixture model
0.8768933230	intrinsic dimension
0.8761985040	text documents
0.8761675792	spiking neural networks
0.8761534914	scene recognition
0.8761445595	default logic
0.8760687201	social network
0.8760606788	multiple choice
0.8757409313	bipartite graph
0.8756731224	np complete
0.8756496996	video frame
0.8755686096	high dynamic range
0.8755286976	noise removal
0.8754460344	late fusion
0.8753525168	globally optimal
0.8753117584	partially observed
0.8752910280	neuro fuzzy
0.8752332930	digital ecosystems
0.8751880191	hash functions
0.8751743939	multi modal
0.8751281063	convolutional layers
0.8751102341	question answering qa
0.8750904207	large scale
0.8748580974	visual question answering
0.8747273192	disjunctive logic
0.8747226408	visual tracking
0.8745964419	fine tuning
0.8745487635	conjugate gradient
0.8745354242	facial expression recognition
0.8745242570	eye movements
0.8745021544	structured output
0.8743908992	recurrent unit
0.8743303551	event detection
0.8743103535	extreme learning machine
0.8742977309	convolutional neural
0.8741889777	rough set theory
0.8741147600	bayes optimal
0.8739643562	background knowledge
0.8739118959	video games
0.8739095502	tree structure
0.8738789060	gradient boosting
0.8737973165	error bounds
0.8737632822	normal form
0.8737349020	video clips
0.8736307594	generative adversarial network gan
0.8735891318	network topology
0.8735727316	character recognition
0.8734513518	skin lesions
0.8733792570	quality assessment
0.8733300718	human action recognition
0.8732265184	object proposals
0.8730971876	regret bound
0.8730414221	heuristic search
0.8729225639	programming tplp
0.8726863284	deep networks
0.8724930740	text generation
0.8723628486	speech synthesis
0.8721372457	hand pose estimation
0.8721327891	natural gradient
0.8720345930	temporal logic
0.8719708210	knowledge graphs
0.8719602645	problem solving
0.8719573645	actor critic
0.8718161806	white box
0.8717971525	medical images
0.8717945210	undirected graphical models
0.8717286374	density ratio
0.8715548247	support vector machines svms
0.8715039351	large margin
0.8713606277	multiple kernel learning
0.8712444195	data streams
0.8711012150	short term memory
0.8709684281	character level
0.8709116315	policy search
0.8707676401	signal recovery
0.8707567503	user preferences
0.8707121819	cost sensitive
0.8707112271	annotated corpora
0.8705629156	naive bayes classifier
0.8705514959	speaker recognition
0.8704902996	graphical lasso
0.8704560797	possibilistic logic
0.8704425118	low dose
0.8702546749	sat solver
0.8702536339	situation calculus
0.8700640386	fully convolutional networks
0.8699227152	image classification
0.8698519403	spatial pyramid
0.8694355541	recurrent neuralnetwork
0.8692094915	gaussian mixture model
0.8692072629	hilbert spaces
0.8690556718	linear equations
0.8689973893	semi automatic
0.8689921305	change point detection
0.8689835943	unlabeled data
0.8687517037	hyperspectral image
0.8687188101	saliency map
0.8686010313	long range
0.8684733746	dynamic environments
0.8681880949	random projection
0.8681689282	single image super resolution
0.8679996457	bi directional
0.8679487794	streaming data
0.8678979054	exhaustive search
0.8678151283	convolutionalneural networks cnns
0.8678138490	human robot interaction
0.8677280870	additive noise
0.8677003870	binary codes
0.8676312480	order statistics
0.8675347130	fine grained
0.8675030474	widely applicable
0.8674960661	multi objective
0.8673790459	fixed point
0.8673689435	ms coco
0.8673293847	knowledge acquisition
0.8673012017	skip gram
0.8672629334	convolutional neuralnetwork cnn
0.8671868481	deep convolutional networks
0.8670908300	knowledge representation
0.8670430296	confusion matrix
0.8670050223	convolutional neuralnetworks
0.8669776114	recommendation systems
0.8669290297	convergence properties
0.8669263654	mixed membership
0.8668888808	causal discovery
0.8668022951	high dimensional
0.8667161929	partial order
0.8666145942	mixed integer
0.8665242034	singular values
0.8664485180	speech enhancement
0.8663695333	text summarization
0.8662138266	biologically plausible
0.8659490519	random fields
0.8659189120	energy efficient
0.8658651724	asymptotically optimal
0.8656289864	single stage
0.8655168651	quasi newton
0.8653299752	video frames
0.8652945150	constraint satisfaction problem
0.8652420455	human activities
0.8651657473	feedforward neural networks
0.8651633791	belief propagation bp
0.8649678011	coarse grained
0.8649591376	object localization
0.8648662348	noise reduction
0.8648517289	artificial life
0.8646698091	probabilistic models
0.8645939972	motion estimation
0.8645336478	spatially varying
0.8645107724	random sampling
0.8642734062	context sensitive
0.8642637622	natural scene
0.8639983000	residual network
0.8639318196	principal componentanalysis
0.8639141285	spd matrices
0.8639000841	probabilistic reasoning
0.8638569203	gaussian graphical models
0.8638444273	texture synthesis
0.8637740527	nature inspired
0.8637078606	semantic web
0.8635931998	shot learning
0.8635541496	lesion segmentation
0.8635076259	bandit problem
0.8635025545	stochastic approximation
0.8634137400	exploration exploitation
0.8633815326	sparsity inducing
0.8633361705	deep convolutional
0.8632761548	robust pca
0.8632608083	intelligent agents
0.8631346976	belief functions
0.8630731374	unsupervised domain adaptation
0.8627390957	multi agent
0.8627264439	data science
0.8627076772	continuous variables
0.8626529474	gaussian noise
0.8626058077	short text
0.8625405097	rank aggregation
0.8623420946	free energy
0.8622493640	word spotting
0.8621030505	parameter tuning
0.8620345133	vc dimension
0.8619199635	semantic similarity
0.8616469610	human body
0.8616060887	boolean functions
0.8616018419	handwritten digit
0.8614816339	error correcting
0.8613192443	vector field
0.8613128711	low complexity
0.8612679370	graph embedding
0.8612671189	brain mri
0.8612547349	image synthesis
0.8610362529	sparse representations
0.8608814568	min max
0.8608589300	rgb images
0.8607034454	multitask learning
0.8606021977	short term
0.8604100006	convolutional neuralnetwork
0.8603375425	block diagonal
0.8602987954	compositional distributional
0.8602285605	relative entropy
0.8598143050	lifelong learning
0.8596071267	sentiment classification
0.8595890849	recurrent networks
0.8595601340	open world
0.8593715808	adverse drug
0.8592101861	loss function
0.8589879976	random variables
0.8589592692	partition function
0.8589101981	hierarchical dirichlet process
0.8587832405	visual saliency
0.8586497369	correlation filters
0.8586491200	visual concepts
0.8586176187	tensor decompositions
0.8585516927	body parts
0.8584745739	hyperparameter optimization
0.8584608131	rank minimization
0.8584461509	open set
0.8583138347	feature extractors
0.8580079928	deep architectures
0.8579075994	mutation rate
0.8578982507	bayes rule
0.8578898339	imitation learning
0.8578695255	formal semantics
0.8576175082	deep nets
0.8576057276	mini batch
0.8575082975	implicit feedback
0.8573086087	upper bounds
0.8572691646	recurrent neural networks rnn
0.8572323369	support vector regression
0.8571652822	computational linguistics
0.8571636656	video summarization
0.8571535511	kullback leibler divergence
0.8570700663	statistical inference
0.8570383916	nearest neighbor search
0.8568040666	semantic parsing
0.8567776148	semantic relations
0.8567218714	equivalence classes
0.8564737832	missing values
0.8562286351	model checking
0.8562243590	heavy tailed
0.8562008668	lexical resources
0.8561904404	neural architectures
0.8560861988	high precision
0.8560461527	convolutional neural networkscnns
0.8557094394	experimental design
0.8556173026	fully automated
0.8555636373	marginal distribution
0.8553934852	phase retrieval
0.8550501912	word alignment
0.8549207023	fully convolutional network
0.8547391046	law enforcement
0.8545043780	digit recognition
0.8543873879	sufficient statistics
0.8541225933	cross modal
0.8540990768	ground truth
0.8539519896	worst case
0.8539160828	deep rl
0.8537838264	satellite images
0.8537635041	multi scale
0.8537460377	multi layer
0.8536941847	soft constraints
0.8536706986	open ended
0.8535438862	visual perception
0.8535160422	norm regularization
0.8534498375	photo realistic
0.8534213874	prior knowledge
0.8534067700	exact inference
0.8533725494	english language
0.8533549655	scale invariant
0.8532173693	activation function
0.8529743747	inference engine
0.8529134729	generalization bounds
0.8528507784	region growing
0.8528452700	computational intelligence
0.8527532594	answer sets
0.8526901226	edge preserving
0.8525047622	instance segmentation
0.8524115074	street view
0.8523142980	sensor networks
0.8520675521	group sparsity
0.8520464539	latent representations
0.8520211443	kernel density estimation
0.8519990207	lp relaxation
0.8519968782	symbolic regression
0.8518931839	approximate nearest neighbor
0.8518135219	efficient implementation
0.8518116157	dialogue systems
0.8517230864	rough sets
0.8516035517	line search
0.8514803336	online convex optimization
0.8513770478	privileged information
0.8513133674	context dependent
0.8510929378	leibler divergence
0.8510643155	open domain
0.8510345437	metric spaces
0.8509611156	dynamic time warping
0.8509549042	deep generative models
0.8508793693	correlation filter
0.8507192891	policy evaluation
0.8506856717	natural languageprocessing
0.8506138387	decision support systems
0.8504525496	noisy labels
0.8503602149	visual cues
0.8503464302	convolutional neural networkcnn
0.8503461317	single frame
0.8502968459	hash function
0.8502085434	word segmentation
0.8501385254	high speed
0.8501323616	online learning
0.8500711173	depth map
0.8500566643	correlation coefficient
0.8495571349	deep neuralnetworks
0.8495304402	camera pose
0.8495232111	point sets
0.8494196243	natural language generation
0.8493790166	spatial temporal
0.8493263408	convolutional neural networks cnn
0.8493094374	sequence prediction
0.8492211693	stochastic block model
0.8491997506	object detector
0.8490521002	generalized linear models
0.8490373891	rule based
0.8489629503	variational bayesian
0.8489545402	similarity measures
0.8488128139	triplet loss
0.8487832758	multi view
0.8487561566	supportvector machine
0.8485435060	frank wolfe
0.8484984922	human activity recognition
0.8484915783	belief function
0.8484436237	multiple instance learning
0.8483791883	argumentation frameworks
0.8482854623	support vectormachines
0.8481674368	multi criteria
0.8480770168	metropolis hastings
0.8480507006	manifold learning
0.8480358447	text corpora
0.8479722626	hierarchical bayesian
0.8477990735	gait recognition
0.8477137320	linear convergence
0.8476307835	probability density
0.8476307421	word similarity
0.8474473501	structural equation
0.8473846964	scene parsing
0.8473503252	boundary detection
0.8470360072	stochastic variational inference
0.8467651651	log likelihood
0.8466996027	high dimension
0.8465977615	convolutional network
0.8465947267	goal oriented
0.8465881017	regression trees
0.8464461345	vehicle routing
0.8462035153	hidden markov
0.8461353341	naturallanguage processing
0.8461185608	semi supervised learning
0.8460102326	armed bandits
0.8459217480	depth estimation
0.8458489371	belief network
0.8458411725	posterior sampling
0.8458317579	logic program
0.8458313090	context free
0.8457076582	low resource languages
0.8456765608	label noise
0.8454557666	deep cnn
0.8454050985	sparse recovery
0.8452618782	action spaces
0.8452106691	graph theoretic
0.8450426839	color space
0.8448374144	machine intelligence
0.8448075784	linear transformation
0.8446349028	greedy algorithm
0.8443927388	video sequences
0.8441657816	conditional probabilities
0.8440108775	low resolution
0.8438633831	high order
0.8437837201	deep convolutional neural network
0.8437631195	place recognition
0.8436970703	feature descriptor
0.8434187114	cryo em
0.8433606134	svm classifier
0.8432486465	cross domain
0.8430654785	false positive rate
0.8430147477	blur kernel
0.8429989157	distance measures
0.8429791507	answer set semantics
0.8429462817	image caption
0.8429066732	code switching
0.8428723673	camera calibration
0.8427073439	crowd sourcing
0.8426138176	softmax loss
0.8425756003	lifted inference
0.8424480989	tree structured
0.8424477693	markov decision process
0.8423123878	text classification
0.8422740285	temporal dynamics
0.8422315361	fundus images
0.8420448494	latent factor
0.8420126659	video object segmentation
0.8419334571	image inpainting
0.8417277410	bandit problems
0.8415453603	logical forms
0.8415236430	likelihood ratio
0.8414918302	external memory
0.8413866588	open source software
0.8413803951	plan recognition
0.8413185519	graph matching
0.8411932747	word vectors
0.8411866334	pixel level
0.8410716782	target tracking
0.8410428738	l0 norm
0.8410020008	gold standard
0.8409091060	spiking neural network
0.8408944831	proposal generation
0.8407896238	recurrentneural networks
0.8407794214	probability measures
0.8407722220	high dimensions
0.8404932090	spatial resolution
0.8403946531	multi layer perceptron
0.8402293682	native language
0.8401229417	feature matching
0.8400842445	annotated corpus
0.8400761906	task oriented
0.8400367663	posterior probabilities
0.8399587639	pareto optimal
0.8399196300	structured sparsity
0.8398993738	parallel corpora
0.8396660189	social sciences
0.8395784072	domain shift
0.8395573411	signature verification
0.8394107158	gaussian distribution
0.8393443170	language independent
0.8392208366	mid level
0.8392100281	iris recognition
0.8391871244	data association
0.8391744376	assignment problem
0.8391490049	low resource
0.8391140711	loss minimization
0.8389368000	dempster shafer
0.8389114177	undirected graphical
0.8388353157	extended version
0.8387449052	scheduling problem
0.8386173205	weakly supervised learning
0.8385836388	sequential monte carlo
0.8381889274	human computer interaction
0.8381380000	em algorithm
0.8380822649	gaussian kernel
0.8378104690	response generation
0.8377259439	np hard
0.8375687870	long term dependencies
0.8373747350	reward functions
0.8372915913	locally linear
0.8372581848	low dimensional
0.8372543155	convergence analysis
0.8368745775	scene text
0.8367674494	policy gradients
0.8367591423	language understanding
0.8367008970	arabic language
0.8365110572	ad hoc
0.8364317059	batch size
0.8362958872	fully automatic
0.8362881668	normal distribution
0.8362255474	optimal policies
0.8361612606	local descriptors
0.8361400130	bounding boxes
0.8361219345	submodular function
0.8360597045	adversarial networks
0.8359951984	directed graphs
0.8359786499	multi channel
0.8358861978	acoustic modeling
0.8358390239	robot navigation
0.8357492164	feature vectors
0.8357353625	image enhancement
0.8356446052	alternating direction method
0.8356282978	statistical relational
0.8353321460	human action
0.8353118002	incomplete information
0.8352441962	max flow
0.8350521337	multi resolution
0.8349364668	landmark localization
0.8348836646	line segments
0.8348715564	covariance matrices
0.8348261982	low cost
0.8347513156	sufficient condition
0.8347287458	graph construction
0.8347061944	scoring function
0.8344302619	longshort term memory
0.8343304991	hyperspectral images
0.8342598012	short texts
0.8342512785	energy efficiency
0.8341508612	medical image analysis
0.8340360180	spatial relations
0.8339329508	sample size
0.8339227525	ego motion
0.8339077032	low rank matrix
0.8338741067	image generation
0.8338542481	l1 regularization
0.8336559911	depth sensors
0.8336539319	longshort term memory lstm
0.8335503229	ct images
0.8335099817	world wide web
0.8333454482	cost functions
0.8333289545	external knowledge
0.8331789790	representation learning
0.8331463844	stochastic processes
0.8330384446	affinity matrix
0.8328962210	fully convolutional neural network
0.8328348483	risk sensitive
0.8327763539	event recognition
0.8326952106	automated reasoning
0.8326362920	decision boundary
0.8324327475	matrix decomposition
0.8323464395	deep neuralnetwork
0.8323095641	restricted boltzmann
0.8322515356	question answer
0.8321790644	natural language understanding
0.8321403431	complex valued
0.8321312757	multi objective optimization
0.8319820787	machine learningtechniques
0.8315988541	object proposal
0.8315263692	image patches
0.8314005607	post editing
0.8312887661	meta learning
0.8312557919	hierarchical structure
0.8312112107	kernel matrix
0.8312054991	probability distribution
0.8310300186	depth maps
0.8309049118	semantic role
0.8307625010	multi task
0.8306308929	compression ratio
0.8305403768	salient regions
0.8303552085	contour detection
0.8303453047	long short termmemory
0.8303337882	stochastic gradients
0.8303214341	convolutionalneural networks
0.8302305036	naive bayesian
0.8302304771	human actions
0.8302032038	probabilistic logic
0.8301336434	natural images
0.8300412041	programming languages
0.8299800665	traffic flow
0.8298104937	action space
0.8297616472	low precision
0.8295633670	high density
0.8293875090	global optimality
0.8293254003	precision recall
0.8292697296	word order
0.8291410119	feature engineering
0.8290588257	convex functions
0.8290429827	evaluation metric
0.8289256941	digital images
0.8287786164	support vectormachine
0.8286062015	moving average
0.8285299251	hidden variables
0.8283440566	image quality assessment
0.8283098092	pre processing
0.8281681344	latent variable model
0.8281439225	feature space
0.8281303112	stable models
0.8280258348	euclidean space
0.8280032567	search heuristics
0.8279991535	low rank matrix completion
0.8278876449	feature learning
0.8277710139	basis functions
0.8277206284	brute force
0.8274299298	gene expression data
0.8273932518	stochastic gradientdescent
0.8273834827	deepconvolutional neural network
0.8271388444	perceptual quality
0.8270829417	pixel wise
0.8269623992	state transition
0.8269085565	norm minimization
0.8268608140	sensor data
0.8268541197	inverse problem
0.8267199480	cross language
0.8265972510	multi relational
0.8265885578	multi agent systems
0.8265076358	evolutionary computing
0.8264561201	decision makers
0.8264496583	software development
0.8262245145	distance metric
0.8261902456	convergence guarantees
0.8261392833	markov logic
0.8258217944	visual words
0.8255884891	human eye
0.8255005169	remote sensing image
0.8254216553	connected components
0.8252646775	frame rate
0.8252444260	support vector machines svm
0.8252089859	sparse pca
0.8246946049	generalization ability
0.8246458272	word length
0.8245394024	english translation
0.8242762853	closed form
0.8242521673	multi document summarization
0.8237745865	linear program
0.8237703712	soft attention
0.8237565868	high quality
0.8236127643	human perception
0.8233632499	fuzzy sets
0.8233565999	labeled data
0.8233470943	long short term
0.8233167113	deep hashing
0.8230019128	facial action
0.8228214133	texture features
0.8226191636	bias variance
0.8226059419	exact recovery
0.8225589601	synaptic weights
0.8225296958	image deblurring
0.8223821684	belief change
0.8223507690	benchmark dataset
0.8223452220	semantic representations
0.8222624735	scene flow
0.8222077981	black box optimization
0.8221891876	word sense
0.8221213377	information processing
0.8220418502	multi camera
0.8220412499	l2 norm
0.8218339440	fuzzy c means
0.8216918381	video sequence
0.8216656946	deep belief networks
0.8213933933	hyper parameter
0.8211645708	convolutional neuralnetworks cnns
0.8210579350	deep learning dl
0.8210248316	medical image segmentation
0.8209958045	curve auc
0.8209731382	limited memory
0.8208878817	facial images
0.8208565180	medical image
0.8208418802	face detector
0.8207651558	survival analysis
0.8206934327	sg mcmc
0.8205418274	audio visual
0.8204872893	small world
0.8204651559	visual search
0.8204111236	saliency prediction
0.8201299981	scale free
0.8198393491	novelty detection
0.8197669951	ising models
0.8196400760	class specific
0.8196345047	principalcomponent analysis
0.8195571364	reward function
0.8195466708	weakly supervised object
0.8194695265	domain independent
0.8194383893	siamese network
0.8194121662	regret bounds
0.8193310311	dynamic scenes
0.8191844742	low rank approximation
0.8191538776	unsupervised feature learning
0.8190748451	probability theory
0.8187146817	artificial neural
0.8186783159	cross modality
0.8186175706	scene text detection
0.8186165126	multi person
0.8185273630	fixed size
0.8184223631	moment matching
0.8184003506	semantic concepts
0.8183511722	arm identification
0.8183424299	uniform sampling
0.8183317707	ell1 norm
0.8180621319	real valued
0.8180540603	graph theory
0.8178962390	multi core
0.8178623421	error bound
0.8178365304	long short term memory networks
0.8177671963	object categories
0.8177267698	kullback leibler
0.8176639745	knowledge graph
0.8175397625	action localization
0.8175114453	texture classification
0.8174810738	entity resolution
0.8174233780	fully connected layers
0.8172696092	cost function
0.8172196394	generative model
0.8171403987	parallel corpus
0.8171099798	deep neural nets
0.8169463171	cma es
0.8166079629	low bit
0.8165796553	false positives
0.8164923391	malware detection
0.8161520745	faster r cnn
0.8160869137	max product
0.8160218549	recent advances
0.8158946592	human poses
0.8158190654	speech processing
0.8157523838	skeleton based
0.8155401712	data collection
0.8154510577	adversarial samples
0.8153997769	handwritten digits
0.8153085262	fully convolutional neural networks
0.8152975872	fitness function
0.8152012930	multiple views
0.8151966888	negative sampling
0.8151804462	regularization parameter
0.8151727145	deepreinforcement learning
0.8150110115	random graph
0.8149800061	low rank tensor
0.8149493376	news articles
0.8149237810	deep neural networks dnn
0.8149133442	observational data
0.8148548786	group sparse
0.8147782177	single view
0.8147534423	speech signals
0.8146840266	empirical risk
0.8146022699	variational approximation
0.8145859809	data mining techniques
0.8144972135	feature map
0.8142705852	common sense
0.8142484032	connectionist temporal
0.8141531514	multivariate gaussian
0.8141281149	monocular depth
0.8140380871	pre trained
0.8139809189	high confidence
0.8136976004	sum product networks
0.8135264901	low power
0.8135238084	answer pairs
0.8134907437	sentence level
0.8132398369	multi instance
0.8132169544	image fusion
0.8132054961	bayesian optimisation
0.8128861601	web pages
0.8128855506	deep neural
0.8128518136	visual content
0.8128460203	hand pose
0.8128337236	transfer function
0.8127479291	step size
0.8127373532	language generation
0.8127306355	smooth functions
0.8127297687	video game
0.8125133411	face identification
0.8124444146	user experience
0.8123871933	material recognition
0.8123665580	dialog state
0.8121871681	random variable
0.8121506103	intra class
0.8120852268	differential equation
0.8120323741	gray scale
0.8116142088	singular value decomposition
0.8115394133	neural network architecture
0.8113673863	iteration complexity
0.8113503006	view synthesis
0.8113029127	instance level
0.8112094700	latent factors
0.8111184671	human motion
0.8110420623	recurrentneural network
0.8110348637	person videos
0.8109614379	gaze estimation
0.8109527212	ising model
0.8108133031	motion segmentation
0.8102962199	high level
0.8099879041	color images
0.8099742168	recurrent network
0.8098900600	data driven
0.8097978474	english german
0.8097935824	attention mechanisms
0.8097612569	lighting conditions
0.8097138214	convolutional layer
0.8097087241	neural network architectures
0.8096617872	convolutionalneural network
0.8094950296	pac bayesian
0.8093783976	multi dimensional
0.8092749660	feature descriptors
0.8092298160	embedded systems
0.8091560031	logistic loss
0.8090496448	pac bayes
0.8090422577	document level
0.8090056855	tree structures
0.8089695240	structural similarity
0.8089248205	machine reading
0.8089025861	transition based
0.8088628914	person re identification
0.8086500438	high frequency
0.8086109745	supervised hashing
0.8085806608	image super resolution
0.8084052945	semantic labeling
0.8083920868	image annotation
0.8083081857	multi source
0.8082570086	image sequences
0.8080109873	statistical analysis
0.8079470307	sar images
0.8079258128	kernel density
0.8075970488	microscopy images
0.8075180311	natural languages
0.8074078332	brain activity
0.8071815463	gaussian distributions
0.8071544180	multi frame
0.8070281638	physical systems
0.8069164635	fixed points
0.8066736003	predictive modeling
0.8065993110	search space
0.8065876843	early stage
0.8064549340	probabilistic programs
0.8063057299	ensemble methods
0.8062753626	relational data
0.8061718386	expert advice
0.8061489119	feature detectors
0.8061049192	object category
0.8060033912	curriculum learning
0.8059929388	statistical machinetranslation
0.8058995133	spectral norm
0.8058175351	data fusion
0.8055789583	sparse subspace clustering
0.8053892548	decision rules
0.8051999386	code mixed
0.8051622332	spatial context
0.8049932802	sum product
0.8049539213	language model
0.8047587456	pair wise
0.8046662678	linear classifiers
0.8046107282	radial basis
0.8043697048	relation classification
0.8042712306	domain knowledge
0.8041876753	visual object tracking
0.8040651120	log linear
0.8039710246	dirichlet allocation lda
0.8038811324	salient objects
0.8038431397	semantic image segmentation
0.8038217693	data stream
0.8036635027	type ii
0.8035326375	infinite dimensional
0.8035121874	lstm networks
0.8035067597	conditional distributions
0.8034641797	multi stage
0.8031445835	contextual information
0.8031332622	fast convergence
0.8030349699	multi step
0.8030165282	hyper parameters
0.8029790960	partially observable markov
0.8028635167	multi level
0.8027509095	deep network
0.8026433043	graph structured
0.8025237124	penalty functions
0.8024864146	condition number
0.8023817371	object segmentation
0.8020482170	feature sets
0.8020044543	markov networks
0.8020008756	autonomous agents
0.8016790167	predictive models
0.8014434626	supervised classification
0.8012002403	commonsense knowledge
0.8010441223	object instances
0.8008531330	salient object
0.8007068898	computationally efficient
0.8006530704	similarity metric
0.8006044956	search strategy
0.8004824257	input output
0.8003692488	cnn architectures
0.8003681225	language models
0.8002912583	cross view
0.8001625398	convolution neural network
0.8000958443	vector valued
0.7999699955	distance metric learning
0.7999166081	stochasticgradient descent
0.7998312936	fully connected layer
0.7998035864	multi label classification
0.7997757821	multi sensor
0.7994780368	rgb image
0.7991348491	natural language inference
0.7990962402	undirected graph
0.7990253168	bethe free
0.7989523567	post processing
0.7988330369	memory efficient
0.7986272002	human judgments
0.7982473980	fourier domain
0.7982338192	information gain
0.7981097260	discrete variables
0.7980274569	surrogate loss
0.7976999974	acoustic models
0.7976420797	end users
0.7975461714	distance measure
0.7975242748	utility functions
0.7975026519	topic model
0.7970718710	multi task learning
0.7970592342	deepneural networks
0.7970344167	camera motion
0.7967775998	scheduling problems
0.7966331232	optimal solutions
0.7961869664	neural style transfer
0.7961678574	multiple sources
0.7959508970	trade offs
0.7958373351	ai systems
0.7957651600	face images
0.7957234676	parse trees
0.7956629215	accelerated gradient
0.7955652419	object classification
0.7954309751	optical flow estimation
0.7953664715	word error rate
0.7949526329	robust principal component analysis
0.7946680149	statistical physics
0.7946477021	deep belief network
0.7946238102	phrase based
0.7945807401	global optimum
0.7944828649	feed forward neural network
0.7943959418	data sets
0.7943870712	texture analysis
0.7943843351	depth images
0.7943003647	temporal difference learning
0.7942983175	satellite image
0.7940596152	human intelligence
0.7940477178	likelihood estimation
0.7940221923	gender classification
0.7939394171	markov decision
0.7937713813	performance measures
0.7936793372	squared loss
0.7935112831	sentence pairs
0.7935063761	model fitting
0.7933763410	expression recognition
0.7933555015	maximum mean discrepancy
0.7933315784	dialog systems
0.7930640202	incremental learning
0.7930549015	deep learning architecture
0.7929540316	forward pass
0.7929291566	minimization problem
0.7928688132	short term memory lstm
0.7928337490	multi label
0.7927265639	power consumption
0.7926580099	data analysis
0.7926441997	nystrom method
0.7926042859	global constraints
0.7926024945	latent topics
0.7924570415	conditional generative adversarial
0.7918539220	domain experts
0.7917163886	handcrafted features
0.7916595860	learning rates
0.7916464961	multi object tracking
0.7915573321	level set
0.7915261498	decision rule
0.7914859682	deep recurrent
0.7913909278	local optimum
0.7913165725	natural scenes
0.7910917809	decision process
0.7910555072	single agent
0.7907849179	user generated
0.7902735417	additive models
0.7901761151	word level
0.7901490588	objective function
0.7901244119	bi lstm
0.7900985152	synthetic data
0.7898695671	joint training
0.7896856502	word frequency
0.7889588652	precision matrix
0.7888587828	public health
0.7888042616	community structure
0.7887355374	speech signal
0.7885658870	root mean square
0.7885562404	population size
0.7884494385	sparse matrix
0.7881637939	efficient inference
0.7881363827	image analysis
0.7880683991	technical report
0.7880288182	sample efficient
0.7877007584	region based
0.7876219738	visual attributes
0.7875942029	automatically generated
0.7875766989	single image
0.7869917570	feed forward neural networks
0.7869893030	kernel pca
0.7869101867	iterative optimization
0.7866837289	vision tasks
0.7864516362	vessel segmentation
0.7858626592	vector machine
0.7858572223	single layer
0.7857955901	multi class
0.7856248126	unseen classes
0.7853968973	risk factors
0.7850446827	probabilistic graphical
0.7848948118	search spaces
0.7848552372	gaussian graphical
0.7845431552	business process
0.7842002914	hidden states
0.7837232573	complex networks
0.7836702199	complex systems
0.7836147599	nonnegative matrix
0.7833707573	vision based
0.7832017840	fine grained recognition
0.7827223173	deep residual
0.7824451880	vehicle detection
0.7823863087	natural image
0.7823706201	causal relationships
0.7819985916	sparse bayesian learning
0.7819773572	low level
0.7818127999	autonomous systems
0.7817461224	neural networks cnns
0.7816598742	sequential data
0.7814969171	parallel computing
0.7813414464	dose ct
0.7812070088	encoder decoder network
0.7806780859	incomplete data
0.7806028104	sar image
0.7803117547	theoretic approach
0.7801163090	recurrent units
0.7800237538	video processing
0.7799375829	faster convergence
0.7797223993	content based
0.7796691171	reconstruction error
0.7791494851	linear measurements
0.7790420621	multiple instance
0.7790294791	youtube 8m
0.7785953734	generalization bound
0.7781675124	markov models
0.7780218121	residual learning
0.7778094371	machine learning techniques
0.7777419938	generativeadversarial networks
0.7777130686	latent structure
0.7777008399	cost effective
0.7776226400	image patch
0.7773772530	real world
0.7773533305	convolutional filters
0.7768798030	hidden state
0.7767290056	adversarial loss
0.7760971589	utility function
0.7758624147	domain invariant
0.7755869751	early detection
0.7754961864	spike trains
0.7754393347	moving object
0.7751753167	graph based
0.7750262501	scene classification
0.7747793262	automatic segmentation
0.7747613402	low rank representation
0.7747143463	kernel regression
0.7747014188	document clustering
0.7746042160	fine tuned
0.7740785681	face database
0.7737638966	synthetic images
0.7734020358	deep residual networks
0.7732560473	labeled samples
0.7730896918	linear discriminant
0.7730349682	patch based
0.7729876262	low quality
0.7727206803	target language
0.7727080248	hidden units
0.7726835089	pre training
0.7723532360	local minimum
0.7721062524	neural language models
0.7719331119	semantic embedding
0.7719160429	cnn features
0.7718439245	rank matrix
0.7718126590	binary variables
0.7716918123	synthetic aperture
0.7716163478	vision systems
0.7716057650	feature representations
0.7712046485	kernel methods
0.7711470331	data analytics
0.7710508272	spatial relationships
0.7708291071	unifying framework
0.7707744233	single pixel
0.7707545122	processing units
0.7707150046	visual representations
0.7704703614	approximate bayesian
0.7702435571	weight update
0.7702251925	deep architecture
0.7701243781	parameter free
0.7700480012	depth image
0.7696204114	locality sensitive
0.7693291534	image understanding
0.7692852327	point process
0.7690984580	language identification
0.7689797317	invariant features
0.7688628990	fold cross validation
0.7686702424	sparse signals
0.7685920766	object interactions
0.7680343805	dynamic texture
0.7675123780	density function
0.7671988151	performance evaluation
0.7670116521	web based
0.7670039991	imbalanced data
0.7668827770	fine grained classification
0.7667552885	mri images
0.7666969609	causal effects
0.7666787084	key points
0.7662241191	convergence speed
0.7660698911	higher level
0.7659792402	word vector
0.7655921361	buffet process
0.7653765295	graph kernels
0.7653325497	markov chain monte
0.7652263124	density estimator
0.7646079620	error rates
0.7645307499	hand crafted features
0.7644161660	image database
0.7641605974	newton method
0.7641226854	sequence modeling
0.7640174285	ensemble learning
0.7639384053	fuzzy clustering
0.7636429754	sat instances
0.7636032540	fully bayesian
0.7635234611	distance function
0.7634857004	computer aided diagnosis
0.7631969519	mechanical turk
0.7630949834	image recognition
0.7628029736	alternating direction
0.7626686896	multivariate time series
0.7626083470	tree search
0.7624599047	metric space
0.7623825749	geometric structure
0.7623781874	graph based semi supervised
0.7622432583	deep generative
0.7621084341	k nearest neighbor
0.7620973792	free text
0.7619584207	information bottleneck
0.7619353043	general purpose
0.7618880261	neural networkarchitecture
0.7616124309	set theory
0.7612600435	context specific
0.7610636215	set programming asp
0.7606268757	self driving cars
0.7605463105	support vectors
0.7597687206	stochastic search
0.7596683253	deepneural network
0.7592150986	deepconvolutional neural networks
0.7591221396	lstm rnn
0.7587154558	global context
0.7586949913	linear models
0.7586542189	deep convolutional neural networks cnns
0.7584928764	clustering algorithm
0.7584095884	objective functions
0.7582596879	spiking neural
0.7581782622	kernel hilbert space
0.7573811249	feature spaces
0.7573290005	scale space
0.7567934195	source domain
0.7565945795	graph signals
0.7560987176	hidden layer
0.7560735327	optimization problem
0.7560370685	multi armed bandit problem
0.7560297964	information theory
0.7559848057	kernel matrices
0.7556846767	probabilistic modeling
0.7555239293	energy function
0.7553680151	object boundaries
0.7552327714	minimization problems
0.7551995508	prediction error
0.7551906541	human robot
0.7550198431	regression models
0.7547931861	semantic representation
0.7546638389	visual quality
0.7545026668	energy based
0.7544511185	search algorithm
0.7544485982	multi labelclassification
0.7543317925	weighted graph
0.7542226014	user item
0.7538624057	hand crafted
0.7536645995	data set
0.7536240177	convex optimization problems
0.7533796280	non negative matrix factorization
0.7532424782	linear constraints
0.7531665612	global minimum
0.7531560599	discriminative features
0.7527910796	smooth convex
0.7522861548	neural network cnn
0.7522078811	random matrix
0.7520546884	making decisions
0.7520359426	video segmentation
0.7516321139	object classes
0.7514664856	policy optimization
0.7512882843	deep learning architectures
0.7511380114	ontology based
0.7510757322	feature vector
0.7510533867	adversarial network gan
0.7510381423	sparse approximation
0.7509563112	training samples
0.7508060851	echo state
0.7502385623	mathematical model
0.7501860506	partial information
0.7499978509	ultrasound images
0.7498303810	monte carlo methods
0.7497804797	data sources
0.7496971302	multi agent reinforcement learning
0.7496527968	linear algebra
0.7490835267	information flow
0.7490410408	convolutional features
0.7489346840	low contrast
0.7488193488	carlo tree search
0.7487298227	temporal dependencies
0.7487190842	image quality
0.7482139132	x ray
0.7481681571	feature points
0.7480365762	inartificial intelligence
0.7479908049	dimensional space
0.7479607301	multi output
0.7478652139	machine learningalgorithms
0.7475762680	data integration
0.7475267941	computational complexity
0.7474648831	static images
0.7473684841	approximation error
0.7473352933	deep features
0.7472951350	state spaces
0.7472196781	binary classifier
0.7471169686	robust optimization
0.7470726561	categorical data
0.7469507254	parameter settings
0.7469348527	penn treebank
0.7465699493	sparse codes
0.7461990200	temporal evolution
0.7460989389	cluster analysis
0.7460958514	text processing
0.7460262523	visual features
0.7458697067	perceptron mlp
0.7457899295	massive data
0.7455604390	minimum number
0.7455501051	feature level
0.7455202822	posteriori map
0.7452709419	adversarial networks gans
0.7452240687	initial conditions
0.7451893626	combinatorial optimization problems
0.7451506020	graph clustering
0.7448562448	feature representation
0.7446035712	answer questions
0.7443023244	statistical estimation
0.7442246021	wavelet based
0.7437757389	multiple modalities
0.7435155549	human behavior
0.7433024503	matrix factorization nmf
0.7431377770	allocation lda
0.7429747949	pattern classification
0.7427257150	low frequency
0.7426511653	solution quality
0.7426500342	hand written
0.7425674125	k nearest neighbors
0.7425010108	translation quality
0.7424469664	small objects
0.7423250972	human intervention
0.7422571333	expressive power
0.7421553326	entity recognition
0.7420630583	decision making process
0.7420569551	multi class classification
0.7419890732	chain monte carlo mcmc
0.7419343722	vision applications
0.7417571901	causal models
0.7411871969	color image
0.7411083280	source language
0.7410238522	attention based
0.7406077566	performance metrics
0.7405393807	single pass
0.7404625804	physical world
0.7398939396	spectral spatial
0.7398100610	deep convolution
0.7396464332	pose variations
0.7393991541	decision processes
0.7392012363	vector machine svm
0.7391967817	appearance based
0.7389900302	average precision
0.7385993816	light weight
0.7383815842	acoustic model
0.7376223984	control policies
0.7374404910	semi supervisedlearning
0.7374098457	first order logic
0.7371993087	projected gradient
0.7368579619	causal structure
0.7367894286	image descriptors
0.7366566854	vector machines
0.7364905223	nlp tasks
0.7361484809	shafer theory
0.7360389811	future directions
0.7357723353	neuroimaging data
0.7356442659	continuous state
0.7354935973	self organizing maps
0.7351848626	frame level
0.7347021310	zero shot
0.7340445482	action detection
0.7338557601	embedding space
0.7337417966	binary patterns
0.7335541069	shape analysis
0.7335207968	object discovery
0.7334939788	point set
0.7334359024	user friendly
0.7333671522	optimization algorithm
0.7330030955	validation set
0.7328129393	language processing nlp
0.7326310310	class classification
0.7325286735	factor graph
0.7321337895	high dimensional space
0.7318171208	sensing cs
0.7317571859	sequential decision
0.7314829885	statistical learning
0.7311854989	reference image
0.7308669414	random field crf
0.7306051633	model free
0.7303230485	kernel based
0.7299509363	dependency graph
0.7299418782	extremely large
0.7299121942	noisy images
0.7298943104	planning problems
0.7298025800	deeper understanding
0.7296140910	video classification
0.7295194128	adversarial nets
0.7294761999	general theory
0.7292168500	human beings
0.7291668349	hyperspectral data
0.7291460293	text recognition
0.7290293950	observable markov decision processes
0.7285204846	invariant feature
0.7283291915	action units
0.7280695908	deep metric learning
0.7280316653	margin based
0.7279813497	classification accuracy
0.7278492970	weight matrix
0.7275532177	network architectures
0.7274997173	computational cost
0.7273776720	eeg data
0.7257985694	hmm based
0.7255786304	generated samples
0.7255743260	temporal reasoning
0.7254235156	object level
0.7253374403	layer wise
0.7251914650	closed form solution
0.7250318555	latent feature
0.7249412876	non local means
0.7248982906	latent dirichlet
0.7241511001	point processes
0.7240348601	finite sample
0.7239959603	image representation
0.7238425218	parameter selection
0.7234867030	small sample
0.7234168984	cross entropy loss
0.7232888285	tree based
0.7231474364	dnn based
0.7230273929	approximation algorithms
0.7230094046	gp regression
0.7229275990	multi label learning
0.7227660152	formal framework
0.7225435942	face reconstruction
0.7225082529	sparse signal
0.7223051204	memory footprint
0.7222514003	fine tune
0.7222296022	speech recognition asr
0.7222017446	dynamic systems
0.7218956026	output space
0.7217338196	object parts
0.7214558975	user behavior
0.7213429877	class labels
0.7212012085	graph structure
0.7211782733	evolution strategy
0.7209411222	joint distribution
0.7206231138	latent tree
0.7206015553	large graphs
0.7203494012	prior information
0.7202129750	decision problems
0.7198913151	network structure
0.7196066429	visual reasoning
0.7194932492	linear functions
0.7194009545	case study
0.7192824324	deep reinforcement
0.7191567901	predictive power
0.7191428338	stochastic gradient methods
0.7190139070	visual object
0.7188499517	decision making problems
0.7187131390	object class
0.7186111942	map estimation
0.7184697246	entropy based
0.7181904822	intensive care
0.7178208881	vector representation
0.7175545797	local features
0.7175276937	agent based
0.7174120553	tomography ct
0.7169678357	traveling salesman
0.7169603238	tumor segmentation
0.7169391008	e commerce
0.7165540913	highly scalable
0.7164025546	theoretical properties
0.7163611087	facial features
0.7163095645	deep convolutional neural
0.7161395700	historical data
0.7160749855	video analysis
0.7159681953	sampling scheme
0.7155446470	rule mining
0.7154536528	constraint based
0.7153679354	multipliers admm
0.7153011541	highly correlated
0.7143725982	information fusion
0.7143345298	neural architecture
0.7142674189	large scale video
0.7142338901	training data
0.7138379068	image pairs
0.7134957855	human machine
0.7132723875	distributed representation
0.7127948557	description length
0.7127070320	lstm network
0.7123671100	word representation
0.7121790200	optimal regret
0.7118112198	convolutional recurrent
0.7115906780	deep q network
0.7115701539	error reduction
0.7115548403	divide and conquer
0.7115154725	square error
0.7115057531	acoustic features
0.7114687685	u net
0.7114571839	recognition accuracy
0.7112465307	text to speech
0.7110269663	kernel learning
0.7109140418	robust subspace
0.7107444790	task specific
0.7106297797	deep convolutional network
0.7104863516	human vision
0.7104059711	spectral methods
0.7102934093	state space models
0.7102695500	component analysis pca
0.7100619696	computationally intensive
0.7100511653	vector machines svm
0.7089587550	linear combination
0.7088433944	kernel functions
0.7083975435	fine grained image
0.7082403128	data acquisition
0.7081887867	machine translation systems
0.7080888635	complexity bounds
0.7075038763	asr systems
0.7075025711	visual semantic
0.7070342028	ct image
0.7068458996	greedy algorithms
0.7062830839	optimization techniques
0.7061678719	theoretical guarantees
0.7059973669	road network
0.7059013149	mri data
0.7057093470	well founded semantics
0.7056812719	transition probabilities
0.7054385686	machine learning algorithms
0.7053876532	algorithmic framework
0.7053057159	brain images
0.7052970020	self paced
0.7051992944	mean squared error
0.7050398533	linguistic features
0.7050308975	semantic information
0.7046042702	highly accurate
0.7045442412	training set
0.7045420463	high dimensional data
0.7041343841	clustering algorithms
0.7040291026	image representations
0.7035519028	input space
0.7031177264	multiple languages
0.7028086914	gradient based
0.7026048805	gan training
0.7024987545	weighted sum
0.7020541323	finite set
0.7018546926	object pose
0.7016632972	relative error
0.7013872555	structured data
0.7013489794	statistically significant
0.7013216376	twitter data
0.7012581778	high resolution images
0.7011963516	imagenet classification
0.7011804802	highly efficient
0.7008699588	data dependent
0.7006511093	experimental comparison
0.7006403535	temporal information
0.7004715663	text document
0.7003593214	learning rate
0.7002137513	high performance
0.6994484393	statistical modeling
0.6993442215	real life
0.6993293091	spatial reasoning
0.6991744956	alow dimensional
0.6990095984	network architecture
0.6988312625	gpu based
0.6987760305	self organizing
0.6986675305	action classification
0.6986112050	noisy observations
0.6984871646	mean square error
0.6978030648	zero shot learning
0.6977356622	deep structured
0.6976173142	face image
0.6973092756	neighbor search
0.6972610112	textual data
0.6969250816	kernel function
0.6968103577	model averaging
0.6967777307	back propagation
0.6966839836	forhigh dimensional
0.6966737550	expert knowledge
0.6960585231	single label
0.6960419609	computationally expensive
0.6960401653	training examples
0.6959668189	expected reward
0.6959082239	structured sparse
0.6955653976	control problems
0.6951275948	target domain
0.6949916519	similarity matrix
0.6946323475	boosting algorithms
0.6946009456	bleu points
0.6945442122	aided diagnosis
0.6942531175	feedforward neural
0.6941872068	latent semantic
0.6939018342	hamiltonian monte
0.6937644132	computational models
0.6937146467	k means clustering
0.6936419796	frame based
0.6935933439	linear classifier
0.6931135137	optimization algorithms
0.6931111210	directed acyclic
0.6930524271	heuristic algorithms
0.6928276121	random search
0.6927677620	text corpus
0.6924854047	image content
0.6922219670	geometric features
0.6921045333	functional data
0.6920143380	distance based
0.6917948376	multi target
0.6917021530	empirical comparison
0.6914819460	visual representation
0.6912279899	video prediction
0.6908978445	change point
0.6907477205	text detection
0.6906787588	spectral graph
0.6906775215	speed ups
0.6905456971	image matching
0.6904908538	large variations
0.6904672530	parameter space
0.6896365657	canonical correlation
0.6896063590	objective optimization
0.6888841100	label space
0.6888496845	temporal context
0.6884728955	sense disambiguation
0.6881065049	deep representations
0.6880143461	image resolution
0.6877617737	look ahead
0.6874836204	penalty term
0.6874223056	human face
0.6872169283	wireless sensor
0.6870564354	text analysis
0.6869429003	colony optimization
0.6868071342	deep convolutionalneural
0.6866283660	prediction accuracy
0.6866219571	generalization performance
0.6865559482	hierarchical dirichlet
0.6864754650	structure discovery
0.6863831499	structure learning
0.6860389019	stationary points
0.6860172766	selection strategy
0.6856508241	reinforcement learning algorithms
0.6855615818	combinatorial problems
0.6854676404	consecutive frames
0.6853318655	l bfgs
0.6851096748	high accuracy
0.6850220261	update rule
0.6849820258	binary decision
0.6849815830	memory networks
0.6846605864	svm based
0.6845469602	multiple tasks
0.6843615516	boosting algorithm
0.6840776074	confidence bound
0.6840611915	rnn based
0.6840562997	learning rule
0.6837064695	test cases
0.6836295936	unmanned aerial
0.6833759924	success rate
0.6831488136	image transformation
0.6830732236	multiple objects
0.6830356338	self organising
0.6827314384	visual appearance
0.6826326791	aperture radar
0.6825449334	cnn based
0.6825323087	decision process mdp
0.6821227639	decision problem
0.6820715816	manually annotated
0.6819137148	target detection
0.6818128601	adversarial network
0.6817041536	least squares
0.6816241388	near infrared
0.6815978814	noisy data
0.6815034884	iterative algorithm
0.6812752882	feature based
0.6809982596	biological systems
0.6804105994	machine learning systems
0.6803124389	whole slide
0.6802527236	deep convolutional neuralnetworks
0.6799214748	decision processes mdps
0.6796194705	markov model
0.6788111275	neural network based
0.6783178251	face shape
0.6780205791	sensitive hashing
0.6778023963	feature set
0.6777108189	video understanding
0.6776997728	special cases
0.6774597519	mathematical models
0.6771320186	multiple levels
0.6769731138	multi view learning
0.6768692426	gradient method
0.6767400847	efficient algorithms
0.6762846071	document images
0.6758570896	semantic content
0.6756199318	translation task
0.6749641454	single objective
0.6747560729	variational methods
0.6745173309	coarse to fine
0.6744336295	language pairs
0.6743016433	theoretical analysis
0.6741888697	attention model
0.6738632543	model based
0.6735814512	means clustering algorithm
0.6732911786	event based
0.6732113770	multiple output
0.6732053296	image search
0.6731911617	saliency models
0.6726365414	manual annotation
0.6725388717	upper bounded
0.6723561572	relational learning
0.6723249141	dimensional vector
0.6719672208	hidden layers
0.6719421723	output layer
0.6719106806	feature selection method
0.6719055100	semantic labels
0.6716641165	finite dimensional
0.6716192838	human annotated
0.6713139417	step sizes
0.6710139738	cnn architecture
0.6709843501	self organization
0.6708019183	logic based
0.6706748774	signal to noise ratio
0.6700021272	detection rate
0.6696225051	matrix recovery
0.6696207437	sampling strategy
0.6694954140	latent features
0.6692738586	experimental study
0.6692287460	images captured
0.6684639704	comparative study
0.6684639538	qualitative analysis
0.6684192230	selection problem
0.6682834972	complex network
0.6679581595	memory usage
0.6678364676	class label
0.6676730606	recent developments
0.6674822121	data compression
0.6665972313	preliminary results
0.6662908816	reasoning systems
0.6659969760	web images
0.6659238709	based image retrieval
0.6658074613	distributed optimization
0.6654679311	closely related
0.6653439736	aerial vehicles
0.6653207193	ranking based
0.6652177355	large data sets
0.6649242308	temporal structure
0.6648142972	high resolution image
0.6647230440	foreground objects
0.6646782114	ahigh dimensional
0.6644330795	regularization term
0.6638084778	classification error
0.6635099024	experimental evaluation
0.6634280559	mean shift
0.6631708676	end to end trainable
0.6631603558	case studies
0.6626992804	learned features
0.6624157799	processing unit
0.6619355158	least squares regression
0.6618712517	high variance
0.6608281070	character based
0.6607196332	gradient methods
0.6603633577	k nn
0.6601775956	human level
0.6599633532	incomputer vision
0.6599294304	optimal solution
0.6598807158	multiple agents
0.6598177944	machine learning methods
0.6594119520	non stationary
0.6593585509	stochastic block
0.6590220333	inrecent years
0.6589250566	hashing methods
0.6586217227	point detection
0.6583327810	compares favorably
0.6581840366	test set
0.6581046721	information criterion
0.6573871251	inner product
0.6567031453	r package
0.6566709063	user interaction
0.6562812360	real worldapplications
0.6562375680	gradient descent sgd
0.6562307599	joint optimization
0.6557007806	open problems
0.6556164418	supervised machine learning
0.6555326407	multiple kernel
0.6552849514	fuzzy inference
0.6550240511	markov network
0.6549587501	labeled examples
0.6548156615	armed bandit problem
0.6547323967	deep learning algorithms
0.6546715146	internet of things
0.6544955014	embedding models
0.6544026844	surrogate model
0.6541561822	performance measure
0.6538735851	value decomposition svd
0.6536759662	pascal voc 2012
0.6535625442	numerical simulations
0.6534902056	alternating direction method of multipliers
0.6534175550	deep learning models
0.6527120324	correlation analysis
0.6527086330	query image
0.6526016117	local structure
0.6520359526	tensor based
0.6520135922	empirical studies
0.6519976681	information sources
0.6518511046	alternating direction method of multipliers admm
0.6518451171	n gram
0.6517097482	landmark detection
0.6516916309	subspace learning
0.6514151537	image pixels
0.6512879518	mathematical framework
0.6512666918	error detection
0.6510576774	power method
0.6509800913	joint learning
0.6507528587	image dataset
0.6504672480	k means
0.6502417656	machine learning models
0.6497100851	dictionary based
0.6496874278	satisfaction problems
0.6496829170	semantic analysis
0.6483895604	optimization framework
0.6480878622	sufficient conditions
0.6478373292	constant factor
0.6476443064	fuzzy inference system
0.6476148166	regression problem
0.6475271032	experimental resultsdemonstrate
0.6473404289	camera views
0.6473373508	posterior probability
0.6471346698	orders ofmagnitude
0.6469540687	rnn model
0.6469189630	experimental resultsshow
0.6469124790	knowledge based
0.6466463325	performance improvement
0.6465172611	inreal world
0.6462855067	medical data
0.6462131122	video representation
0.6460890289	mnist dataset
0.6459417592	human expert
0.6458986965	sparse linear
0.6458785941	quantitative evaluation
0.6458095998	neural network model
0.6458024265	co occurrence
0.6457518940	posterior distributions
0.6456906217	inmachine learning
0.6455942995	mean square
0.6452608553	aconvolutional neural network cnn
0.6452111818	fully supervised
0.6451994754	feature selection methods
0.6451983418	prediction tasks
0.6451307209	architecture search
0.6450293815	prediction models
0.6449130284	structural information
0.6444451745	training sets
0.6443583366	vast majority
0.6443561372	multiple scales
0.6442867543	synthetic andreal
0.6441503252	upper and lower bounds
0.6440684701	local feature
0.6439791537	learning algorithms
0.6438564833	multi layered
0.6437561033	bandit algorithms
0.6434620138	automatic evaluation
0.6432388412	state estimation
0.6431330223	number ofparameters
0.6429643054	video dataset
0.6426254118	machine learning approaches
0.6424597282	memory consumption
0.6422662631	degrees of freedom
0.6422264643	recognition rate
0.6419860143	mean field
0.6414893461	term memory
0.6414830948	determinantal point
0.6408542954	generated images
0.6407826660	unified framework
0.6405527053	predictive model
0.6402388992	chain monte carlo
0.6399967920	non monotonic
0.6399518333	dynamic networks
0.6398996085	information content
0.6398961805	discriminative models
0.6397264972	large scale image
0.6396146906	inductive logic
0.6396079637	learning machines
0.6395626252	noise levels
0.6395159741	pre trained word
0.6392982497	squared error
0.6392978222	jointly trained
0.6392435496	linear model
0.6391705727	united states
0.6389078476	comparative analysis
0.6387482976	network embedding
0.6387078231	inter class
0.6386141884	neural networkcnn
0.6386065066	posterior inference
0.6385506447	neural network rnn
0.6385216767	jointly learning
0.6384889209	gan based
0.6383301905	front end
0.6383031248	distributed stochastic
0.6382858989	graph representation
0.6382619878	vector machines svms
0.6381108136	markov model hmm
0.6380945247	primal dual
0.6379197770	unsupervised methods
0.6377811188	answering vqa
0.6377750628	neural machine
0.6375888619	data points
0.6375875392	deep recurrent neural networks
0.6374973678	value iteration
0.6374668790	structural properties
0.6374271114	immune systems
0.6373311373	hybrid approach
0.6372038924	carlo mcmc
0.6371535322	semantic space
0.6369294321	statistical properties
0.6368579424	single task
0.6366577635	off policy
0.6364254214	computing systems
0.6362925010	few shot learning
0.6360202100	pixel based
0.6359604134	principled approach
0.6354933400	likelihood function
0.6349560079	deep feature
0.6349018678	depth information
0.6347522133	multi armed
0.6346991744	gradient descent algorithm
0.6344706016	spike and slab
0.6339734541	observed variables
0.6337867762	approximate nearest
0.6337004929	onreal world
0.6336174305	deep learning based
0.6335854367	prior distribution
0.6335786857	probabilistic model
0.6325969020	fusion method
0.6325183245	andreal world
0.6322727123	human experts
0.6319845453	wide web
0.6317635667	higher dimensional
0.6316476346	least square
0.6315965085	sampling algorithms
0.6314596518	state representation
0.6310404659	monte carlo mcmc
0.6309242207	sample efficiency
0.6307087317	policy learning
0.6303356664	lstm model
0.6303194650	attention models
0.6300907220	posterior distribution
0.6298490431	true distribution
0.6297731401	swarm optimization
0.6296302134	large datasets
0.6291929253	point wise
0.6286476922	bayesian framework
0.6285952398	computer vision
0.6285761558	alarge scale
0.6283585778	optimal policy
0.6283582151	deep convolutional neural network cnn
0.6283343029	function evaluations
0.6281548099	learning theory
0.6275123371	routing problem
0.6274173355	computational requirements
0.6271351036	pooling layer
0.6266300653	attention network
0.6263375792	robust estimation
0.6262132069	classification problems
0.6261822883	adversarial learning
0.6260016098	noise level
0.6254703720	embedding vectors
0.6253833533	time series
0.6252758821	unseen data
0.6252753788	recognition rates
0.6251342871	optimization methods
0.6251210915	pascal voc 2007
0.6248696579	bag of words
0.6247777259	semantic features
0.6245972607	detection algorithms
0.6245787634	recognition systems
0.6243367145	mnist cifar
0.6241334913	human subjects
0.6235876983	self supervised
0.6229598632	image acquisition
0.6228461100	memory network
0.6224355257	decomposition based
0.6224001112	recognition performance
0.6222555224	main contributions
0.6221789345	search algorithms
0.6218610581	tothis end
0.6217984092	data structures
0.6216223641	decision support system
0.6211667485	electronic health
0.6210426750	high probability
0.6209566227	improved performance
0.6208668350	discrete wavelet
0.6206160274	discrete optimization
0.6200582067	conditional distribution
0.6196546274	real worlddatasets
0.6196309524	error analysis
0.6192225792	corpus based
0.6192004619	empirical evaluation
0.6191249835	online optimization
0.6189827293	source sentence
0.6185189640	online learning algorithms
0.6184721165	large sample
0.6181503511	linear dynamical
0.6179532769	empirical analysis
0.6178649414	stationary point
0.6178063594	rgb d
0.6177550762	basis function
0.6176511102	real worlddata
0.6176338137	state tracking
0.6175544617	coco dataset
0.6171169401	rgb d images
0.6170256268	deep learning techniques
0.6169936229	selection algorithm
0.6169702243	higher quality
0.6165555939	communication cost
0.6165054397	appearance model
0.6163588997	noisy image
0.6162201035	target object
0.6157607132	robust principal component
0.6154842621	vocabulary size
0.6152572390	ground truth data
0.6149344452	rank approximation
0.6148372779	optical coherence
0.6148299830	deep models
0.6145852359	high dimensional datasets
0.6145688294	efficient distributed
0.6144816213	predictive accuracy
0.6143722193	parallel data
0.6142652608	computational model
0.6140950187	areal world
0.6139103841	complexity analysis
0.6138909955	pooling layers
0.6135123658	r cnn
0.6134518646	semantically meaningful
0.6131773455	bayesian approach
0.6123177151	statistical models
0.6122019020	approximation algorithm
0.6120839148	spatial features
0.6120768367	detection methods
0.6120357556	vice versa
0.6119191204	widely adopted
0.6118694541	feed forward neural
0.6117654197	lower dimensional
0.6112268863	learning rl
0.6110228737	synthetic dataset
0.6106906872	automatic speech
0.6105516104	anend to end
0.6101985151	pre trained cnn
0.6100841970	parameter values
0.6100451116	few shot
0.6099837762	analysis tools
0.6099035956	near optimal
0.6094814869	classification models
0.6092146990	image based
0.6092036657	hierarchical model
0.6091087759	attention maps
0.6087613796	mild assumptions
0.6086329195	large scale datasets
0.6079441299	visual data
0.6079381922	row and column
0.6078578055	video based
0.6077447956	extreme learning
0.6077248501	hierarchical classification
0.6067390343	parametric models
0.6066790558	computational framework
0.6066754798	non rigid
0.6066236600	feature detection
0.6064074017	learning mechanism
0.6059361452	linear convergence rate
0.6055124700	heterogeneous data
0.6054938271	neural network models
0.6054864005	training deep neural networks
0.6052007959	relational models
0.6050855712	automatic classification
0.6049479268	empirical study
0.6046293698	input features
0.6045450641	non convex optimization
0.6043712642	lower level
0.6042942356	means clustering
0.6037233746	hand engineered
0.6036459596	initial state
0.6030425780	previous works
0.6028710932	an empirical study
0.6026238624	quantitative analysis
0.6024088489	starting point
0.6024059414	population based
0.6023809657	empirical evidence
0.6022382245	peak signal to noise
0.6018299118	dual coordinate
0.6017444524	large scale machine learning
0.6014938931	benchmark problems
0.6014467740	pre processing step
0.6012414044	visual information
0.6006976587	deep learning approaches
0.6006364904	optical character
0.6005442634	traffic data
0.6003524816	pre trained deep
0.6002664975	spatial information
0.5995271741	sum of squares
0.5994865779	inference problems
0.5994539110	performs favorably
0.5992888644	labeled and unlabeled
0.5990132965	performance analysis
0.5988513821	learning paradigm
0.5988370552	pixel values
0.5984297588	manifold structure
0.5983479558	based approach
0.5982538935	translation systems
0.5980258696	new york
0.5979715128	selection method
0.5975948363	hierarchical reinforcement
0.5975125598	estimation of distribution
0.5973086663	computer aided
0.5970886994	bayes classifier
0.5969269370	singular value
0.5968599734	model counting
0.5968296102	neural networks rnns
0.5968028725	n grams
0.5966141272	convergence results
0.5965837414	optimization method
0.5959544104	depth data
0.5956646882	classification algorithms
0.5956066296	raw data
0.5955838519	significant improvements
0.5952523496	low level features
0.5948222281	machine learning applications
0.5946943206	neural models
0.5945553366	information systems
0.5945283354	k svd
0.5945165147	search procedure
0.5943279979	matching algorithm
0.5942184873	clustering approach
0.5942108990	accurate predictions
0.5941823687	relevant features
0.5938184959	an open source
0.5935811615	self organized
0.5930353662	research directions
0.5929180162	image set
0.5927151597	ofmachine learning
0.5922998841	complexity results
0.5922043429	translation tasks
0.5920815174	regression problems
0.5919889798	web data
0.5918313531	sparsity based
0.5914201381	training procedure
0.5911325353	mean average precision
0.5911134390	word recognition
0.5910867828	resonance imaging
0.5909398502	memory requirements
0.5907624810	previously unseen
0.5901501809	network analysis
0.5901495486	high dimensionality
0.5896874763	semantic structure
0.5895552355	learning algorithm
0.5893195800	related tasks
0.5889324405	memory based
0.5887065446	lstm based
0.5886312508	similarity based
0.5885321334	bleu score
0.5884442847	fully convolutional neural
0.5883931735	deep learning approach
0.5880451680	generalized linear
0.5879262394	entire image
0.5877555945	branch and bound
0.5877443243	iterative algorithms
0.5873425165	t sne
0.5871884506	convex function
0.5871754599	random features
0.5870447839	text based
0.5869140073	conditional generative
0.5868629171	model uncertainty
0.5866768597	motion features
0.5864941715	inference algorithms
0.5860989943	benchmark datasets
0.5859939992	computer graphics
0.5854745781	bayesian models
0.5853248426	rapid growth
0.5853055077	pattern based
0.5852105330	automatic detection
0.5851860844	input sequence
0.5851207415	priori knowledge
0.5848447788	problem instances
0.5842844066	human evaluation
0.5840796634	extensively studied
0.5839644462	efficient online
0.5839456711	discrete data
0.5837980275	relative improvement
0.5837949955	based reasoning
0.5837890182	clustering technique
0.5835974364	human language
0.5832940807	significantly improves
0.5831709052	recent progress
0.5831431802	benchmark functions
0.5825640892	de noising
0.5823681046	co occurrences
0.5822646719	theproposed algorithm
0.5818590472	strong baselines
0.5817612255	noise free
0.5817321496	trial and error
0.5817304216	non parametric
0.5810726103	update rules
0.5808759937	series of experiments
0.5806686624	text data
0.5806571698	segmentation algorithm
0.5801461224	organizing maps
0.5795315736	classification tasks
0.5789542854	second order
0.5788578106	planning problem
0.5788484919	negative examples
0.5785811741	error prone
0.5783812238	parameter learning
0.5781032164	maximization em algorithm
0.5780286021	machine learning approach
0.5779914533	kernel hilbert spaces
0.5779362399	achieved great success
0.5779338329	tens of thousands
0.5778968612	fixed length
0.5777847597	learning based
0.5776104332	labeling problem
0.5767922563	linear optimization
0.5765644388	theproposed method
0.5763364009	simple linear
0.5762357442	hybrid model
0.5761669837	deep learning framework
0.5760039394	imaging data
0.5758560799	parametric model
0.5753223088	thecomputational complexity
0.5752230322	test sets
0.5750195031	learning framework
0.5749031490	discriminative model
0.5748286872	side information
0.5746490888	re id
0.5743459549	one shot
0.5740846622	connected layers
0.5736578985	based approaches
0.5735222310	annotated data
0.5731093183	cifar 100
0.5727024151	supervised semantic segmentation
0.5726768228	a comprehensive survey
0.5725073344	theoretical framework
0.5720475146	maximization em
0.5720374524	filter based
0.5718694769	health records
0.5718086880	variation tv
0.5715779194	collected data
0.5714874728	optimal number
0.5714284935	stochastic process
0.5713359014	sparse data
0.5712800999	important features
0.5711873582	experimental analysis
0.5711018978	language specific
0.5710863801	real world data
0.5710203281	theexperimental results
0.5706539822	sensor network
0.5705591056	image sequence
0.5703141369	significantly outperforms
0.5702249618	tracking algorithm
0.5698928710	graph signal
0.5691185018	label information
0.5689779663	low dimensional space
0.5687903618	multi view data
0.5683183449	bayesian learning
0.5682904791	learning approach
0.5680297576	context information
0.5679105879	predictive performance
0.5677712924	sampling methods
0.5677271309	single images
0.5676622674	q learning
0.5675720599	regression model
0.5675101001	labeled training data
0.5671219364	level features
0.5666853347	machine translation nmt
0.5666593885	point based
0.5659269654	widely studied
0.5658959697	mean absolute
0.5657040073	segmentation methods
0.5655863978	attention networks
0.5650342088	level labels
0.5648786918	translation nmt
0.5644944894	network design
0.5642162702	application areas
0.5639127222	cnn models
0.5638874046	thispaper presents
0.5637571162	analysis cca
0.5636682393	spatial domain
0.5636484453	prediction model
0.5634236758	temporal data
0.5630166972	multi document
0.5627184051	variational auto
0.5626721171	relevant information
0.5623675131	fewer parameters
0.5623212276	field theory
0.5619568058	prediction problems
0.5617168514	inference methods
0.5616389530	related problems
0.5615785725	fmri data
0.5614681706	practical applications
0.5613609261	thispaper proposes
0.5609048337	joint model
0.5606695740	clustering techniques
0.5606602538	layer perceptron
0.5605411820	matching problem
0.5604629368	illumination conditions
0.5603884868	training algorithm
0.5603694021	optimisation problems
0.5602067412	self adaptive
0.5601810576	statistical model
0.5596070455	facial image
0.5591612188	recent years
0.5589226735	human visual system
0.5587439194	symmetric positive
0.5585326551	inference procedure
0.5583751714	orders of magnitude
0.5583329756	descent algorithm
0.5581887483	deep convolutionalneural networks
0.5576256960	aneural network
0.5573967818	inthis paper
0.5573082949	computational burden
0.5571656509	causal model
0.5570973709	acyclic graph
0.5567899223	numerical experiments
0.5567694734	neural networks cnn
0.5559863511	syntactic information
0.5556386411	k nearest
0.5555836118	level annotations
0.5552846239	learning task
0.5552729167	convex loss
0.5551734744	significantly improve
0.5550890306	bayesian model
0.5547526014	f score
0.5546128308	based clustering
0.5545651919	f measure
0.5545614901	scales linearly
0.5541822299	computational power
0.5540352159	efficient training
0.5539398717	carefully designed
0.5538391546	video data
0.5534764254	graphics processing
0.5532493104	time varying
0.5528059966	logarithmic factors
0.5527352420	model compression
0.5525859989	recommender system
0.5524565037	image regions
0.5523112887	neural networks anns
0.5521335254	manually labeled
0.5520313284	structure from motion
0.5518841588	stochastic variational
0.5518587686	computational resources
0.5513684528	recent works
0.5512406703	computational efficiency
0.5511064692	conceptually simple
0.5510509063	non negative matrix
0.5509074836	experimental data
0.5508830725	segmentation performance
0.5508589086	conduct extensive experiments
0.5507086017	computer assisted
0.5503950861	estimation error
0.5503927492	target image
0.5503271421	founded semantics
0.5502266496	takes place
0.5498066035	deep learning methods
0.5497795551	perform poorly
0.5494142124	information loss
0.5494012927	current state
0.5492515799	dimensional subspace
0.5491588123	retrieval task
0.5491199667	limited data
0.5490512543	descent method
0.5488020696	clustering method
0.5487777097	online social
0.5486091498	user defined
0.5478128201	salesman problem
0.5477899080	classification problem
0.5477487765	an information theoretic
0.5476766323	simulated data
0.5473414685	global features
0.5470061356	human performance
0.5463234932	extensive experiments
0.5460257901	easy to implement
0.5458914460	previously published
0.5457009485	self similarity
0.5451486889	sparse models
0.5451459253	unsupervised feature
0.5450821999	entropy loss
0.5448716226	online learning algorithm
0.5443464186	experimental results
0.5431986266	real applications
0.5431447797	mixture of experts
0.5426972849	random noise
0.5424811557	machine learning based
0.5421665626	content based image
0.5420455448	input variables
0.5418988160	document image
0.5416782366	structure prediction
0.5416096643	tracking algorithms
0.5415259858	computer science
0.5412985344	high level features
0.5411956857	decomposition svd
0.5409584826	classification techniques
0.5409348725	reconstructed images
0.5407120546	exploration and exploitation
0.5405066449	robot interaction
0.5402724842	vision research
0.5399966505	face datasets
0.5398264356	statistical methods
0.5397612910	image classification tasks
0.5397472517	classification methods
0.5397394199	mining techniques
0.5396594043	x ray images
0.5394776637	hybrid algorithm
0.5392965294	forward and backward
0.5389729895	matrix approximation
0.5388100423	learned representations
0.5386294825	representations learned
0.5386171352	visual feature
0.5385437872	recurrent convolutional
0.5383989110	latent representation
0.5382019671	reconstruction quality
0.5377318019	long standing
0.5374807750	supervised manner
0.5374302751	background noise
0.5374121808	search problems
0.5373253359	increasingly popular
0.5371184733	complex tasks
0.5370110443	model complexity
0.5366833395	hard problems
0.5366548921	flow estimation
0.5365539475	end to end
0.5362937408	deep boltzmann
0.5360243494	engineered features
0.5358995915	multi object
0.5357785447	experimental evidence
0.5354742424	dirichlet allocation
0.5354240929	ourexperimental results
0.5353490006	answering qa
0.5351702293	non gaussian
0.5349582983	conditional random
0.5348944569	processes mdps
0.5346821443	temporal action
0.5345792526	recent literature
0.5344567846	compact representation
0.5344242088	linear classification
0.5344063788	efficient learning
0.5343544214	network fcn
0.5340483482	recently proposed
0.5330713469	a case study
0.5330607668	neural attention
0.5327115763	top down
0.5326353551	squares regression
0.5325296521	reinforcement learning algorithm
0.5322770334	state ofthe art
0.5321973027	image sets
0.5321511109	statistical information
0.5320397142	first order
0.5320087778	regularization methods
0.5318151912	type methods
0.5316738412	non convex
0.5314558260	accurately predict
0.5313865594	leave one out
0.5313741870	visual recognition tasks
0.5313605977	thetraining set
0.5311442903	translation model
0.5309629925	rl algorithms
0.5307828763	existing methods
0.5306771038	embedding based
0.5306735886	conference on uncertainty
0.5303449597	classification performance
0.5301453654	andmachine learning
0.5296921945	theproposed approach
0.5295815942	stable model
0.5295547622	well founded
0.5290640548	estimation problem
0.5290583560	resnet 50
0.5289252854	excellent performance
0.5288422100	loopy belief
0.5286953552	image features
0.5284633953	attention based neural
0.5283642541	box attacks
0.5283369431	part of speech pos
0.5282501467	first person
0.5282314962	learning rules
0.5282253371	cifar 10
0.5281577078	matrix estimation
0.5280692926	detection algorithm
0.5280630318	paper describes
0.5270086845	input signal
0.5270028146	significant progress
0.5269994595	crafted features
0.5268732934	uniform distribution
0.5266741545	agent systems
0.5263584875	discriminative learning
0.5263452371	strengths and weaknesses
0.5262762197	modeling framework
0.5249981424	test error
0.5248718427	reduction methods
0.5248353658	machines svms
0.5247581407	trained cnn
0.5243325032	curse of dimensionality
0.5242100236	network parameters
0.5240848457	neural network training
0.5240210343	vgg 16
0.5238026975	real world applications
0.5237720974	classifier performance
0.5233606229	input text
0.5232786404	efficient algorithm
0.5222610692	paper presents
0.5222239991	substantial improvements
0.5222047831	flow based
0.5221199964	accurate segmentation
0.5219246152	control problem
0.5218220414	deep representation
0.5212043298	large corpora
0.5211428588	atari 2600
0.5206884227	sample sizes
0.5202412301	multiple datasets
0.5200862619	learning agent
0.5197797817	recently introduced
0.5194093010	existing approaches
0.5193399967	cnn model
0.5192851016	online algorithms
0.5187221323	scene images
0.5182860715	randomly generated
0.5182316757	convex optimization problem
0.5181848180	syntheticand real
0.5178997632	neural model
0.5178556335	training corpus
0.5178319013	thecomputational cost
0.5178159624	learning agents
0.5175261495	based classifiers
0.5174147350	consistently outperforms
0.5173481778	sp theory
0.5168304895	supervised setting
0.5163522289	modeling approaches
0.5159429134	initial results
0.5158343317	brain computer
0.5157680327	online convex
0.5150139506	training instances
0.5149061392	aconvolutional neural network
0.5148325629	self driving
0.5147966672	segmentation method
0.5145683930	clustering based
0.5145438726	state of theart
0.5145262654	test images
0.5143795419	a unified view
0.5142408865	small scale
0.5139491291	two stream
0.5136823441	valued data
0.5130443549	process regression
0.5129003813	sound and complete
0.5125772365	selection methods
0.5123968643	performance guarantees
0.5122816786	theoretical findings
0.5122174442	factorization nmf
0.5116011445	regression methods
0.5114932644	linear function
0.5111308198	similarity learning
0.5098976708	real data
0.5095341916	encoder and decoder
0.5094883056	k means algorithm
0.5091768760	machines svm
0.5089727856	field of view
0.5088712208	open problem
0.5087940353	order logic
0.5086685582	sampling based
0.5084631896	ill posed
0.5082648796	controlled natural
0.5082450170	large networks
0.5082019594	based reinforcement learning
0.5081430336	problems involving
0.5079650072	visually similar
0.5075778254	network structures
0.5075703923	binary data
0.5074091819	stochastic variance
0.5074024868	visual question
0.5072217553	polynomial time algorithm
0.5064771424	state action
0.5064388051	direction method of multipliers
0.5060863560	zero sum
0.5058739831	algorithm selection
0.5058326487	space complexity
0.5057840084	precision and recall
0.5051359284	no regret
0.5050100728	theproposed framework
0.5046105519	local binary
0.5042244352	classification method
0.5041307706	stochastic algorithms
0.5040429759	theoretical results
0.5039246697	large scale problems
0.5032873738	optimal strategy
0.5029521599	retrieval performance
0.5028040295	classification task
0.5027617624	trained end to end
0.5025636264	driven approach
0.5023068410	times faster
0.5022804179	recent studies
0.5020617243	taking into account
0.5013436228	target function
0.5011751601	instance learning
0.5009853798	markov random
0.5009586280	norm based
0.5008928090	robust to noise
0.5007061688	takes into account
0.5006491756	preprocessing step
0.5002768259	time series data
0.5000053233	6 dof
0.4999781074	image to image translation
0.4994830035	side effects
0.4990200023	process models
0.4989179113	reduction techniques
0.4984581155	continuous time
0.4983633823	time series classification
0.4983136521	approximate solution
0.4977289552	extracted features
0.4976326562	deep recurrent neural
0.4975995274	neural network approach
0.4974349968	unsupervised method
0.4971497733	descent algorithms
0.4970138401	large numbers
0.4969510499	embedding model
0.4965462985	superior performance
0.4962777285	labelled data
0.4961582148	person re id
0.4960763586	nearly optimal
0.4959045219	data representation
0.4958049248	mnist cifar 10
0.4954506513	set programming
0.4949702200	words and phrases
0.4949188266	direction method of multipliers admm
0.4947018537	distributed learning
0.4946390721	continuous data
0.4946290516	features extracted
0.4944737735	previous studies
0.4943779120	real time
0.4941632966	advantages and disadvantages
0.4940642825	deep q
0.4940035394	recent successes
0.4938231944	thecurrent state
0.4936397258	time series analysis
0.4932876089	performance improvements
0.4927635611	visual place
0.4927486207	regression function
0.4925959867	rank tensor
0.4922108677	real world datasets
0.4921317790	large scale data
0.4919750265	part of speech tagging
0.4918941514	image data
0.4917519603	great potential
0.4916717465	gaussian random
0.4916340273	joint probability
0.4912366387	resonance imaging mri
0.4912021084	context based
0.4911526060	based face recognition
0.4910686682	probabilistic generative
0.4910639777	dimensional manifold
0.4909363756	expert system
0.4906871839	paper proposes
0.4899189352	functional magnetic
0.4896446378	travel time
0.4896109039	image translation
0.4895754150	based methods
0.4894056518	major challenges
0.4890530243	recognition tasks
0.4890375953	target domains
0.4890340658	real world scenarios
0.4889621142	considerable attention
0.4889151773	qualitative and quantitative
0.4888142517	dimensional embedding
0.4886519290	ensemble method
0.4886417407	recent deep learning
0.4886266110	image text
0.4886141528	theobjective function
0.4884916813	network models
0.4881877754	alternative approaches
0.4878940939	desirable properties
0.4871160112	segmentation network
0.4869983444	deep learning model
0.4867979445	estimation method
0.4862080066	real time applications
0.4861332706	image space
0.4860710226	promising results
0.4859689244	linear unit
0.4859420699	positive and negative
0.4858657689	learning problems
0.4858344557	higher accuracy
0.4858043898	solution space
0.4855293203	imaging mri
0.4855051196	land use
0.4854694527	training process
0.4854495789	speech tagging
0.4851553829	model parameters
0.4851066008	co clustering
0.4848955027	world wide
0.4847818401	single machine
0.4844578283	sampling method
0.4844188113	critical role
0.4843049454	sub pixel
0.4840200106	vector based
0.4833750793	pre defined
0.4833037034	order of magnitude
0.4832519195	search based
0.4828698066	images acquired
0.4824704553	thatthe proposed method
0.4822627082	performance gains
0.4817326364	intelligence ai
0.4817192313	quantitative and qualitative
0.4816955040	original images
0.4815720313	shape model
0.4814299983	previously proposed
0.4814218454	algorithm converges
0.4813964076	based models
0.4812174276	3d point clouds
0.4810636199	learning methods
0.4809925111	value functions
0.4806058443	maximization algorithm
0.4805788272	bottom up
0.4804985559	level information
0.4803105530	sequence to sequence
0.4797822958	factors of variation
0.4795731196	convex problems
0.4794952931	bayesian methods
0.4794601882	address thisproblem
0.4792316475	main result
0.4792088943	data structure
0.4790283476	absolute error
0.4790125883	distributed data
0.4789935842	machine learning tasks
0.4788348334	one pass
0.4784534178	trained network
0.4779419250	key insight
0.4776112185	statistical machine
0.4775856477	polynomial time
0.4771880978	semantic models
0.4770191852	image domain
0.4768163032	resolution images
0.4765977278	two stage
0.4765483969	immune system
0.4762525254	off line
0.4757540043	inference algorithm
0.4754032068	evaluation method
0.4752088321	clustering results
0.4750313778	non negative
0.4746812414	low computational cost
0.4746525396	short termmemory
0.4744397074	vision algorithms
0.4743743673	test accuracy
0.4739198634	point of view
0.4735299273	sampling algorithm
0.4735158858	compared tothe
0.4734363866	proposed method
0.4732681063	integer linear
0.4728233549	ofdeep learning
0.4728130368	powerful tools
0.4726095572	complex data
0.4725074704	specific features
0.4724410639	increasingly important
0.4717196206	a deep learning approach
0.4717010112	estimation problems
0.4716823827	design choices
0.4716078609	segmentation accuracy
0.4712947925	highly competitive
0.4711151701	robust face
0.4708679076	process mdp
0.4706979755	level vision
0.4706881349	possible worlds
0.4706538287	multiple labels
0.4706428863	optimization process
0.4706068167	speech data
0.4703073389	great success
0.4700638311	person re identification re id
0.4699338584	gradient algorithm
0.4698768530	supervised clustering
0.4698556352	tracking methods
0.4693152930	under uncertainty
0.4691655101	input images
0.4690326745	optimization approach
0.4688242721	non deterministic
0.4682920007	theproposed model
0.4679347425	matching methods
0.4673690914	network learns
0.4672902806	segmentation tasks
0.4671884212	automatic generation
0.4671485746	local information
0.4670623171	non uniform
0.4667246545	motion information
0.4663040982	graph convolutional
0.4660172442	lower and upper
0.4659747765	potential applications
0.4657680994	input image
0.4653132498	detection dataset
0.4653100774	local image
0.4651338219	learning based approach
0.4650515857	models trained
0.4649261272	online algorithm
0.4647081839	image databases
0.4641562356	top n
0.4641220527	large collections
0.4639947090	significantly outperform
0.4638574843	optimization pso
0.4632682755	positive rate
0.4628219956	structured learning
0.4627401228	general framework
0.4625067774	simple and efficient
0.4624568265	supervised approach
0.4624109675	article describes
0.4623659854	theoretical bounds
0.4623529756	np hard problem
0.4620154764	single image super
0.4619286780	image similarity
0.4611773696	video datasets
0.4611321101	learning tasks
0.4610375230	tracking performance
0.4606245280	computational effort
0.4603309859	proof of concept
0.4602307243	static and dynamic
0.4594801733	general intelligence
0.4594038244	significantly reduces
0.4592334779	voc 2007
0.4588025680	score based
0.4582086640	thetraining data
0.4577076324	general setting
0.4574029149	computational costs
0.4573003400	annotated datasets
0.4571743003	theoretically and empirically
0.4570727864	fuzzy model
0.4568458740	rank representation
0.4568289019	modal retrieval
0.4565788894	mean squared
0.4563759678	re identification
0.4562004537	future research
0.4559022471	empirical results
0.4556282565	real world data sets
0.4551048378	top k
0.4549597807	linear combinations
0.4547423821	detection techniques
0.4545770029	benchmark data sets
0.4543521045	deep belief
0.4543101879	formal concept
0.4542700870	previous approaches
0.4542114168	peak signal
0.4537360188	widely applied
0.4534454018	real images
0.4533680345	state of art
0.4529959207	chest x
0.4527308631	probabilistic approach
0.4524328970	body part
0.4521919144	unsupervised deep
0.4519627552	recently developed
0.4518773909	syntactic and semantic
0.4518564116	search techniques
0.4518033298	quantitative results
0.4514421256	significant role
0.4512921280	times faster than
0.4511661050	sequence based
0.4510523681	deep model
0.4508782958	based onthe
0.4507185607	inference systems
0.4500768370	optimization procedure
0.4500579454	the world wide web
0.4498436405	recent results
0.4498037282	evaluation methods
0.4496354475	learning scheme
0.4495728257	simulation studies
0.4493555713	research area
0.4486151452	cnn trained
0.4485456605	accuracy rate
0.4483602691	non local
0.4481014140	regression tasks
0.4480253929	easily extended
0.4478098633	task learning
0.4476240743	graph model
0.4474680076	rank matrix completion
0.4474569380	using convolutional neural networks
0.4473195440	source and target
0.4471158131	prediction methods
0.4471137012	search results
0.4470057120	visual and textual
0.4466453602	learning to rank
0.4464859845	previous methods
0.4461956388	based classification
0.4461814757	additional information
0.4460323967	order markov
0.4459910645	case based
0.4459016834	machine learning problems
0.4457931733	reasoning about
0.4457022716	next generation
0.4454483123	machine translation system
0.4453154983	expression data
0.4450017406	recently gained
0.4449911920	computation cost
0.4449434723	training sample
0.4449242711	learning process
0.4448183019	data processing
0.4447721736	thelearning process
0.4445693012	successfully applied
0.4442195511	end to end learning
0.4441578108	real world problems
0.4439070928	significantly improved
0.4436943613	fast algorithm
0.4436666441	entity recognition ner
0.4436397402	target distribution
0.4433408638	acyclic graphs
0.4432660709	large numberof
0.4432366856	programming approach
0.4430557464	sparse gaussian
0.4430299674	spatial data
0.4427396395	experimental results demonstrate
0.4423585453	main focus
0.4414614299	extensive experimental
0.4413914953	analysis ica
0.4408621171	competitive performance
0.4407828418	time delay
0.4404671842	real world dataset
0.4401459954	application domains
0.4399155524	common practice
0.4398845648	supervised training
0.4396544696	clustering methods
0.4396150580	upper and lower
0.4391383054	recent approaches
0.4391128060	learning problem
0.4389567200	public datasets
0.4386327656	automatically learn
0.4386122408	large number
0.4385994553	a machine learning approach
0.4384866138	existing works
0.4384600405	detection framework
0.4384190062	multiple domains
0.4382439491	current research
0.4381677957	organizing map
0.4381510634	recursive neural
0.4373407349	learning models
0.4372832221	generative neural
0.4369634812	rnn models
0.4367927075	test problems
0.4363897163	result shows
0.4361191217	algorithm called
0.4360730984	sequence learning
0.4358146311	translation models
0.4356656440	re ranking
0.4356579380	sequence models
0.4356203110	non linear
0.4355514857	numerical examples
0.4354241471	neural networks dnns
0.4350438914	sparse subspace
0.4349057508	regression based
0.4349044181	end to end training
0.4347615868	comprehensive survey
0.4346559491	comparable performance
0.4342968682	unified view
0.4341198831	3d human pose estimation
0.4340771686	geometric properties
0.4335733440	preliminary experiments
0.4335068419	adaptive learning
0.4334759170	retrieval based
0.4333159042	three dimensional
0.4331748091	prediction performance
0.4327490226	timing dependent
0.4327120922	achallenging problem
0.4324445194	specifically designed
0.4322229032	large scale learning
0.4316979788	sequential monte
0.4314252305	structure based
0.4307196540	visual speech
0.4306890695	design problem
0.4302889851	training objective
0.4296525765	language processing tasks
0.4295230148	temporal features
0.4293871526	generation process
0.4292723052	owl 2
0.4292424372	research efforts
0.4291343181	two dimensional
0.4287933536	small size
0.4285280275	performance gain
0.4280836735	grained recognition
0.4280182071	increasing attention
0.4279407969	awide range of
0.4272110389	control tasks
0.4271360796	jointly learns
0.4270663536	prediction task
0.4270568361	one hidden layer
0.4268775026	recognition task
0.4266379263	temporal patterns
0.4265550127	jointly learn
0.4263162361	havebeen proposed
0.4262737932	training method
0.4262309431	minimum description
0.4258671031	speech recognition system
0.4255794088	a comparative study
0.4250787157	gp models
0.4250752458	detection and tracking
0.4246538841	trade off
0.4243470679	out of vocabulary
0.4239096993	existing techniques
0.4238289627	impressive performance
0.4236663885	recognition of handwritten
0.4236644748	a comparative analysis
0.4233423073	automatically detect
0.4230654018	distribution algorithms
0.4229785517	detection accuracy
0.4224401439	structured models
0.4222051931	system identification
0.4219782066	memory lstm
0.4219043334	high degree
0.4218263402	mnist and cifar
0.4216511433	sequence to sequence models
0.4211013599	time dependent
0.4210920971	extensive experimental results
0.4210590890	search problem
0.4210267585	spatial and temporal
0.4209833797	detection performance
0.4207518818	propose anovel
0.4205801366	recognition ocr
0.4197721972	results suggest
0.4195626369	comprehensive experiments
0.4193874283	dimensional data
0.4193616562	solved efficiently
0.4189567276	based ona
0.4188484045	sufficiently large
0.4187996531	segmentation task
0.4187630882	algorithm achieves
0.4187368736	training images
0.4186425411	based machine translation
0.4183215718	theory and practice
0.4181704217	research areas
0.4180762133	labeled images
0.4178538514	retrieval tasks
0.4172134963	learning method
0.4171823992	significantly faster
0.4170912226	driving cars
0.4169524093	et al
0.4169304607	underlying graph
0.4164960500	kitti dataset
0.4164744338	rich languages
0.4162035849	method outperforms
0.4160168186	rl methods
0.4157048585	impressive results
0.4156865547	directions for future
0.4155896259	experimental results showthat
0.4153833158	an encoder decoder
0.4153374041	significantly reduce
0.4152571470	prediction problem
0.4152353396	image level
0.4150588292	problem of recovering
0.4145116572	large scale visual
0.4142130144	image to image
0.4140255289	3d point cloud
0.4139994723	synthetic and real world datasets
0.4138953113	blind image
0.4138715988	existing algorithms
0.4138228721	rate of convergence
0.4137924487	data driven approach
0.4133695484	simulation results
0.4132573409	a unified approach
0.4131353159	observed data
0.4131052717	natural language processing tasks
0.4127748217	resolution image
0.4127411624	single model
0.4126059713	larger scale
0.4120875242	recent research
0.4118501961	human computer
0.4117385906	dimensional spaces
0.4116232829	practical application
0.4110916021	graph models
0.4099315231	one class
0.4097591148	complex models
0.4092236084	a generative model
0.4091663216	learning representations
0.4090639059	annotated training
0.4088646936	box optimization
0.4087497636	evaluation shows
0.4087290120	global and local
0.4084976261	poor performance
0.4084379083	convolutional neural network based
0.4084107049	ourproposed method
0.4082909140	specific knowledge
0.4081043089	significant improvement
0.4079927825	learning strategy
0.4077083022	linear time
0.4075066386	crucial step
0.4074723343	based representations
0.4074324603	local and global
0.4073606374	human visual
0.4071429934	classification results
0.4068755356	takes as input
0.4067259846	labeled training
0.4066503255	paper introduces
0.4065764679	large scale multi
0.4064600453	binary pattern
0.4060062217	comparable accuracy
0.4059972610	real world images
0.4053784913	classification based
0.4053180796	paper discusses
0.4050173588	value function
0.4049949907	training error
0.4046076961	estimation accuracy
0.4046039292	fast and accurate
0.4044616562	proposed algorithm
0.4043557318	oftraining data
0.4040895074	data samples
0.4035458812	using deep convolutional neural networks
0.4031438412	network size
0.4026512958	model achieves
0.4025349766	unsupervised approach
0.4017579576	video object
0.4011727000	efficiently solved
0.4009977846	high computational cost
0.4005044316	neural networks rnn
0.4004192127	world data sets
0.4003949036	signal to noise
0.3997818715	special case
0.3996312378	linear units
0.3996226562	data collected
0.3995241004	extensive evaluation
0.3988748441	a deep learning framework
0.3986534907	important aspect
0.3984827872	dynamic bayesian
0.3979343912	shown promising results
0.3974234394	high level semantic
0.3973410572	vision community
0.3971616195	first order methods
0.3970260446	et al 2015
0.3968804095	traditional methods
0.3968363235	image datasets
0.3968197395	sequence to sequence learning
0.3967630556	synthetic datasets
0.3967322910	promising performance
0.3964468200	training and testing
0.3963411714	sparse learning
0.3962739111	empirically evaluate
0.3961167960	large text
0.3959523678	traditional approaches
0.3958905997	excellent results
0.3958493076	article presents
0.3952285738	sources of information
0.3949860128	level representation
0.3949182344	test data
0.3947380822	challenging task
0.3947186020	an overview
0.3944024654	detection task
0.3943916263	average accuracy
0.3942285738	amounts of data
0.3941193881	network training
0.3939980295	proposed approach
0.3935466336	an agent based
0.3935044244	inthe literature
0.3933601738	learning architectures
0.3933238583	et als
0.3929468200	input and output
0.3922421964	current methods
0.3921636497	de facto
0.3921607397	agent reinforcement learning
0.3919821934	highly effective
0.3918885770	number of clusters
0.3918034404	solving large
0.3917764195	numerical results
0.3915602061	number of topics
0.3915033706	design and implementation
0.3914549904	based features
0.3913631431	fixed number
0.3910463876	supervised and unsupervised
0.3910392626	freely available
0.3907200938	method achieves
0.3905836074	representations of words
0.3902221922	test image
0.3900095474	connected layer
0.3896100590	competitive results
0.3895063064	empirical data
0.3894250169	automatic method
0.3892621569	method called
0.3890955790	a probabilistic framework
0.3890251030	training dataset
0.3885469360	theoretical and empirical
0.3881359793	the effectiveness ofour
0.3879583934	large amounts
0.3879521507	dynamical system
0.3874502694	area of research
0.3874412020	inference problem
0.3872304043	a strong baseline
0.3872211249	programming asp
0.3870790500	perform inference
0.3868217740	real data sets
0.3867595648	unlike previous
0.3858331795	voc 2012
0.3854083683	optimal performance
0.3853752665	numerous applications
0.3848139969	long short
0.3847970899	competing methods
0.3846768626	on theother hand
0.3846229632	segmentation results
0.3842389248	mild conditions
0.3841921784	previous research
0.3838542722	efficient and scalable
0.3835886027	simulated and real
0.3834650830	machinelearning algorithms
0.3833794003	efficient optimization
0.3833322767	paper investigates
0.3832858238	experimental studies
0.3831253914	task of predicting
0.3831253914	fast and robust
0.3829854704	standard benchmarks
0.3826332877	efficient methods
0.3822991916	media text
0.3820023776	a unified framework
0.3818306558	series data
0.3813185181	recent methods
0.3811796761	sample data
0.3811105325	synthetic and real
0.3808959290	using deep neural networks
0.3801810631	neural sequence
0.3801473722	learning approaches
0.3801362817	trained models
0.3801174698	train and test
0.3799449742	across domains
0.3790647986	underlying structure
0.3789769679	data for training
0.3789731829	model outperforms
0.3789086027	accuracy and robustness
0.3787332642	analysis shows
0.3786231956	an improved
0.3784830814	constant time
0.3783614869	method of multipliers admm
0.3781934953	image feature
0.3781105325	accuracy and efficiency
0.3778724903	image and video
0.3778628395	input data
0.3778612443	technique called
0.3777907758	detection approach
0.3776410702	success of deep learning
0.3775894968	trained and tested
0.3773027856	an efficient algorithm
0.3772138658	speed and accuracy
0.3770488547	embedding methods
0.3768869508	classification algorithm
0.3762321261	model size
0.3759638236	process gp
0.3759145680	descent methods
0.3756138400	network based
0.3755997300	model learns
0.3754671640	classification framework
0.3754547307	results obtained
0.3751397918	accurate and robust
0.3747331416	detection network
0.3746433528	complementary information
0.3744526828	improve performance
0.3744014052	faster r
0.3743942871	inference method
0.3742299481	algorithm for solving
0.3742179643	efficient and effective
0.3742142504	neuralnetworks cnns
0.3738724903	detection and recognition
0.3738508985	proposed framework
0.3737021645	search methods
0.3736851263	an end to end fashion
0.3732750409	with expert advice
0.3732402983	source of information
0.3731119190	classification approach
0.3729341594	a modified version
0.3722114781	key idea
0.3720562950	current approaches
0.3719098738	based method
0.3718246233	method of multipliers
0.3715534821	approach outperforms
0.3713717143	representation based
0.3713434346	experimentalresults demonstrate
0.3713320021	results showed
0.3712230988	frames per second
0.3711246647	a deep convolutional neural network
0.3710672354	dimensional euclidean
0.3710024571	noise models
0.3709213193	the sp theory
0.3708359743	orders of magnitude faster
0.3707969570	recommendation system
0.3706088404	non smooth
0.3704409881	resource languages
0.3702874031	main idea
0.3700424069	neural style
0.3697720521	regression and classification
0.3697351122	recognition system
0.3693617451	public dataset
0.3692855277	resolution sr
0.3691569649	classification and regression
0.3690138658	effective and efficient
0.3689700176	deep learning method
0.3688008294	training of deep
0.3684593292	c means
0.3682133994	detection and segmentation
0.3681738075	state and action
0.3679557504	term dependencies
0.3678508605	methods for learning
0.3677869233	bothsynthetic and real
0.3676197918	real and synthetic
0.3675149085	realistic images
0.3674844755	computer vision algorithms
0.3674800335	coordinate system
0.3672838929	prior model
0.3670962246	automatically generate
0.3670819741	running time
0.3670274374	learned models
0.3667890704	analysis tasks
0.3664660348	optimal rate
0.3663483299	a computational model
0.3661980332	finite time
0.3659854819	network model
0.3658553854	training and test
0.3654265836	prior works
0.3653075767	an interactive
0.3652376272	sparse bayesian
0.3652045079	approach achieves
0.3650176257	root mean
0.3648092719	training and inference
0.3645646156	generated data
0.3643581958	distances between
0.3643553531	time of flight
0.3643407686	synthetic and real world data
0.3642702263	recognition ner
0.3638178770	algorithms for learning
0.3637995873	an end to end manner
0.3635511462	encouraging results
0.3633339008	demonstrate theeffectiveness
0.3633100136	discrete time
0.3632860301	number of iterations
0.3630554410	et al 2016
0.3630326702	negative matrix factorization
0.3628340812	learning technique
0.3624890877	clustering problem
0.3624568092	based algorithms
0.3624081037	0 1
0.3623303895	deeplearning based
0.3622089819	machine learning algorithm
0.3621716218	version ofthe
0.3618889499	vision problems
0.3618215350	a deep learning based
0.3617780198	propose anew
0.3613501693	tasks including
0.3612343202	reduction technique
0.3612279921	main contribution
0.3611986610	optimization based
0.3608648228	a hybrid approach
0.3607413447	an adaptive
0.3605831026	area under
0.3605302994	existing models
0.3604031860	probability model
0.3603521270	algorithm outperforms
0.3603432584	ability to learn
0.3603427931	demonstrate thatour
0.3603094298	supervised methods
0.3600637540	training strategy
0.3598977873	dimensional features
0.3594972128	threshold value
0.3591841137	thatour method
0.3591553193	observable markov
0.3589334438	conduct experiments
0.3588505312	search method
0.3583781158	number of data points
0.3580557686	block model
0.3579139172	extraction methods
0.3574438359	improve theperformance
0.3569773636	past decade
0.3569545641	key step
0.3564364358	proposed model
0.3560008901	networks anns
0.3557129897	synthetic and real data
0.3556096998	algorithm ga
0.3550071272	existing literature
0.3549488957	data size
0.3544953768	top 5
0.3543558712	training stage
0.3543471930	proposed approach outperforms
0.3543269963	key challenges
0.3539954230	time consuming
0.3539124610	source data
0.3539064383	approach improves
0.3536774549	general problem
0.3535902065	past few years
0.3535119255	extensive empirical
0.3533993743	speed up
0.3532875181	features learned
0.3528873411	noise ratio
0.3522668829	framework called
0.3520722268	et al 2013
0.3518524275	under mild conditions
0.3517541253	number of training samples
0.3517025490	effectiveness and efficiency
0.3517022443	modeling approach
0.3515779831	empirically demonstrate
0.3511018929	training algorithms
0.3506383621	inverse reinforcement
0.3503584274	learning procedure
0.3499709788	problem of predicting
0.3498065821	experiments showthat
0.3494683402	large data
0.3489735978	an ontology
0.3489685526	depth analysis
0.3488046439	method named
0.3488020021	making problems
0.3486740380	conventional methods
0.3484545754	research field
0.3483793462	a small fraction
0.3483594997	recognition methods
0.3481228018	unsupervised domain
0.3480974965	inference tasks
0.3480826199	a large scale
0.3479058374	empirical performance
0.3477019615	baseline methods
0.3472914745	research topic
0.3470929649	thatthe proposed
0.3469105878	detection and classification
0.3465105575	publicly available datasets
0.3459930136	held out
0.3458420049	end to end speech
0.3457255421	non linearity
0.3453594869	efficiently learn
0.3453291808	original image
0.3452757286	sequence to sequence model
0.3452437005	time frequency
0.3452038489	deepconvolutional neural
0.3451881234	problem of detecting
0.3444442093	a bayesian approach
0.3444360409	processing step
0.3443608952	high spatial
0.3436697626	unsupervised manner
0.3436615962	an open question
0.3435007142	independent component
0.3434290206	variable models
0.3433478057	simple and effective
0.3428387483	non trivial
0.3424816489	algorithm named
0.3421992646	convolutional sparse
0.3420852364	resulting model
0.3419730483	standard benchmark
0.3416819695	an attention based
0.3416749727	et al 2010
0.3416737862	present anovel
0.3415681091	monte carlo tree
0.3415072044	biological neural
0.3414544319	to end trainable
0.3412015342	real and synthetic data
0.3411205209	semantic image
0.3407831803	applications including
0.3406001221	per pixel
0.3404686993	multiple layers
0.3404584213	power of deep
0.3402527237	learning techniques
0.3401627621	number of measurements
0.3400384735	an integrated
0.3397001163	taking advantage of
0.3396061520	analysis reveals
0.3395222290	significant performance
0.3393239294	ofour approach
0.3392958674	model structure
0.3392737166	local means
0.3391111900	detection method
0.3389864540	rank 1
0.3386581331	training neural networks
0.3377623122	dialogue system
0.3377531667	open question
0.3377464050	face recognition using
0.3376847253	grained classification
0.3371581988	et al 2014
0.3370886396	task at hand
0.3369943651	word based
0.3365977018	decoder architecture
0.3365415359	a genetic algorithm
0.3365038412	speech pos
0.3362894560	recently shown
0.3361685450	improves performance
0.3359513174	expected value
0.3359468084	based algorithm
0.3353378632	non differentiable
0.3353244230	non euclidean
0.3351013124	extensive experiments demonstrate
0.3349754039	components analysis
0.3348804249	a unified
0.3347652896	real datasets
0.3343722409	high levels
0.3342323798	3d pose estimation
0.3341644419	deep semantic
0.3340059421	the semantic web
0.3336730869	person re
0.3336545708	significantly higher
0.3327239972	kernel ridge
0.3322799496	a weakly supervised
0.3321384812	unlike existing
0.3315511909	3d human pose
0.3315394199	types of data
0.3312194596	upper bounds on
0.3311932143	results reveal
0.3311789276	sheds light
0.3310154529	existing solutions
0.3309105954	an efficient
0.3308179329	local learning
0.3306594264	an evolutionary
0.3305064634	clustering problems
0.3303513592	inference process
0.3301967126	time horizon
0.3301386349	processing steps
0.3301035364	learning systems
0.3299283995	difference learning
0.3296618616	most probable
0.3295835068	error mse
0.3295721747	fundamental problem
0.3295183646	the art methods
0.3289762995	intelligent system
0.3289075600	network dcnn
0.3287179385	comparable results
0.3281179290	deep multi
0.3280309075	label classification
0.3276568638	target data
0.3275940933	both synthetic and real world
0.3270910341	synthetic and real world
0.3270738368	experiment results
0.3270037279	support systems
0.3269042706	framework named
0.3267741490	each data point
0.3266649351	without losing
0.3265824945	simulated and real data
0.3264052398	log n
0.3261295623	regularized least
0.3259197007	extremely high
0.3256929494	defined in terms
0.3255966439	recognition algorithms
0.3255603517	bridge between
0.3254699271	and vice versa
0.3245569242	world datasets
0.3244610642	mutual information between
0.3244427750	propagation bp
0.3240622711	per iteration
0.3235124316	continuous speech
0.3231366343	automatic image
0.3229585998	publicly available
0.3228350852	multi agent reinforcement
0.3227541221	using genetic algorithm
0.3226226251	non overlapping
0.3225984781	problems including
0.3225513449	type 2
0.3223630725	extract features
0.3222564230	use case
0.3221497441	limited training
0.3221166827	significant attention
0.3220494157	method produces
0.3219669634	supervised tasks
0.3216902218	machinelearning methods
0.3216304130	labeling tasks
0.3216153459	detection systems
0.3213827781	a generative adversarial network
0.3212731363	an evolutionary algorithm
0.3210627124	synthetic and real datasets
0.3207917421	artificial and real
0.3206710951	et al 2012
0.3204427766	based techniques
0.3204110779	fundamental problems
0.3202486894	a wide range
0.3199285878	challenging datasets
0.3197552005	central role
0.3195485260	a closed form solution
0.3195144245	interest roi
0.3194773462	data distribution
0.3193971481	inreal time
0.3192285430	the art
0.3187614626	classifier based
0.3186911796	concept analysis
0.3186249187	recognition asr
0.3185687323	learning based methods
0.3181982677	put forward
0.3179856999	simple approach
0.3176425294	run time
0.3174436384	a deep neural network
0.3172320541	improved accuracy
0.3161023021	space models
0.3150927002	paper addresses
0.3150811890	segmentation based
0.3150556199	detection results
0.3146634809	upper bound on
0.3144175453	selection process
0.3138033056	detection problem
0.3136550309	segmentation algorithms
0.3132830407	fold cross
0.3125399356	results showthat
0.3118565021	based systems
0.3117960754	inthe context of
0.3117874050	gap between
0.3114603395	variational lower
0.3113180882	using machine learning
0.3112807924	analysis pca
0.3112674371	outperforms thestate of
0.3112383592	shown great
0.3112023986	base completion
0.3110764460	sheds light on
0.3110480720	a general framework
0.3107076703	the past decade
0.3106045709	large scale dataset
0.3105254618	two sample
0.3099202335	an efficient approach
0.3098359385	proposed method achieves
0.3094630495	comparative study of
0.3094506000	alarge number of
0.3093083193	approach yields
0.3092743676	attention in recent
0.3091156611	out of sample
0.3083404583	learning ml
0.3079926389	experiments demonstrate
0.3079800384	supervised deep
0.3077120533	standard datasets
0.3076182879	baseline model
0.3074624576	hard problem
0.3071540192	decoder framework
0.3070395940	task of learning
0.3069230103	end to end fashion
0.3067606065	a wide variety
0.3067117174	part of speech
0.3064401853	conduct extensive
0.3063262256	method for estimating
0.3060639914	thewell known
0.3059798297	world applications
0.3058994358	deep metric
0.3058148877	active research
0.3058096591	domains including
0.3057078895	strong performance
0.3057048935	original data
0.3056044427	achieves higher
0.3054733149	similar performance
0.3054443138	computer vision problems
0.3054261836	data point
0.3053027773	a novel approach
0.3053015248	from scratch
0.3050135849	the shelf
0.3046181710	number of steps
0.3046124412	3d shapes
0.3045900850	previous results
0.3045885826	discriminative power
0.3044014352	experimentally demonstrate
0.3043937205	reinforcement learning methods
0.3041459180	important issue
0.3040290474	computer vision applications
0.3039453906	one step
0.3038048022	non asymptotic
0.3036483339	ofour method
0.3031059107	datasets including
0.3027735305	computational results
0.3024986069	great importance
0.3024902415	bayesian deep
0.3021889755	an attention mechanism
0.3020080356	challenging problem
0.3012971682	non invasive
0.3012222909	partial differential
0.3011126489	method works
0.3007589295	mean discrepancy
0.3005049773	model trained
0.3004826369	current state of
0.3002883230	gradient information
0.3001137912	learning for image
0.3000781951	simple to implement
0.3000519363	efficient deep
0.2999503230	carried out
0.2997689332	supervised data
0.2997025762	the dempster shafer
0.2996633903	artificial data
0.2996247745	identification problem
0.2995158082	an unsupervised
0.2992922587	builds upon
0.2990822417	to noise ratio
0.2989366595	a recurrent neural network
0.2988777022	d dimensional
0.2983739443	qualitative results
0.2980981761	full precision
0.2978704955	3d reconstruction
0.2975970385	an upper bound
0.2974811232	applications ranging
0.2971564525	an extension
0.2971191133	network gan
0.2970698303	near future
0.2970329719	results on real
0.2962897739	temporal classification
0.2961553734	study shows
0.2960461656	proposed methods
0.2958301229	3d shape
0.2956366625	central problem
0.2955003237	number of samples
0.2953478762	a bayesian model
0.2953275002	data clustering
0.2950142222	prior knowledge about
0.2948814259	small number
0.2947474261	questions about
0.2946058278	et al 2017
0.2945304792	the united states
0.2941947834	3 d
0.2941753879	region of interest
0.2941278574	al 2013
0.2941082210	results confirm
0.2940535647	above mentioned
0.2938534168	rank one
0.2936259065	approach significantly outperforms
0.2935596635	resulting algorithm
0.2933347392	pose estimation from
0.2932129648	an unsupervised manner
0.2931520570	an innovative
0.2931434447	models hmms
0.2930937408	common approach
0.2929720510	network ann
0.2928500113	neural language
0.2927930722	simple neural
0.2915908281	a large dataset
0.2913856795	short time
0.2912003387	sequence model
0.2909316116	not necessarily
0.2902953700	important applications
0.2899377950	main results
0.2898650133	the art performance
0.2896752825	order of magnitude faster
0.2893831911	question answering vqa
0.2891795067	no longer
0.2886664395	main goal
0.2885594873	problem specific
0.2884616291	vital role in
0.2884197079	method yields
0.2881250030	connections between
0.2878177664	computation time
0.2874861257	this paper proposes
0.2874332245	provide evidence
0.2873814527	using recurrent neural networks
0.2873584554	an ensemble
0.2865296040	this paper presents
0.2862391073	robustness against
0.2860474099	key challenge
0.2859191001	powerful tool
0.2858074136	proposed architecture
0.2855710355	a general purpose
0.2853575006	achieves state of
0.2851263674	wide variety
0.2847802538	learning based method
0.2847275283	lower bounds on
0.2846396468	superior results
0.2844077169	an interpretable
0.2843340589	takes advantage
0.2842278452	the proposed approach
0.2839826402	strong baseline
0.2837782233	paper demonstrates
0.2833912524	time warping
0.2833386685	ever increasing
0.2829368500	important role
0.2826006320	networks cnns
0.2825958367	demonstrate thatthe
0.2824007271	approach consists
0.2821051861	an incremental
0.2816279542	problems in machine
0.2812822960	a survey
0.2811376269	interplay between
0.2810843863	main challenges
0.2809303403	so called
0.2807597964	space time
0.2806955795	without sacrificing
0.2806583476	neural networkscnns
0.2806152154	vision system
0.2806093679	problem of finding
0.2805431056	datasets demonstrate
0.2804684445	paper reports
0.2804197101	3d pose
0.2803656602	takes advantage of
0.2803188744	the proposed method
0.2802188527	modern machine
0.2801521064	research community
0.2799974089	results demonstrate
0.2797517781	a brief
0.2795876989	paper explores
0.2792835739	different modalities
0.2791506192	machine learning community
0.2791109921	existing systems
0.2790181102	two phase
0.2789166876	paper develops
0.2788175549	data matrix
0.2787700227	learning discriminative
0.2786314696	best of ourknowledge
0.2785310667	bayesian approach to
0.2784636019	a posteriori map
0.2784406378	whole image
0.2782780011	change over time
0.2780198609	problem at hand
0.2779614737	number of observations
0.2775556009	methods fail
0.2774120660	a bayesian network
0.2772120230	propagation algorithm
0.2771588535	general model
0.2761244834	based model
0.2758629119	ofthe proposed
0.2758297277	rank matrices
0.2758274694	difficult problem
0.2757956732	perform experiments
0.2756176552	space model
0.2756149048	higher performance
0.2747632411	information contained in
0.2746000579	well suited
0.2745005012	noisy or
0.2744478770	et al 2011
0.2743824586	well understood
0.2741746232	existing datasets
0.2731722510	novel objects
0.2730704805	data selection
0.2727476992	difficult task
0.2725790728	a deep architecture
0.2723933721	estimation methods
0.2723604654	well established
0.2720282567	value decomposition
0.2719867506	learning machine
0.2718825257	algorithm based
0.2716121268	based semi supervised
0.2713779078	an in depth
0.2712825775	commonly used
0.2711192868	per frame
0.2709270898	great deal
0.2708913369	detection system
0.2707856583	methods require
0.2702078592	necessary and sufficient
0.2701196891	report results
0.2700466458	dimensional structure
0.2695735848	dependencies among
0.2694618514	appearance changes
0.2694086725	lower bound on
0.2693970501	process model
0.2692790766	distributed deep
0.2692245211	reason about
0.2690932722	features extracted from
0.2690773501	language inference
0.2690172895	model called
0.2689712737	based multi
0.2689509327	in recent years
0.2689335879	there exists
0.2688931307	basic idea
0.2687619963	bidirectional long
0.2685761592	data source
0.2685397492	achieve competitive
0.2684624316	follow up
0.2684470800	results compared
0.2680666311	the art approaches
0.2679296326	huge number
0.2677716129	taken into account
0.2675617440	key problem
0.2670332257	massive amounts of
0.2669703968	al 2015
0.2668867967	a by product
0.2668029775	sub optimal
0.2664247866	an enhanced
0.2662000585	this paper describes
0.2661548608	a new paradigm
0.2658970627	technique based
0.2656895236	complex problems
0.2656009454	classification using
0.2655087507	challenging problems
0.2654319227	shown to outperform
0.2653109545	top 1
0.2650386715	published results
0.2650127991	significant improvement over
0.2648013034	a convolutional neural network cnn
0.2647057615	challenging dataset
0.2646262469	learning sparse
0.2643422426	a special case
0.2639633883	based semi supervised learning
0.2636953846	very deep
0.2636904907	main objective
0.2636829409	of sample extension
0.2636475381	data generation
0.2636147613	trained networks
0.2632302464	function based
0.2628431680	the effectiveness ofthe
0.2627722344	label learning
0.2626514763	an automated
0.2623487925	experimental result
0.2620552746	a simple algorithm
0.2617439877	network approach
0.2617019575	a new approach
0.2614626845	a convolutional neural network
0.2612908990	tradeoff between
0.2612576974	network inference
0.2609367349	learning deep
0.2608550429	training of deep neural
0.2607530990	multiple data
0.2607463291	number of parameters
0.2605092849	an algebraic
0.2604883150	correlation between
0.2603753619	small data
0.2601690505	underlying data
0.2597520122	computer simulations
0.2596947896	set size
0.2593603173	paper examines
0.2591534996	forward neural network
0.2590535898	based image
0.2589454873	algorithm for learning
0.2588524827	traditional approach
0.2578919717	standard methods
0.2578331360	generated content
0.2573298910	experimental results indicate
0.2568247253	advances in deep learning
0.2567468632	widely used
0.2565165093	large amounts of
0.2563556891	proposed method outperforms
0.2563493207	time complexity
0.2563404096	very large
0.2563400301	dimension d
0.2563275541	convex problem
0.2562197583	robust method
0.2561073609	based classifier
0.2559368031	the proposed algorithm
0.2553334054	popular methods
0.2552910151	recognition method
0.2551080142	proposed technique
0.2550757259	an iterative
0.2550189586	optimal algorithm
0.2549874981	training deep
0.2549872372	two layer
0.2549170265	a single image
0.2549006921	thanstate of
0.2547802973	n dimensional
0.2547399655	efficient approach
0.2546897303	real time performance
0.2546009928	faster than
0.2540834273	a deep convolutional
0.2534113323	decide whether
0.2532464249	outperformsstate of
0.2532099753	huge number of
0.2530971193	existingstate of
0.2530962317	distance between
0.2528393320	of speech tagging
0.2527801049	an important role
0.2525579037	previousstate of
0.2524651001	a comprehensive
0.2523421194	related methods
0.2521743224	simple method
0.2521214167	attention in recent years
0.2520971193	severalstate of
0.2520625297	the artperformance
0.2518629085	learning classifiers
0.2515137322	a deep network
0.2515030056	method improves
0.2513350320	approach combines
0.2510115545	two player
0.2509850244	tracking system
0.2509607601	expected number
0.2508924802	architecture called
0.2504506095	while maintaining
0.2504160946	method for detecting
0.2502464249	achievestate of
0.2501643000	more importantly
0.2501092995	language based
0.2500093443	learning setting
0.2498217590	dense 3d
0.2497360804	based framework
0.2496690538	the art methods in terms
0.2492741950	isequivalent to
0.2492045656	reported results
0.2489435932	the decision maker
0.2489057662	relationships among
0.2485168733	method to generate
0.2482787894	an efficient method
0.2482320762	approach based
0.2480260664	simple yet effective
0.2478490126	imagenet dataset
0.2477464249	achievesstate of
0.2477159504	box complexity
0.2477112354	built upon
0.2477086678	supervised learning methods
0.2476533236	field crf
0.2476031427	learning based approaches
0.2475995152	view learning
0.2472846938	number of nodes
0.2472548131	2 d
0.2471263964	execution time
0.2468336606	based action recognition
0.2466313048	go beyond
0.2465279103	processing tasks
0.2464887038	theoretic framework
0.2463666487	a non parametric
0.2462481085	network classifiers
0.2462216370	wide range of
0.2458959931	2d and 3d
0.2458667229	this article presents
0.2457818795	simple algorithm
0.2456076833	magnitude faster
0.2452076949	recent advances in
0.2450460395	the proposed framework
0.2449871787	making process
0.2446171471	large number of
0.2446033492	a probabilistic model
0.2443822664	a data driven approach
0.2441663794	computer vision tasks
0.2440891993	proposed algorithms
0.2440468794	a low cost
0.2440067343	gram model
0.2435191778	experiments conducted
0.2434798780	high computational
0.2432506534	a fixed number
0.2432297070	data sparsity
0.2429985028	results demonstratethat
0.2427773834	standard approach
0.2426743145	presented approach
0.2426271070	well studied
0.2425202952	outperform existing
0.2423306918	necessary and sufficient conditions
0.2422788541	a neural network
0.2421625674	regions of interest
0.2420787274	a preliminary
0.2418674426	on demand
0.2418603941	al 2014
0.2417228221	class of problems
0.2416929180	an end to end
0.2416090958	the art results
0.2415113454	with high probability
0.2414685325	one dimensional
0.2410929149	a statistical model
0.2410345018	function f
0.2407858196	user study
0.2403185969	fast learning
0.2403181446	significant improvements over
0.2402771890	attention recently
0.2402712220	taking advantage
0.2401649840	analysis ofthe
0.2400590411	experimental results show
0.2399898169	a bayesian framework
0.2399032087	approach called
0.2398274080	non adaptive
0.2397700149	a great deal
0.2396830674	massive amounts
0.2394392038	training phase
0.2391169728	based tracking
0.2390087637	top performing
0.2387910004	regret bounds for
0.2386584983	art methods
0.2379769956	relationships between
0.2379111194	computational experiments
0.2378401027	many real world applications
0.2375261177	much easier
0.2374143959	trade off between
0.2372238451	each time step
0.2371824076	statistical language
0.2370642595	test results
0.2363168586	presence of noise
0.2362754645	existing results
0.2360725197	more precise
0.2360317558	computational methods
0.2358899290	leave one
0.2356531287	this paper introduces
0.2352873518	i vector
0.2352528654	class of models
0.2352155563	a user study
0.2351707555	recently deep
0.2351257193	control system
0.2351003470	variable model
0.2349905216	expected number of
0.2344859429	recognition using
0.2344253246	multiple object
0.2340897078	segmentation problem
0.2340227711	becoming increasingly
0.2340104403	detection using
0.2339190524	an investigation
0.2339076911	demonstrate theeffectiveness of
0.2338160856	coherence tomography
0.2337815747	discrepancy between
0.2328162671	language learning
0.2326795549	an effective approach
0.2326523977	a large corpus
0.2324994283	an intelligent
0.2321691576	major challenge
0.2320936353	due tothe
0.2320379382	take into account
0.2319232413	this article describes
0.2317346090	classification system
0.2316937989	an alternative
0.2316191447	number of examples
0.2315707388	approach leads
0.2314379850	dataset consisting of
0.2310888009	high dimensional feature
0.2305931832	x and y
0.2299622892	algorithm for computing
0.2299511130	based segmentation
0.2298804269	the art baselines
0.2297591399	best arm
0.2295260165	learning bayesian
0.2290922471	relations between
0.2289773821	closely related to
0.2289321726	aimed at
0.2287979471	the proposed model
0.2287961204	scale optimization
0.2285708781	the present paper
0.2285579387	problems in machine learning
0.2284231974	achieve state of
0.2281607144	a unifying
0.2281495021	previous state of
0.2281058505	multi way
0.2280703880	an online
0.2280472987	significant impact on
0.2280152951	without requiring
0.2280009658	the ground truth
0.2277347507	level semantic
0.2272433155	contrast to existing
0.2269461387	equivalence between
0.2269078650	approach performs
0.2268675636	ray images
0.2267759705	approach enables
0.2267476357	fuzzy c
0.2267252956	the last decade
0.2265991652	bayesian decision
0.2265168289	wide range of applications
0.2263800838	identification re id
0.2261241784	does not require
0.2260483953	our main contribution
0.2259966910	step toward
0.2259797754	a deep learning
0.2259671261	low computational
0.2259281359	end to end approach
0.2258160889	network trained
0.2255586084	a neural network based
0.2254552088	each iteration
0.2253875236	a feed forward
0.2253032955	readily available
0.2251698871	networks trained
0.2250180613	therelationship between
0.2250134793	two step
0.2249507866	an order of magnitude
0.2248002853	an iterative algorithm
0.2244243816	algorithms for solving
0.2243814144	computer interaction
0.2242732635	noise model
0.2234607265	set semantics
0.2231231813	vital role
0.2228225619	3d facial
0.2227354459	correspondence between
0.2226337958	a natural language
0.2225842403	trained model
0.2222353329	an unsupervised learning
0.2217314998	extraction method
0.2215087730	q network
0.2214708731	a latent variable
0.2213494406	modified version
0.2212280715	applications require
0.2211499230	achieved great
0.2208232912	an analytical
0.2207439072	this paper investigates
0.2204135748	correlations among
0.2203827818	learning settings
0.2202561100	based technique
0.2202011426	model gmm
0.2200222953	an effective method
0.2193946471	computational time
0.2193524772	the penn treebank
0.2192007670	extensive experiments on
0.2190229883	lot of attention
0.2189152435	based regularization
0.2187543967	the covariance matrix
0.2182123344	an empirical
0.2180904340	an input image
0.2179282938	algorithm to compute
0.2179032885	paper shows
0.2178305695	from observational data
0.2178267858	a maximum likelihood
0.2177313838	a reinforcement learning
0.2173559741	greater than
0.2172674213	for multi label
0.2172611677	during training
0.2169985605	small number of
0.2169903250	state of
0.2168837911	method shows
0.2167189283	proposed methodology
0.2165565377	similar results
0.2161483246	10 cifar 100
0.2161038088	on mobile devices
0.2158001104	series prediction
0.2157105750	each node
0.2153596302	an empirical evaluation
0.2153210978	previous algorithms
0.2151093198	2d pose
0.2150140399	a major challenge
0.2149166216	proposed to solve
0.2145191908	better generalization
0.2143500765	problem of learning
0.2142618352	problem of estimating
0.2141731974	outperforms state of
0.2139972400	scale datasets
0.2139966779	neural network trained
0.2138139311	challenging tasks
0.2137831232	blind source
0.2137691270	scale image
0.2136457392	improves upon
0.2135717490	this paper addresses
0.2135623659	a priori
0.2134773209	first step towards
0.2133553605	analysis techniques
0.2132636567	the art algorithms
0.2129528488	model to learn
0.2128478982	other agents
0.2127009186	recent success
0.2126457801	decoder network
0.2125208479	paper considers
0.2123607351	convolutional deep
0.2122197157	q networks
0.2120196461	as wellas
0.2118259649	more expressive
0.2117469975	very high
0.2116057852	existing state of
0.2115139704	based optimization
0.2111664262	linear combinations of
0.2108143730	identification using
0.2107455143	method for solving
0.2107409778	recent deep
0.2106078476	performs better than
0.2104092081	order to learn
0.2102957012	success of deep
0.2102777188	the nuclear norm
0.2100566383	a mobile robot
0.2100316032	end to end deep
0.2100110699	a machine learning
0.2099534550	localization using
0.2096658526	a rule based
0.2095760987	this paper
0.2094850116	inthis work
0.2093962913	a generalized
0.2093299442	similarities between
0.2091408779	this technical report
0.2090045581	3d object
0.2089962487	based on
0.2084663431	an application
0.2084216634	algorithmbased on
0.2079486764	thenotion of
0.2078965151	the gaussian process
0.2078216861	each pixel
0.2077831649	framework for learning
0.2077726348	capable of learning
0.2076235429	based upon
0.2075086382	approach shows
0.2072966094	as special cases
0.2071087509	translation system
0.2070082624	modal data
0.2068404495	an f1 score
0.2067216465	method performs
0.2067019254	aswell as
0.2066693017	accurate results
0.2065560431	a single pass
0.2064549239	step approach
0.2064479153	an effective
0.2062321726	insight into
0.2061830511	level performance
0.2061172527	very simple
0.2061104869	a simple
0.2061059562	frequently used
0.2060847610	3d face
0.2060791763	long time
0.2059381313	rank structure
0.2058850743	graph g
0.2056217468	rapid development of
0.2056088247	into account
0.2055855707	increasing number
0.2054839286	worse than
0.2053489359	based architecture
0.2052967235	based approach to
0.2052631308	allows usto
0.2050038200	first search
0.2048679306	crucial role
0.2048631816	set of experiments
0.2047320918	under certain conditions
0.2046099879	thestate of
0.2044957008	networks dnns
0.2043780443	end to end neural
0.2043608593	this thesis
0.2043550222	recent state of
0.2042346668	each round
0.2042099636	outperform state of
0.2041996556	an infinite
0.2041712438	similarity between
0.2041055153	estimation using
0.2039735947	algorithm to solve
0.2039476133	more reliable
0.2038828906	referred to as
0.2038093975	while retaining
0.2037903663	segmentation using
0.2037257739	paper studies
0.2034787378	detailed analysis
0.2033826890	number of classes
0.2033805782	caused by
0.2033666420	on line
0.2030899096	methods for solving
0.2029860030	much smaller
0.2029623746	models achieve
0.2028829488	a two stage
0.2028731393	management system
0.2028521196	image super
0.2028383918	experiments on synthetic
0.2028018611	an open problem
0.2027179198	knowledge about
0.2027036491	approach relies
0.2026496517	an analytic
0.2024419912	plethora of
0.2023105150	a parallel
0.2021905239	processing applications
0.2021882683	large scale machine
0.2020282443	wide variety of
0.2018148667	a nonparametric
0.2015871836	a closed form
0.2015024554	a review
0.2014146186	propose to learn
0.2013381478	over fitting
0.2010821761	links between
0.2009557978	prediction using
0.2008440496	a modular
0.2007333087	successfully applied to
0.2007316766	increasing number of
0.2006033101	evaluation results
0.2004994279	isbased on
0.2002903818	much simpler
0.1997625900	while keeping
0.1997400186	performance comparable
0.1997258183	a largenumber
0.1997052535	the proposed architecture
0.1996231172	algorithms based on
0.1993962902	a long standing
0.1993832718	achieving state of
0.1993178557	otherstate of
0.1990377926	more efficient
0.1989311496	well known
0.1985077526	research problem
0.1983173297	at risk
0.1983004495	based representation
0.1981874299	algorithm performs
0.1978805782	serve as
0.1977181913	set of features
0.1975868624	last few years
0.1974771584	class of algorithms
0.1971964258	pre trained on
0.1971769400	a deep neural
0.1970526155	broad class
0.1968054311	determine whether
0.1966939526	an event
0.1966232697	experiments conducted on
0.1965128668	more accurate
0.1964322722	an evaluation
0.1963869153	a large scale dataset
0.1963500211	general method
0.1960599663	compared to existing
0.1957881457	retrieval system
0.1956302447	f1 score of
0.1955727314	previous work
0.1954674740	data sets demonstrate
0.1946509575	order to address
0.1946258905	the proposed methodology
0.1945145733	a novel
0.1943480631	important information
0.1942472345	language text
0.1941523958	decomposed into
0.1940455349	a deep
0.1931488382	results provide
0.1930883512	to date
0.1928461206	period of time
0.1927495911	key component
0.1924568608	computer vision community
0.1923937145	person re identification re
0.1923165757	very fast
0.1922526704	sub problems
0.1921161616	the proposed technique
0.1920573367	form solution
0.1919411142	significantly faster than
0.1918293939	the main idea
0.1917909847	benchmark data
0.1916454465	compares favorably to
0.1915596499	relationship between
0.1915561481	our main result
0.1915091694	learning strategies
0.1914925338	rapid development
0.1911666507	a semi supervised
0.1911565312	a black box
0.1911211992	method to estimate
0.1909541682	until now
0.1908156907	a multitude
0.1907460303	world data
0.1907087423	information about
0.1906635650	too much
0.1906479004	a theoretical analysis
0.1906321999	dealing with
0.1905966869	smaller than
0.1902766388	broad range
0.1902751060	the art performances
0.1902650427	best performing
0.1897615432	the kullback leibler
0.1895539786	in situ
0.1894746642	classification datasets
0.1893128806	analysis methods
0.1888363830	compared to
0.1887725553	a new
0.1886969697	regarded as
0.1886493837	an embedded
0.1884551632	an important step
0.1883500669	range of applications
0.1879860030	relatively small
0.1877770497	fed into
0.1877730488	more robust
0.1876909595	divided into
0.1876132275	shown promising
0.1876046740	sub linear
0.1874638773	3d surface
0.1873750304	to understand
0.1873575963	number of
0.1873047641	withstate of
0.1872733562	there exist
0.1872565730	an extended
0.1872277237	ranging from
0.1871900075	to learn
0.1871816695	even though
0.1871523190	method learns
0.1871341716	lower bounds for
0.1870530520	popular method
0.1869407772	a new perspective
0.1868853622	a large margin
0.1866356952	3d scene
0.1862707298	human like
0.1862086514	performance comparable to
0.1860059698	the global optimum
0.1858805782	viewed as
0.1858521541	paper aims
0.1858432686	posed problem
0.1858181972	number of variables
0.1855869788	no need
0.1855347476	tostate of
0.1855033229	beviewed as
0.1854358840	convolution neural
0.1854298020	model based on
0.1854114207	retrieval using
0.1853299373	the performance ofthe
0.1852436143	2d images
0.1851587207	a high resolution
0.1850843043	nmt system
0.1849209453	a posteriori
0.1847373500	differences between
0.1846772702	depending on
0.1846678461	this paper wepropose
0.1846506189	well defined
0.1845367631	world problems
0.1842440179	in spite
0.1841015803	experiments on real
0.1840997531	a novel architecture
0.1840964632	incorporated into
0.1839901137	a crucial role
0.1838898850	achieved state of
0.1837618367	lead to
0.1837513982	probabilistic framework
0.1837224117	multitude of
0.1833871817	results on synthetic
0.1833134581	the other hand
0.1831745084	wide range
0.1831181508	arrive at
0.1830576707	a large number
0.1830307776	sufficient conditions for
0.1829555407	grained image
0.1827497301	experimental results on
0.1826796681	sub tasks
0.1826771895	balance between
0.1826083995	method for learning
0.1825245554	word error
0.1824533266	a low rank matrix
0.1823475262	model performs
0.1823454947	the out of
0.1823366435	thenumber of
0.1823296615	link between
0.1820370436	arebased on
0.1819452955	difference between
0.1818998647	improve upon
0.1818709912	al 2010
0.1818272473	based learning
0.1817435334	in high dimensions
0.1816553587	box models
0.1813795730	inspired by
0.1813791026	method based on
0.1813760192	non zero
0.1813601010	applications ranging from
0.1811639221	a versatile
0.1811375468	a general approach
0.1809573240	scale data
0.1808342548	shed light
0.1808243880	in addition
0.1807204276	method requires
0.1806984741	large dataset
0.1806246951	relations among
0.1805716444	approach towards
0.1803185121	suffers from
0.1801975481	the otherhand
0.1801323140	in such cases
0.1801089308	this article
0.1799362485	means algorithm
0.1798808574	subjected to
0.1796003638	a widerange
0.1795479796	the upper bound
0.1795319677	to solve
0.1795253725	weighted sum of
0.1795007413	suffer from
0.1794303318	a siamese
0.1793179591	the main challenge
0.1792221140	a rich set
0.1791957069	interactions between
0.1791690394	the art techniques
0.1790289574	of speech pos
0.1788037030	data distributions
0.1787846756	dataset demonstrate
0.1784323970	most importantly
0.1784096586	so far
0.1782956454	a corpus
0.1782889526	more complicated
0.1782507792	large numbers of
0.1780470399	an open
0.1779129845	relies on
0.1778295844	a powerful tool
0.1777876757	dimensional sparse
0.1777596236	this paper examines
0.1776590942	a key component
0.1775443251	passing algorithm
0.1773866745	trained end to
0.1773587684	an oracle
0.1772774362	level feature
0.1772772595	to assist
0.1772725149	domain data
0.1772552653	a broad range
0.1771854196	large corpus
0.1771794171	on two public
0.1769993083	improve theperformance of
0.1769685928	high dynamic
0.1769505506	performance compared to
0.1769349402	this paper discusses
0.1769205291	rather than
0.1768972442	the proposed scheme
0.1768721738	discriminate between
0.1768187651	level representations
0.1766517193	based face
0.1765791455	next step
0.1762965818	rely on
0.1761029994	total number
0.1759728403	the sametime
0.1755753695	dimensional linear
0.1754931399	the target domain
0.1752448098	a low dimensional
0.1750626802	total number of
0.1750528307	much larger
0.1750456229	an accelerated
0.1747581794	an adversary
0.1746197087	satisfaction problem
0.1745296023	test time
0.1742697348	with other methods
0.1741172820	little attention
0.1738590745	object of interest
0.1738351001	framework based on
0.1736760442	time steps
0.1732301790	to end training
0.1731965271	interactions among
0.1731707865	the key idea
0.1731469609	a multi scale
0.1731324366	quality images
0.1730950906	mismatch between
0.1730463903	new insights
0.1729574462	an explanation
0.1729297562	last decade
0.1729276614	methods rely
0.1728640492	per class
0.1728270314	a library
0.1728058659	a tutorial
0.1726715660	level visual
0.1723567190	the basic idea
0.1721769118	insights into
0.1718711880	inspiration from
0.1716890844	a fast
0.1716539128	avariety of
0.1716498250	a popular approach
0.1715449530	models provide
0.1712722715	uncertainty about
0.1712434251	on two challenging
0.1712221880	proposed network
0.1711528925	at test time
0.1706790331	the proposed methods
0.1705920992	a bottom up
0.1705153927	approach to learn
0.1704112597	under consideration
0.1703105120	largenumber of
0.1701773226	great interest
0.1697381809	this paper explores
0.1695896528	a taxonomy
0.1694860104	an important issue
0.1694304691	a probability distribution
0.1692836360	early detection of
0.1692374672	the target language
0.1689170219	support system
0.1687610361	show thatthe proposed
0.1687378870	an optimal solution
0.1686617649	on three datasets
0.1685857297	an agent
0.1685024052	in vivo
0.1684931476	algorithm based on
0.1683118328	tend to
0.1682973774	on several datasets
0.1682471695	different views
0.1682089389	more precisely
0.1681485782	spite of
0.1681447774	to end speech
0.1680215685	general approach
0.1679839625	approach based on
0.1678762833	build upon
0.1678702997	recognition problems
0.1677779059	based object
0.1677016211	built on top
0.1676671402	method achieved
0.1673899337	a pre trained
0.1672369354	to detect
0.1669526107	method outperforms state of
0.1669472258	on two benchmark
0.1668933457	this paper wepresent
0.1667459874	performs well
0.1666617649	on two datasets
0.1666200034	the search space
0.1665742977	this paperwe present
0.1664045226	proposed approaches
0.1663013144	more interpretable
0.1662971994	a promising approach
0.1662930138	belong to
0.1662838128	neural network dnn
0.1661589019	an extensive
0.1660191064	relying on
0.1659718230	a probabilistic
0.1659477174	reconstruction using
0.1658474315	theuse of
0.1657226868	carlo methods
0.1656514086	processing methods
0.1655845199	motivated by
0.1654910547	a parameterized
0.1654613180	mapping between
0.1653901640	a framework
0.1653642801	participated in
0.1653454947	a sequence to
0.1653271600	the effectivenessof
0.1652830752	a gaussian process
0.1652134118	aset of
0.1651841617	a knowledge base
0.1651587504	to sequence model
0.1650634414	the art accuracy
0.1650327086	a small number
0.1650257851	an epsilon
0.1646272085	the art models
0.1646167216	this paperwe propose
0.1644044899	order methods
0.1643368022	self learning
0.1643258762	general case
0.1642931522	makes use of
0.1641628349	the input image
0.1641607134	3d cnn
0.1640612544	in depth analysis
0.1638838018	a novel neural network
0.1638396890	an unbiased
0.1638225963	a low rank
0.1637470466	a comparison
0.1636908174	the original image
0.1636861693	outperforms existing
0.1636567439	results showing
0.1636182129	give rise to
0.1636174901	serves as
0.1634752804	most relevant
0.1634636357	method consists
0.1632516265	the discriminative power
0.1631429405	an explicit
0.1629684260	doing so
0.1628508250	a formal
0.1628061301	this chapter
0.1627878233	based search
0.1627681384	overall accuracy
0.1626895876	gives rise to
0.1624236749	an introduction
0.1623966356	small fraction of
0.1622925853	overstate of
0.1621866793	the training set
0.1621523489	to generate
0.1620703012	a challenging problem
0.1619999778	non standard
0.1617623821	learning community
0.1617536839	to compute
0.1616638894	shed light on
0.1616183171	the bethe
0.1614762231	the global minimum
0.1614481067	faster and more
0.1614020584	the proposed method outperforms
0.1613955361	applied to
0.1613215832	more attention
0.1612897613	models trained on
0.1612878233	based inference
0.1611978062	each stage
0.1611655068	to predict
0.1610450694	governed by
0.1607539452	most likely
0.1607071270	the objective function
0.1603873184	correspond to
0.1601613998	the euclidean distance
0.1600589562	a comparative
0.1600400766	affected by
0.1600048079	goodness of
0.1598336568	on several real
0.1598232752	an algorithmic
0.1597418130	method on two
0.1597027463	a geometric
0.1596280132	this paper considers
0.1595766781	method to learn
0.1595637664	on two publicly
0.1595302796	set of
0.1595298157	converted into
0.1595123647	an automatic
0.1594089948	larger than
0.1593614379	sub network
0.1592103739	a real world
0.1591576572	compared to previous
0.1591033437	a regret bound
0.1590761685	experiment results show
0.1589302891	the proposed algorithms
0.1588878027	an initial
0.1587294188	at different scales
0.1586552750	able to learn
0.1586247630	each layer
0.1584809250	best first
0.1584675114	neuralnetwork cnn
0.1584512421	scale machine learning
0.1584500236	each frame
0.1582323633	as aresult
0.1581948402	a detailed analysis
0.1581801725	particularly suitable
0.1580619981	linear and non
0.1580553877	forward neural networks
0.1580441188	variety of applications
0.1578658474	for large scale problems
0.1578091369	relationship among
0.1577139316	aims at
0.1576018938	this paper develops
0.1575427179	performance compared
0.1574911257	widerange of
0.1574289633	recent work
0.1573968186	a dual
0.1571275352	depends on
0.1571167065	at least
0.1570817305	asubset of
0.1570802359	compared with existing
0.1570116454	the multi armed bandit
0.1569532635	the research community
0.1567652084	a challenging task
0.1564567682	3d objects
0.1563693288	still remains
0.1563233619	todeal with
0.1562596565	broad class of
0.1562519443	deals with
0.1562222051	based only on
0.1558089048	more stable
0.1557591365	of great importance
0.1556702797	most notably
0.1553178232	a single
0.1551418034	the first step
0.1551237775	scale problems
0.1550092468	superior performance over
0.1546749634	series analysis
0.1546234377	number of features
0.1546036614	across languages
0.1545742944	the state space
0.1543468014	correlations between
0.1542631580	problem of identifying
0.1542524247	existing multi
0.1541501911	the partition function
0.1541447525	if and only if
0.1540144480	focuses on
0.1539115996	al 2016
0.1539008520	the original
0.1537496196	focus on
0.1537231871	this paperwe
0.1536935799	to thisend
0.1536550890	datasets shows
0.1535607752	to classify
0.1533688305	specific data
0.1533596553	large variety
0.1532841208	as well asthe
0.1532675135	characterized by
0.1532349677	methods perform
0.1532348192	a finite number
0.1531930939	a complete
0.1531224136	an integer
0.1531056199	art performance on
0.1530074268	network called
0.1529221083	a simple method
0.1528395280	the proposedmethod
0.1526416977	at different levels
0.1525519338	obtain state of
0.1525383226	error rate of
0.1523230497	accordance with
0.1523040884	a convolutional network
0.1521690304	a stationary point
0.1521590784	model to predict
0.1521149994	proposed scheme
0.1520509886	the internet
0.1519681728	to overcome
0.1519632894	method takes
0.1519203642	huge amount
0.1518877108	on two real
0.1518575314	emerged as
0.1518378655	results on two
0.1517269983	the input data
0.1517089404	single network
0.1516940388	the imagenet dataset
0.1516578128	consisting of
0.1516377325	problem in computer vision
0.1516363505	mean average
0.1516118229	the main challenges
0.1516090610	dimensional problems
0.1515989898	interacts with
0.1515527058	recent progress in
0.1515338944	much faster
0.1515032025	more realistic
0.1514910657	capture long
0.1514495989	a riemannian
0.1514461354	less than
0.1514235389	depend on
0.1513899680	due to
0.1512866954	a simple model
0.1511702379	information provided
0.1511532400	acombination of
0.1511363400	a high dimensional
0.1510661871	much fewer
0.1510371870	quantitative analysis of
0.1509885762	currentstate of
0.1509610983	the attention mechanism
0.1506538081	automatic detection of
0.1506276952	extraction using
0.1504917389	lower bound for
0.1503487873	deal with
0.1502331387	assumptions about
0.1501613840	model semantics
0.1501556235	generation using
0.1501342106	the fly
0.1501120001	a broad class
0.1500321665	the marginal likelihood
0.1499950925	this problem
0.1499950346	efficient algorithms for
0.1499330291	conduct experiments on
0.1498709464	to obtain
0.1498604252	each cluster
0.1498061444	a vital role
0.1496635245	in many cases
0.1495004637	different scales
0.1494932725	dataset consisting
0.1494734067	much attention
0.1494188043	inthe context
0.1493215525	2d image
0.1491340456	a transfer learning
0.1490923600	special case of
0.1490836587	perform well
0.1488607882	within class
0.1487360922	a new measure
0.1485994606	the input space
0.1483371663	to identify
0.1483323899	based approach for
0.1483216032	an image
0.1483022252	present results
0.1482658609	training time
0.1481740115	a new model
0.1481629278	based feature
0.1479780064	the proposed method achieves
0.1478533628	decoder model
0.1478473841	to optimize
0.1477872237	detection based on
0.1476572913	a hierarchical
0.1476490556	an important component
0.1476237272	endowed with
0.1475476558	influenced by
0.1475138197	anumber of
0.1474993284	various kinds
0.1473151679	both worlds
0.1473048290	represented by
0.1470634566	second stage
0.1467422392	to extract
0.1467008876	a computational
0.1465852041	a significant impact
0.1464438739	while preserving
0.1464175716	to infer
0.1462264007	significant amount
0.1461597194	and cifar 100
0.1460226844	resort to
0.1460201334	magnitude faster than
0.1460002487	a plethora
0.1459911508	the training data
0.1459776870	art performances
0.1457863943	a high level
0.1457106796	correspondences between
0.1457017123	with respectto
0.1455780383	iscapable of
0.1455684973	inspired by recent
0.1454508325	methods based on
0.1454089231	to recognize
0.1453789926	new challenges
0.1453357467	an algorithm
0.1452824718	results show thatthe
0.1452423372	conditions under
0.1452203815	does not
0.1451815164	a handful
0.1451449296	relation between
0.1448775799	to train
0.1448732442	a lot of attention
0.1448025350	the past few years
0.1447420931	to improve
0.1447089163	the proposed network
0.1446700638	information contained
0.1445518710	extracted from
0.1444267612	paper deals with
0.1443544950	the factthat
0.1442787303	close to
0.1442485769	do not
0.1442112728	a multilingual
0.1440099718	automatic generation of
0.1439133564	media data
0.1437031424	an adversarial
0.1435126178	the problem of estimating
0.1433890183	followed by
0.1433802515	the art method
0.1432551637	simulation results show
0.1432483435	both synthetic and real data
0.1432449047	this workwe
0.1432135001	both synthetic and real
0.1431633844	better than
0.1431005921	an order of
0.1430622528	automatic segmentation of
0.1429435470	a single model
0.1428061310	a small subset
0.1427484470	an objective function
0.1427266567	main challenge
0.1426229573	recent developments in
0.1426057919	a convex optimization
0.1425029199	agent system
0.1424923655	based method for
0.1422461098	significant improvement in
0.1422370056	end to end manner
0.1422334330	this end
0.1420401995	theexistence of
0.1420220029	an optimal
0.1420204824	view images
0.1420143014	prior knowledge of
0.1420059139	large collection
0.1419614148	more general
0.1419509200	the proposedalgorithm
0.1418791358	statistical properties of
0.1418310026	time step
0.1417144384	of view of
0.1416947891	a systematic
0.1415851992	set of objects
0.1415820518	to determine
0.1415817711	key role
0.1415231439	data generating
0.1415175051	the pascal voc
0.1415005794	a uniform
0.1414024810	3d space
0.1413995204	less sensitive
0.1413868865	capable of
0.1413391900	those obtained
0.1410909381	effectiveness of theproposed
0.1410866078	focused on
0.1410089321	recent success of
0.1409967243	networks convnets
0.1409817245	the output layer
0.1409598753	aiming at
0.1408936768	the em algorithm
0.1408835106	interpreted as
0.1407782582	several orders of magnitude
0.1407542387	prior work
0.1406977808	extraction from
0.1406897757	an active
0.1406297422	measure based
0.1405460915	a large collection
0.1404513029	generalization ability of
0.1404457012	al 2017
0.1404019073	three benchmark datasets
0.1402547782	the optimal solution
0.1402358323	take advantage
0.1401474892	the source domain
0.1401155073	more effective
0.1398702154	to reduce
0.1398290310	human decision
0.1397770028	linear combination of
0.1396384284	an optimization problem
0.1393595032	approach for learning
0.1393465120	stems from
0.1393085862	results on several
0.1392816783	the change of
0.1392085227	each group
0.1389903202	general class of
0.1388528326	level image
0.1388375388	a multilayer
0.1387267028	the current paper
0.1386936404	to address
0.1386860499	convergence rate of
0.1386547527	conference on uncertainty in
0.1385575752	each neuron
0.1382465726	generalizes well
0.1382439259	large amount of
0.1382160205	the target distribution
0.1381936843	range of problems
0.1381060178	acollection of
0.1381040970	obtained results
0.1379487382	coming from
0.1378913858	to navigate
0.1378864014	spanned by
0.1377753282	captured by
0.1377037082	to communicate
0.1374827358	derived from
0.1373939415	astate of
0.1372962847	the training process
0.1372816783	the depth of
0.1371243530	the hidden layer
0.1370540608	the original problem
0.1369619805	by introducing
0.1369213319	a first step
0.1368026054	ofthe art
0.1367979597	world scenarios
0.1367611909	results illustrate
0.1365820518	to create
0.1364916102	the human brain
0.1364436516	as opposed
0.1364358024	two level
0.1363942589	limited amount of
0.1363871050	a joint
0.1363251174	the observed data
0.1362443854	dimensional datasets
0.1361676124	a graph based
0.1361372779	to assess
0.1359853565	comparable to state of
0.1358086431	present experimental results
0.1357838369	systems based on
0.1357565869	easy to use
0.1356926889	efficient method for
0.1355287678	based reinforcement
0.1354051462	theoretical framework for
0.1353897908	ofstate of
0.1353728748	a fine grained
0.1353238357	data instances
0.1349682124	series classification
0.1346676246	for semi supervised learning
0.1346511239	the generalization error
0.1346453827	to discover
0.1345858197	second contribution
0.1345443984	the proposed
0.1343614780	world tasks
0.1342546424	the empirical risk
0.1342458522	generated by
0.1342138552	drawn from
0.1341784496	based modeling
0.1341265542	graphical user
0.1341108097	network to predict
0.1341096901	a generative
0.1339616758	a post processing
0.1337380231	able to detect
0.1337300029	results on three
0.1337100785	the art systems
0.1335414165	the optimal policy
0.1334501822	to perform
0.1334315197	automated method
0.1334103707	transformed into
0.1334032780	a wide range of applications
0.1332537168	did not
0.1331180537	very little
0.1330372106	the machine learning community
0.1330200222	two orders of magnitude
0.1329295182	focusing on
0.1329067617	produced by
0.1328935080	the posterior distribution
0.1327412428	a note on
0.1327056374	refer to
0.1326881696	akin to
0.1325977979	model trained on
0.1322552696	size n
0.1322526654	terms of accuracy
0.1322041605	practical use
0.1321999281	idea behind
0.1321590753	significantly better than
0.1321590357	large training
0.1321411307	methods rely on
0.1319605324	videos using
0.1319197141	to handle
0.1319187315	of interest for
0.1317891358	most common
0.1317809859	quality image
0.1317632307	a holistic
0.1317413968	important research
0.1316243497	the expectation maximization
0.1316150353	generalize well
0.1316102859	the source code
0.1315735602	the joint distribution
0.1314199451	the proposedframework
0.1313474503	bag of
0.1313347762	the first stage
0.1312831028	effective approach
0.1312607629	art algorithms
0.1312275423	an optimized
0.1312247721	version of
0.1310440977	thesuperiority of
0.1310427510	an important
0.1310250449	the model parameters
0.1310084529	to achieve
0.1309285220	survey on
0.1308993191	higher than
0.1306482014	making use of
0.1306435239	framework for multi
0.1305643481	induced by
0.1304171199	effective method
0.1303939519	compared with
0.1303827422	most existing methods
0.1303534104	dataset contains
0.1303501133	very low
0.1303166506	an experiment
0.1303078477	paper wepresent
0.1301688961	workshop on
0.1301532907	a hybrid
0.1301033485	problems in computer
0.1299613024	a widevariety of
0.1299427306	the same
0.1298662622	the cost function
0.1296694857	the main
0.1294939601	compared to state of
0.1294097616	a compositional
0.1293974519	very small
0.1293926235	the long term
0.1292577941	into consideration
0.1292541221	by incorporating
0.1292401382	new method
0.1290806156	more specifically
0.1289994519	improvement over
0.1289855000	a logical
0.1289408396	processing techniques
0.1289153851	to select
0.1288946545	few years
0.1287701556	the art deep
0.1287439296	much more
0.1287216572	two fold
0.1287037638	recognition datasets
0.1286518792	new architecture
0.1284540451	an arm
0.1284479043	time cost
0.1284334541	dimensional representation
0.1283488255	more complex
0.1282720015	based speech
0.1282195371	first and second
0.1281836301	too large
0.1281041285	a serious
0.1280975642	to automate
0.1280323226	differs from
0.1279856094	each class
0.1279724432	show thatour
0.1279641114	an advanced
0.1279105736	in thispaper
0.1279077553	art results on
0.1279001059	led to
0.1278329127	the curse of dimensionality
0.1277597757	access to
0.1277540087	truth data
0.1276245481	the proposedapproach
0.1274281291	efficient method
0.1272852060	source software
0.1270846254	still images
0.1268405813	experiments on benchmark
0.1268235745	this approach
0.1268215581	proposed solution
0.1267969293	the proposedmodel
0.1267414359	to reconstruct
0.1267052592	existing image
0.1264507183	on one hand
0.1263871088	the original data
0.1263248551	the extracted features
0.1260784496	learning architecture
0.1259531277	paper focuses on
0.1259038830	the general case
0.1258121042	learning algorithm for
0.1258102076	tasks such as image
0.1257657171	accuracy compared
0.1257578181	classifiers based on
0.1256058522	important problem
0.1254847570	theperformance of
0.1254604157	an experimental
0.1254223996	a foundation
0.1253068587	many real world
0.1252760750	the wild
0.1252127041	translated into
0.1252075170	to produce
0.1251546819	statistical analysis of
0.1251145623	an implicit
0.1250815073	previous state
0.1250796140	a quantitative
0.1250143261	original problem
0.1249550193	a personalized
0.1249341439	consists of
0.1248606663	the sample size
0.1248349162	the obtained results
0.1248261234	distributions over
0.1247357614	research topic in
0.1245907101	an item
0.1244402116	each word
0.1244035453	the entire
0.1243817083	state ofthe
0.1243498923	represented as
0.1243151859	an object
0.1241907692	the risk of
0.1241636273	compared with state of
0.1241181078	previously known
0.1241116599	deep learning framework for
0.1240980607	this survey
0.1240644693	capture data
0.1240366346	the rapid development
0.1240231367	images taken
0.1239817249	perform better than
0.1239406755	the main objective
0.1239226155	concerned with
0.1238166844	an unknown
0.1236656898	shared across
0.1236399144	in order to overcome
0.1236196291	scheme based on
0.1235896506	a scalable
0.1235397357	a robust
0.1234556467	dealt with
0.1234363727	to capture
0.1233534356	1 2
0.1233137172	in answer set programming
0.1232897242	thedevelopment of
0.1232395452	multiple time
0.1229781878	the point of
0.1227348089	each agent
0.1226676945	algorithm to learn
0.1226562921	al 2012
0.1224888281	applicable to
0.1224414404	among others
0.1223350885	results obtained by
0.1221978390	the loss function
0.1221841325	to extract features
0.1221501526	to ensure
0.1221447068	the knowledge base
0.1221135541	based on convolutional neural
0.1221086950	set of parameters
0.1220815059	a flexible
0.1220365997	do not need
0.1219219702	whether or not
0.1218456589	the main result
0.1218090931	the most important
0.1217812682	a new dataset
0.1217599764	simple way
0.1217593644	diverse set of
0.1215543238	very similar
0.1215408503	to build
0.1215037775	formulated as
0.1214317190	the above mentioned
0.1213227541	results indicate
0.1213079160	made publicly available
0.1211978984	a large
0.1211561236	consist of
0.1209938923	for person re identification
0.1208827329	world dataset
0.1207644144	during inference
0.1207592182	supervised model
0.1206375999	a multi layer
0.1205281552	of interest roi
0.1205159407	to maximize
0.1204794706	to recover
0.1204186975	portion of
0.1204042447	dependencies between
0.1203848035	a random forest
0.1203302255	data collected from
0.1203266031	looking at
0.1202972493	world settings
0.1201536481	last years
0.1200404599	method relies
0.1200280880	end training
0.1199734157	to go
0.1199132046	one million
0.1198462064	the aforementioned
0.1196572016	theusefulness of
0.1195968924	supervised machine
0.1195508807	much lower
0.1194442172	lower bound of
0.1194136791	to end framework
0.1193628523	an essential
0.1193146598	to estimate
0.1192937085	superiority over
0.1192815663	through extensive experiments
0.1192245909	great success in
0.1192123072	the second stage
0.1190909506	based sequence
0.1190004451	propose to use
0.1189944456	as well as
0.1189868595	much higher
0.1188812955	approach to solve
0.1188622741	problem in computer
0.1187714873	theamount of
0.1186966719	introduction to
0.1186792878	the experimental results
0.1186376002	able to capture
0.1186155344	alternating direction method of
0.1186022128	a practical
0.1185660472	a markov decision process
0.1185545147	to evaluate
0.1185377352	network to learn
0.1185020669	an attention
0.1183748652	a randomized
0.1183708856	a minimal
0.1183512440	general class
0.1183426402	empirical analysis of
0.1183152197	np hard in
0.1182640150	recent advances in deep
0.1182078828	good performance
0.1182009454	community detection in
0.1181865100	more compact
0.1181656633	earlier work
0.1181376700	an expert
0.1181086476	anomaly detection in
0.1179936903	extensively used
0.1179646193	the robot to
0.1179050452	distribution over
0.1178726104	large collection of
0.1178709050	to minimize
0.1178667647	the intersection of
0.1178667647	the price of
0.1178180905	across multiple
0.1177501222	to tackle
0.1177009098	important role in
0.1176653758	the area of
0.1176585217	the art performance on
0.1174720794	an analysis
0.1174515736	in order to
0.1173978029	look at
0.1173182236	accompanied by
0.1172531317	independent interest
0.1172389819	approach to
0.1171374853	several orders of
0.1169475166	important tool
0.1168550065	amount of labeled
0.1168403393	theeffectiveness of
0.1167731731	the new model
0.1166949016	a novel hybrid
0.1166288063	comparison between
0.1164885925	the maximum likelihood
0.1164880861	trained on
0.1162513547	on real world datasets
0.1161957704	used to train
0.1161847322	the european
0.1161470212	complex real
0.1161416745	the state ofthe art
0.1161250559	each view
0.1161169773	investigate whether
0.1160873640	interaction between
0.1160853090	the agent to
0.1160489383	make use of
0.1160470689	lower than
0.1159852061	able to
0.1159254592	conditioned on
0.1158725296	to end
0.1158667647	the novelty of
0.1158513354	to accelerate
0.1158185394	with respect to
0.1158166501	fraction of
0.1157990616	a fair
0.1157877011	divergence between
0.1157424978	use cases
0.1157096448	written by
0.1156268811	thequality of
0.1156219094	the latent space
0.1155832881	relatively simple
0.1154582047	the probability distribution
0.1154066292	the interplay
0.1153791459	a key challenge
0.1153336034	to better understand
0.1153261477	novel architecture
0.1152485769	of interest in
0.1152347243	end to
0.1152131026	learning frameworks
0.1151547243	the origin
0.1151521170	related to
0.1151449449	give rise
0.1151302417	the training phase
0.1150853090	the region of
0.1149777362	forward neural
0.1149653367	the formation of
0.1148806288	an end to
0.1148596515	obtained by
0.1148568614	to avoid
0.1146876652	integrated into
0.1146585179	fold first
0.1144979123	the limitation of
0.1144403470	the last few years
0.1144366676	equipped with
0.1143878466	classified as
0.1142596189	the essence
0.1141021854	a widerange of
0.1139320258	to retrieve
0.1139301951	a fundamental task
0.1139097492	thepresence of
0.1138899625	huge amount of
0.1138667647	the calculation of
0.1138667647	the popularity of
0.1138379922	vulnerable to
0.1137861813	the art results on
0.1137549081	prior over
0.1135618294	maximum mean
0.1134951222	method for
0.1134875566	space rkhs
0.1133966805	different languages
0.1133956104	much faster than
0.1133252244	on three benchmark
0.1132338217	3d convolutional neural
0.1129536382	competitive results on
0.1128667647	the significance of
0.1127502596	such as
0.1125850573	definition of
0.1125329008	this paper weintroduce
0.1125083946	a simple yet effective
0.1123290651	the problem of learning
0.1122813172	the feature space
0.1121794085	divided into two
0.1121352199	time scales
0.1119979123	the promise of
0.1119805707	other fields
0.1119637531	the reconstruction error
0.1119301843	the learned model
0.1118564668	alternative approach
0.1118489908	fragment of
0.1117341559	even if
0.1116962459	original model
0.1116783583	able to generate
0.1116613609	a real time
0.1116092249	an important problem
0.1115482161	a two step
0.1114778758	a group of
0.1114233937	small set of
0.1114190040	replaced by
0.1113551188	formalization of
0.1112701117	the proposed models
0.1112555245	the step size
0.1112497458	contrary to
0.1111791035	optimal number of
0.1111747049	a virtual
0.1111310963	strategy based
0.1110220111	consists of three
0.1109979123	the aid of
0.1109492240	promising approach
0.1108438168	shown to
0.1107935396	technique based on
0.1107825072	the pre trained
0.1107639882	a differentiable
0.1107292811	tasks such as
0.1107021050	a novel technique
0.1105900888	more informative
0.1104898138	small subset
0.1103744276	the exploration of
0.1103744276	the magnitude of
0.1103744276	the property of
0.1103238091	under mild
0.1102558412	some recent
0.1101567748	paper extends
0.1101090234	the heart of
0.1099864609	on simulated data
0.1099765162	3d structure
0.1099184260	in practice
0.1098709050	to enhance
0.1098292554	in most cases
0.1098237086	the most prominent
0.1097111532	to distinguish
0.1093667647	the spectrum of
0.1093667647	the occurrence of
0.1093590201	gives rise
0.1093217116	a novel method
0.1093168855	to sequence models
0.1093051011	in order to obtain
0.1092860579	algorithm for
0.1092705777	the art object
0.1091090234	the course of
0.1090447801	easy to
0.1090078671	extensive experiments show
0.1089584294	theoretical analysis of
0.1088772915	in orderto
0.1087966014	the art approach
0.1087932242	a model based
0.1087806480	a small set
0.1087454740	an effective way
0.1087220687	based model for
0.1087206558	with millions of
0.1086601218	compared against
0.1086141620	a key role
0.1084979123	the rise of
0.1084778758	the identity of
0.1084334670	sample complexity of
0.1084073265	learning tools
0.1083744276	the runtime of
0.1083744276	the flow of
0.1083744276	the sensitivity of
0.1083744276	the limit of
0.1083481766	characterization of
0.1083276678	connection between
0.1082537888	broad range of
0.1081518361	this paper studies
0.1081143001	an entropy
0.1080044516	notion of
0.1079748180	a cost function
0.1079238382	different sets of
0.1079238382	of thousands of
0.1078701266	the test set
0.1078652456	three main
0.1076890737	very few
0.1075084243	the state of
0.1074761639	of convergence of
0.1074653367	the interpretability of
0.1073879482	reliance on
0.1073868635	variety of problems
0.1073470542	to track
0.1073130106	piece of
0.1072914221	the literature
0.1072382161	a single network
0.1072061358	treated as
0.1071877846	an architecture
0.1070263968	small subset of
0.1070143820	per image
0.1069593575	interact with
0.1069411169	both training and
0.1069190184	dynamic time
0.1068319065	empirical results on
0.1067969158	a universal
0.1067791028	several benchmark datasets
0.1066915272	a formalism
0.1066387303	the proposed approaches
0.1066307851	a novelapproach
0.1066090234	the strength of
0.1065071187	the backbone
0.1064979123	the requirement of
0.1064778758	the outcome of
0.1064625953	the effects of
0.1063923951	this manuscript
0.1063744276	the topology of
0.1063744276	the topic of
0.1063646528	a multi task
0.1062915486	analysis of
0.1061400444	become increasingly
0.1061151998	adoption of
0.1060643379	a sparse representation
0.1060488128	the first attempt
0.1060468076	to image translation
0.1060316703	the sample complexity
0.1060006807	different domains
0.1059693268	method to solve
0.1058228697	efficient algorithm for
0.1055810614	in isolation
0.1055443710	high degree of
0.1055258188	peak signal to
0.1055079939	the word level
0.1054979123	the extent to
0.1054971949	by up to
0.1054778758	the reliability of
0.1054778758	the composition of
0.1054653367	the style of
0.1054653367	the regret of
0.1054515736	in terms of
0.1054075561	relied on
0.1053667647	the degree to
0.1053667647	the behaviour of
0.1053263820	lower computational
0.1052913706	empirical evaluation of
0.1052664871	an elegant
0.1052218029	an arbitrary
0.1052158868	to improve performance
0.1051814559	a supervised learning
0.1051804504	theidentification of
0.1051385820	each step
0.1051098812	by adding
0.1050601043	scale dataset
0.1050129419	works well
0.1050032214	the convergence rate
0.1048550958	different levels
0.1048254499	both accuracy and
0.1048156960	the university
0.1047875260	on synthetic and real data
0.1047206558	between pairs of
0.1046796293	to achieve high
0.1046488062	based on deep learning
0.1046391267	thesuccess of
0.1046385485	the semantics of
0.1045780110	the problem of finding
0.1045728784	good quality
0.1045701152	growing interest in
0.1045698143	the estimation of
0.1045528503	variety of tasks
0.1045242135	a large set
0.1044653367	the volume of
0.1043930744	the final
0.1043744276	the diversity of
0.1043744276	the interpretation of
0.1043727548	prevalence of
0.1043667647	the other two
0.1043667647	the extent of
0.1042894653	and real world data
0.1042737118	presented here
0.1042195516	limited number of
0.1042149157	kind of
0.1041516942	cope with
0.1041350293	sum of
0.1040737467	interested in
0.1039843593	more sophisticated
0.1039833616	in order to improve
0.1039777680	distinguish between
0.1039499343	the training dataset
0.1039333029	a statistical
0.1038829418	3d hand
0.1038754605	a polynomial time
0.1038079057	amount of training data
0.1037911070	by applying
0.1037643349	more difficult
0.1037074123	number of hidden
0.1036575169	a technique to
0.1036539973	existence of
0.1036346644	a series of
0.1036090234	the flexibility of
0.1036090234	the introduction of
0.1035923140	significantly better
0.1035670729	the object of
0.1035425407	in other words
0.1035362911	able to achieve
0.1034653367	the fusion of
0.1034653367	the precision of
0.1034653367	a robot to
0.1033744276	the description of
0.1033744276	the meaning of
0.1033667647	the direction of
0.1033667647	the scalability of
0.1033667647	the improvement of
0.1033418092	the dynamics of
0.1033402865	this task
0.1033320039	to look
0.1032889205	this issue
0.1032742664	to end fashion
0.1031760840	composed of
0.1031438158	of magnitude faster
0.1031225102	an entity
0.1030926507	to end learning
0.1030493963	aim at
0.1030228567	similar to
0.1029792811	experiments on
0.1029705250	with up to
0.1029694482	the latter
0.1029082012	mixture of
0.1028513023	accuracy compared to
0.1028405086	an approach
0.1028304925	more powerful
0.1027888231	important task
0.1027720298	this paper aims
0.1027243578	based algorithm for
0.1026489456	a second order
0.1026053323	while avoiding
0.1025909780	modeled as
0.1025599978	built from
0.1025524025	benefit from
0.1024789052	the interest of
0.1024778758	the relevance of
0.1024778758	the position of
0.1024778758	the contribution of
0.1024778758	the usage of
0.1024653367	the uncertainty of
0.1024624561	expressiveness of
0.1024571993	the classification accuracy
0.1024506453	neural network architecture for
0.1024356202	the network structure
0.1023965620	to thebest of
0.1023872978	future work
0.1023786893	more flexible
0.1023744276	the product of
0.1023744276	the recovery of
0.1023744276	the principle of
0.1023744276	the influence of
0.1023667647	a population of
0.1022747936	while requiring
0.1022509001	a means to
0.1020813507	determined by
0.1020200559	vector regression
0.1018441663	very popular
0.1018391626	four types
0.1018350247	shows promising
0.1018038443	f score of
0.1017479460	learning framework for
0.1016792797	a novel feature
0.1016692534	more generally
0.1015753787	hardness of
0.1015698143	the face of
0.1015319920	model consists
0.1015293032	platform for
0.1014778758	the sense of
0.1014778758	the difficulty of
0.1014653367	the diagnosis of
0.1014653367	the motion of
0.1014436118	acts as
0.1014418603	the most challenging
0.1013744276	the appearance of
0.1013744276	the discovery of
0.1013744276	the location of
0.1013739947	this paper reports
0.1013546105	combined with
0.1010907475	ageneralization of
0.1010794385	better accuracy
0.1010760741	devoted to
0.1010358930	a modified
0.1010167632	improvements over
0.1007584670	theapplication of
0.1007506723	very effective
0.1007075170	to represent
0.1006735039	this study
0.1006397890	the data into
0.1006397890	the task at
0.1006278012	to alleviate
0.1006079700	each document
0.1005823535	the focus of
0.1005698143	the image in
0.1005698143	the system in
0.1004778758	the purpose of
0.1004778758	the second one
0.1004778758	the importance of
0.1004778758	the construction of
0.1004712423	a process of
0.1004712423	the way to
0.1004653367	the frequency of
0.1004653367	the ratio of
0.1004653367	the length of
0.1004426835	a central role
0.1004181495	to accomplish
0.1003744276	the concept of
0.1003744276	the dimensionality of
0.1003667647	the history of
0.1003490374	large variety of
0.1003055618	each action
0.1002977534	a lightweight
0.1001804504	susceptible to
0.1001491559	a declarative
0.1000769853	a high quality
0.1000685019	the trained model
0.0999962596	used to generate
0.0999944702	two orders of
0.0999578312	the current state
0.0999572673	the latent variables
0.0999339611	two sets of
0.0997890065	theefficacy of
0.0997718081	to mitigate
0.0997636408	the multi armed
0.0997309742	respond to
0.0996966993	difficult to
0.0996295495	by exploiting
0.0996090234	the benefit of
0.0996090234	a pair of
0.0995823535	the challenge of
0.0995823535	the power of
0.0995823535	the addition of
0.0995698143	the information in
0.0995698143	the model with
0.0995698143	the algorithm on
0.0995698143	the control of
0.0995282878	new dataset
0.0994905199	starting from
0.0994789052	the setting of
0.0994789052	the processing of
0.0994789052	a function of
0.0994778758	the potential to
0.0994712423	the way for
0.0994653367	the minimization of
0.0994653367	the variance of
0.0994436293	a novel multi
0.0993714568	in order to achieve
0.0993667647	the subject of
0.0993340174	the problemof
0.0992196774	derivation of
0.0991892160	this work
0.0991266054	the input images
0.0991120786	in order to reduce
0.0990179831	art approaches
0.0990147881	this paper shows
0.0990130189	a system to
0.0989973653	to sequence learning
0.0989527224	very successful
0.0989314076	framework for
0.0988839800	both simulated and real
0.0988191584	more accurately
0.0987679274	performance in terms
0.0987639335	3d human
0.0986090234	the present work
0.0986023899	the potential for
0.0985823535	a dataset of
0.0985823535	a sequence of
0.0985823535	a measure of
0.0984789052	the source of
0.0984789052	the potential of
0.0984778758	the basis of
0.0984778758	the idea of
0.0984778758	a hierarchy of
0.0984778758	the utility of
0.0984778758	the perspective of
0.0984778758	the help of
0.0984712423	a sample of
0.0984653367	the stability of
0.0984653367	the reconstruction of
0.0984653367	the entropy of
0.0984653367	the geometry of
0.0984653367	the evolution of
0.0984238366	thesize of
0.0983744276	the question of
0.0982484362	to remove
0.0981931629	to construct
0.0981379771	member of
0.0981237659	to interpret
0.0981235529	both real and
0.0980997966	a lot
0.0980438301	increasing interest
0.0979922705	as opposed to
0.0979509535	the performanceof
0.0979322103	network trained on
0.0979184324	much less
0.0979150891	an exact
0.0979032596	obtained from
0.0978543188	by proposing
0.0978261673	an increase in
0.0978000533	a generic
0.0977875971	a principled
0.0977870449	the cumulative
0.0977589153	a social network
0.0977206558	in relation to
0.0976090234	the capability of
0.0976090234	a generalization of
0.0976060052	respect to
0.0975921948	our approach
0.0975698143	the method on
0.0975113525	not limited to
0.0974789052	the key to
0.0974789052	the objective of
0.0974778758	the aim of
0.0974653367	the consistency of
0.0974653367	the likelihood of
0.0974653367	the identification of
0.0974653367	the uncertainty in
0.0974653367	the sparsity of
0.0973691643	learned from
0.0973479935	insights about
0.0972602564	deviations from
0.0972206558	an estimate of
0.0971428066	each user
0.0971084650	clearly demonstrate
0.0970127241	the resultant
0.0969183944	a critical
0.0969037361	robust against
0.0969009206	to choose
0.0968854448	an extremely
0.0968732103	a coarse to fine
0.0967920844	to appear in
0.0967206558	in light of
0.0966382756	finite set of
0.0966090234	a total of
0.0966090234	in response to
0.0966057041	the one hand
0.0966023899	a means of
0.0966023899	in term of
0.0965823535	an accuracy of
0.0965698143	the method to
0.0965698143	the solution to
0.0965698143	the system to
0.0964789052	the context of
0.0964789052	the need of
0.0964789052	the output of
0.0964778758	the basis for
0.0964778758	a database of
0.0964712423	and up to
0.0964653367	the behavior of
0.0964653367	the robustness of
0.0964653367	the capacity of
0.0964412327	incomparison with
0.0963916113	accounting for
0.0963667647	the assumption of
0.0963127963	our results demonstrate
0.0963051347	two stages
0.0962898463	methods in terms
0.0962717991	an upper bound on
0.0962544967	classified into
0.0962007195	not clear
0.0961630147	an artificial
0.0960592874	the graph structure
0.0959058410	in comparison to
0.0958859225	much more efficient
0.0958201902	applications such as
0.0958180210	consists of two
0.0958044043	on several benchmark
0.0958029998	an alternative approach
0.0957821214	while achieving
0.0957615032	to implement
0.0957584670	thestructure of
0.0957370029	performs better
0.0956756006	equivalent to
0.0955958484	isthe first
0.0955839083	absence of
0.0955698143	the model of
0.0955698143	the modeling of
0.0955698143	the approach on
0.0955698143	the model on
0.0955698143	the segmentation of
0.0955150754	significant impact
0.0954778758	the issue of
0.0954778758	the core of
0.0954712423	of interest to
0.0954653367	the content of
0.0954172265	different types
0.0953771461	able to predict
0.0953667647	a description of
0.0952367821	expressed as
0.0951922486	the learned features
0.0951520151	received much
0.0950157284	expressive power of
0.0949772696	on top of
0.0949326471	the most common
0.0949269350	learning method for
0.0949155466	assigned to
0.0947696874	in conjunction with
0.0947444292	a combinatorial
0.0947317198	a new architecture
0.0947209164	nature of
0.0947082922	to facilitate
0.0946381235	used to evaluate
0.0946351283	most existing
0.0946090234	a factor of
0.0945823535	in case of
0.0945823535	a system for
0.0945823535	a study on
0.0945795042	overview of
0.0945791428	more than
0.0945698143	the theory of
0.0945698143	the probability of
0.0945698143	the size of
0.0945698143	the training of
0.0945698143	the scale of
0.0945698143	the resolution of
0.0945698143	the loss of
0.0945698143	the evaluation of
0.0945698143	the domain of
0.0945698143	the level of
0.0945698143	the accuracy of
0.0945698143	a model to
0.0945698143	the error of
0.0945698143	the design of
0.0945698143	the structure of
0.0945698143	the user to
0.0945423590	important step
0.0945387511	if and only
0.0945290076	degree of
0.0945027634	gradient descent with
0.0944900683	using deep convolutional neural
0.0944818730	to guide
0.0944789052	the result of
0.0944789052	the comparison of
0.0944789052	a representation of
0.0944789052	the search for
0.0944789052	the architecture of
0.0944786028	current paper
0.0944764109	new benchmark
0.0944653367	the dimension of
0.0944261673	one way to
0.0944242187	proven to
0.0943807113	information from
0.0943515944	to end approach
0.0943258347	a new multi
0.0942598439	amenable to
0.0942409343	a series of experiments
0.0942098266	our results suggest
0.0941909677	popular approach
0.0941647653	the classification performance
0.0941268518	different layers
0.0941258103	emergence of
0.0941162453	used to represent
0.0941113525	in combination with
0.0940485433	small amount of
0.0940418151	simpler than
0.0940078959	a heuristic
0.0939223619	model for
0.0939058410	so as to
0.0939058410	as input to
0.0938766251	to end deep
0.0938620043	to visualize
0.0938079004	very expensive
0.0937973383	the target
0.0937824858	at least one
0.0936797216	ability to
0.0936546300	the test data
0.0936248247	do not require
0.0935823535	a solution to
0.0935823535	a result of
0.0935698143	the study of
0.0935698143	the task of
0.0935698143	the space of
0.0935698143	the cost of
0.0935698143	the framework of
0.0935698143	the detection of
0.0935698143	the process of
0.0935698143	the prediction of
0.0935698143	the extraction of
0.0935698143	the end of
0.0935238930	large set of
0.0934846527	an interesting
0.0934789052	the understanding of
0.0934789052	the generation of
0.0934766430	comment on
0.0934653367	the shape of
0.0934653367	the speed of
0.0934259884	provided by
0.0934210083	a long short term
0.0933764365	visual system
0.0932884412	the original model
0.0932838421	an average
0.0932812907	theconstruction of
0.0932432146	large set
0.0931719774	unified approach
0.0931418284	art result
0.0930752040	proportional to
0.0930711470	incomparison to
0.0930122544	uniqueness of
0.0930113525	a basis for
0.0930076731	a challenge for
0.0929960801	adapted to
0.0929245946	solved by
0.0929203243	by combining
0.0928335206	the minority
0.0928312339	impact on
0.0928185394	by means of
0.0927588830	to manipulate
0.0927575629	classification based on
0.0927520083	different sources
0.0927360789	to resolve
0.0927218214	a reliable
0.0926411144	the availability
0.0926381623	advantage over
0.0926310151	the total number
0.0926196696	mapping from
0.0925967677	used to compute
0.0925823535	to scale to
0.0925823535	this method to
0.0925823535	a way of
0.0925742113	show thatthis
0.0925698143	the order of
0.0925698143	the value of
0.0925698143	a system of
0.0925698143	the sequence of
0.0925698143	the gradient of
0.0925698143	the distribution of
0.0925129973	other languages
0.0924872894	a fuzzy
0.0924712423	with application to
0.0924214392	first stage
0.0924115148	the training samples
0.0923003270	two steps
0.0921402566	the true
0.0921126454	list of
0.0920848729	the artperformance on
0.0920197933	a large number of
0.0920113525	as long as
0.0919296922	better results than
0.0919139200	training set of
0.0918261673	and then use
0.0917766886	two main
0.0917761578	subject to
0.0917755979	a support vector machine
0.0917725049	empirical results show
0.0917135010	a way to
0.0916657686	to synthesize
0.0916207639	numerical experiments on
0.0915864529	an augmented
0.0915710908	the most popular
0.0915698143	the computation of
0.0915698143	the classification of
0.0915698143	the similarity of
0.0915698143	the knowledge of
0.0915464222	prone to
0.0914789052	a method to
0.0914608635	sensitive to
0.0914404595	based framework for
0.0914317965	achieved by
0.0914216709	significantly less
0.0913408408	to acquire
0.0913040848	this paper demonstrates
0.0912600626	approach for
0.0912409682	novel approach
0.0911865034	the second step
0.0911216393	set of images
0.0910601558	results show
0.0910297777	a cascade
0.0910018977	realization of
0.0909265807	3d point
0.0909253921	techniques based on
0.0908820096	each individual
0.0908038756	ease of
0.0906622484	performance of
0.0906254056	finite number of
0.0905967677	used to extract
0.0905698143	the rank of
0.0905698143	the rate of
0.0905698143	the convergence of
0.0905698143	the selection of
0.0905698143	a method of
0.0905698143	a study of
0.0905698143	the representation of
0.0905684347	a summary
0.0905036473	a large variety
0.0904532380	on synthetic and real world
0.0904372784	as much as
0.0904172032	a gaussian mixture
0.0903595478	an autonomous
0.0903417219	not just
0.0902626186	challenging task due to
0.0901857676	in thiswork
0.0901617121	better performance than
0.0901035192	this work presents
0.0900620212	the qualityof
0.0900588938	this note
0.0900113525	as good as
0.0899440406	to calculate
0.0898884153	comprised of
0.0898173068	a large class
0.0898099468	a new algorithm
0.0898032380	not only
0.0897719008	in two ways
0.0897579045	modified version of
0.0896650449	an opportunity
0.0896259670	able to handle
0.0895823535	to search for
0.0895784792	a wide range of
0.0895698143	a theory of
0.0895698143	the solution of
0.0895698143	this model to
0.0895491415	reconstruction from
0.0895406692	by formulating
0.0895214490	as soon
0.0894566590	an approximate
0.0894515736	the effectiveness of
0.0894272184	approximated by
0.0894270112	theimportance of
0.0894090782	a multi class
0.0893127721	lack of
0.0893065047	features based on
0.0893031660	collected from
0.0892908548	act as
0.0891923968	subset of
0.0891596714	contribute to
0.0889949579	an entire
0.0889287220	experiments show
0.0888857174	to provide
0.0888793358	diverse set
0.0888598379	both synthetic data
0.0887753082	to collect
0.0887652153	a common
0.0887381407	dominated by
0.0887148806	attracted much
0.0886292106	computational efficiency of
0.0885823535	for use in
0.0885400691	strategy based on
0.0885076731	to work well
0.0884941234	fixed number of
0.0884405438	with respect
0.0884372784	on two different
0.0884280697	very important
0.0882844322	the input
0.0882553685	to end neural
0.0882532181	the computational cost
0.0882371391	implementation of
0.0881879908	to fuse
0.0881705507	convex optimization with
0.0881692201	incorporation of
0.0881527887	this work proposes
0.0881029933	this regard
0.0880817472	more and more
0.0880796642	than existing methods
0.0880514479	tested on
0.0880113525	instead of using
0.0880103186	and so on
0.0879470723	by employing
0.0879090316	several real world
0.0878624150	show thatthe
0.0878154065	one or more
0.0878098253	an action
0.0877820513	better performance
0.0877592949	wefocus on
0.0877091261	on bothsynthetic and real
0.0877055586	the problem
0.0876222569	fundamental task
0.0875443716	a deterministic
0.0874372784	however in many
0.0874097652	a proof of concept
0.0874029281	this limitation
0.0873944904	theimplementation of
0.0873154065	as far as
0.0873142851	the most informative
0.0872641992	each sample
0.0872329553	whencompared to
0.0871574007	an approximation
0.0871446403	emphasis on
0.0870974142	very promising
0.0870781030	to incorporate
0.0870330868	split into
0.0869254725	responsible for
0.0868945735	stored in
0.0867871823	to prevent
0.0867140467	a graphical model
0.0866243917	the experimental results demonstrate
0.0865827212	a variety of
0.0865746856	therobustness of
0.0865587533	rich set
0.0864538379	used to define
0.0864405937	against adversarial
0.0863624135	to enable
0.0863460450	per second
0.0863036832	make decisions
0.0862880262	make predictions
0.0862873364	approximate inference in
0.0861927007	variant of
0.0861797897	presence of
0.0861171724	family of
0.0860963094	used to guide
0.0860818730	to quantify
0.0860304239	a novel deep
0.0859294124	by leveraging
0.0858313914	a general
0.0858007866	the current
0.0857588429	this letter
0.0857252574	evaluated on
0.0857135010	to work with
0.0856862696	very challenging
0.0856450437	the former
0.0855206805	the art classification
0.0854276115	on par with
0.0852553133	limited number
0.0852104721	number of training
0.0850654065	two or more
0.0850644747	each component
0.0850364455	each region
0.0849704724	combination of
0.0849399748	in computer vision
0.0849269619	a tractable
0.0848511565	adapt to
0.0846325743	the same time
0.0846088451	complexity of
0.0846075614	a difficult task
0.0845261825	to adapt
0.0845236737	experimentalresults show
0.0843878242	different ways
0.0843708136	the main contribution
0.0843410762	set of data
0.0843087822	written in
0.0843029606	more accurate than
0.0841552335	a fundamental problem
0.0841124185	the number
0.0840714771	art results
0.0840695043	segmentation based on
0.0839795421	performance on
0.0839415287	at hand
0.0839229267	dependence on
0.0838982730	not always
0.0838581696	data set of
0.0838208844	achieves better
0.0838078332	considered as
0.0837572017	major challenge in
0.0837298544	a survey of
0.0836944806	attempt to
0.0836911718	effectiveness and efficiency of
0.0836691903	an efficient way
0.0836508165	plug in
0.0836479939	beapplied to
0.0835734280	assumed to
0.0835431858	hard to
0.0835013564	the same class
0.0834704692	class of
0.0834327364	useful information
0.0834108283	an edge
0.0834062937	a proof
0.0833974804	a wide variety of
0.0832981085	the k nearest
0.0832801659	the state ofthe
0.0832512093	the problem of
0.0832332246	described here
0.0831938809	new algorithm
0.0831851353	theaccuracy of
0.0831390614	to develop
0.0830104899	feedback from
0.0830022339	the existing methods
0.0829739793	a finite set
0.0829282513	the teacher
0.0828905901	directly from
0.0828437107	experiments indicate
0.0827998491	a theoretical
0.0827740606	by comparing
0.0826966405	integration of
0.0826216364	reasoning with
0.0824777041	type of
0.0824680472	the special case
0.0824255497	to prune
0.0823370087	relative to
0.0823154266	the presence of noise
0.0821974264	availability of
0.0821903404	supported by
0.0820374515	upper bound of
0.0820347598	refer to as
0.0819616472	new way
0.0819548960	the generator
0.0819381474	each category
0.0819304403	the computational complexity
0.0819299872	our experiments demonstrate
0.0819260345	tailored to
0.0818488937	designed to
0.0818485397	to integrate
0.0818031307	advantage of
0.0817591050	thecomplexity of
0.0816953873	defined by
0.0815949705	a discriminative
0.0815512815	a non linear
0.0815330446	an important task
0.0815300543	to locate
0.0814541807	a pre processing
0.0813407864	growing interest
0.0812913864	the number of variables
0.0812668499	a new image
0.0812525302	defined as
0.0811704728	a fixed
0.0811164374	estimated by
0.0811130368	the fact
0.0810525169	variety of
0.0810312977	fed to
0.0810303968	analogous to
0.0809789434	in spite of
0.0808963145	an inverse
0.0808267676	learning to predict
0.0808007142	advances in
0.0807621412	direction method
0.0807230712	by minimizing
0.0807101436	the role of
0.0806556133	yet effective
0.0806433334	to exploit
0.0806212512	learning approach for
0.0806143383	proof of
0.0804777714	almost all
0.0804382944	the trade off between
0.0804032150	in order to solve
0.0804015396	at training time
0.0803282017	also discuss
0.0802860897	in recentyears
0.0802321730	approach to learning
0.0801694636	the problem of identifying
0.0801477683	comparable to
0.0801455527	in order
0.0801387042	thus far
0.0801374574	used to estimate
0.0801358525	the useof
0.0801346592	a set of
0.0801036275	results in terms
0.0801014702	theproblem of
0.0800228594	classification accuracy of
0.0799122720	successfully used
0.0798707710	problems such as
0.0798603118	order to achieve
0.0798351192	important component
0.0797628662	an additive
0.0797399198	a concise
0.0797083461	challenging due to
0.0796947384	ourexperiments show
0.0796445219	each point
0.0795888468	opposed to
0.0795804263	determination of
0.0795373182	quality of
0.0794567567	converge to
0.0794490379	the top k
0.0794242094	more challenging
0.0794010422	order information
0.0792789463	dataset containing
0.0792695923	to encode
0.0791586574	the author
0.0791507122	a useful tool
0.0791497409	an encoder
0.0791493809	a novel framework
0.0790612667	an example
0.0790024076	a significant improvement
0.0789835699	associated with
0.0788977683	suitable for
0.0786447050	paper wepropose
0.0786351032	the data distribution
0.0785903117	factors such as
0.0784553230	an autoencoder
0.0784390058	compatible with
0.0782964800	features such as
0.0782533743	by analyzing
0.0782426037	by modifying
0.0780992340	performance of theproposed
0.0780944195	arising from
0.0780303968	attributed to
0.0780197419	control over
0.0779630345	regardless of
0.0779467900	an intuitive
0.0778898680	formed by
0.0778747770	proportion of
0.0778476926	by making use
0.0777172482	art systems
0.0776825317	new approach
0.0776708344	a new method
0.0776294921	to reach
0.0775937114	problem of image
0.0775746990	the loop
0.0774213133	an additional
0.0773517593	network architecture for
0.0773326709	creation of
0.0773256496	three steps
0.0773000306	a principled way
0.0772428371	the world wide
0.0772381146	a variety
0.0772255682	the performance of
0.0771879606	the presence of
0.0771797203	achieve high
0.0770787617	supposed to
0.0770695786	used to construct
0.0770658043	the ultimate
0.0770602633	application of
0.0770446825	resulted in
0.0770422057	challenging because
0.0770375590	in theliterature
0.0770254770	to deal with
0.0769884561	an input
0.0769857793	on two real world
0.0769734728	by imposing
0.0769493787	the advent of
0.0769354471	the last years
0.0768946907	constructed from
0.0767411337	for training deep
0.0766933494	computational complexity of
0.0766558092	generated from
0.0766539730	corrupted by
0.0766506688	canbe used
0.0766373369	devices such as
0.0766140551	gains over
0.0766107121	to end manner
0.0765916487	this purpose
0.0765021461	in many areas
0.0764380638	concentrate on
0.0763767617	different classes
0.0763482728	optimal value
0.0762960822	this assumption
0.0762939743	general framework for
0.0762846259	kernel k
0.0762399038	guaranteed to
0.0761591680	as much as possible
0.0761196147	to encourage
0.0760905708	thus providing
0.0760879835	a non trivial
0.0760425291	an artificial neural
0.0760085487	more likely to
0.0759316859	consistent with
0.0758498162	contained in
0.0758351777	order to provide
0.0758224066	other state of
0.0757899253	an attractive
0.0757721725	to annotate
0.0757600049	tool for
0.0756791538	a limited number
0.0755300543	to eliminate
0.0755269435	suited for
0.0754969278	very limited
0.0754772946	order to reduce
0.0754245975	the official
0.0753908237	measured by
0.0753706974	aligned with
0.0752951811	to translate
0.0752619002	the rest
0.0752412607	a rigorous
0.0751590508	become popular
0.0751483671	word co
0.0751161802	widely used for
0.0751004924	conducted on
0.0750472932	extension of
0.0750315225	to execute
0.0748702987	the taskof
0.0748042794	over time
0.0747895704	to converge
0.0747871823	to assign
0.0747642957	a survey on
0.0747384675	used to build
0.0746992410	a small number of
0.0746964099	able to identify
0.0746892603	comparative analysis of
0.0745811487	modeled by
0.0745195462	a convolutional neuralnetwork
0.0744657436	a near optimal
0.0744399509	compared to other
0.0743667794	an extension of
0.0743427378	to refine
0.0743236779	form of
0.0743183862	able to produce
0.0742680006	powerful tool for
0.0742663766	suited to
0.0741880506	insensitive to
0.0741178025	a number of
0.0740847054	the human visual
0.0740728282	most informative
0.0740536790	theresults show
0.0740526241	on par
0.0740384014	the trade off
0.0740144415	used to predict
0.0739960180	by utilizing
0.0739065189	defined over
0.0738925866	most popular
0.0738715915	this project
0.0737706922	sampled from
0.0737701341	theefficiency of
0.0737002430	a small
0.0736232420	large class of
0.0735931393	novel method
0.0735626076	used to improve
0.0735515834	more effective than
0.0734746501	to explore
0.0734732794	to distinguish between
0.0734638748	methodology for
0.0734612502	in many applications
0.0734478724	to initialize
0.0734102890	the number of clusters
0.0733845273	trained with
0.0733503224	the prediction accuracy
0.0732240067	a training set
0.0731903822	inference algorithm for
0.0731230497	the image quality
0.0731168127	subclass of
0.0731021279	the desired
0.0729904754	both spatial and
0.0729839690	thecombination of
0.0729352590	the past
0.0729346811	such asthe
0.0728195926	increasing interest in
0.0727315831	yet powerful
0.0726666633	cast as
0.0726413690	in thecontext of
0.0725803267	isevaluated on
0.0725505434	more efficiently
0.0724589553	a high degree
0.0724030787	able to outperform
0.0723145754	increasingly used
0.0722953508	different types of
0.0722951811	to localize
0.0722562335	allowed to
0.0722302057	to analyze
0.0721696983	take advantage of
0.0720649148	much better than
0.0720232175	but also
0.0720196033	many researchers
0.0719005756	analysis based on
0.0718640694	two distinct
0.0718293690	to find
0.0717710656	a set
0.0717572375	no additional
0.0717484756	algorithms such as
0.0716446836	by integrating
0.0716325743	the first time
0.0715961125	model consists of
0.0715207878	to respond
0.0715019929	well suited for
0.0714724759	performed on
0.0714437058	the second part
0.0714397479	to read
0.0714315768	ideas from
0.0713118609	the classification task
0.0712955236	the most successful
0.0712934681	an information
0.0712650369	widely used in
0.0712533143	solely on
0.0712286564	the number of parameters
0.0711851913	to combine
0.0711699503	to regularize
0.0711052633	the use of
0.0710933133	by maximizing
0.0710915002	accounts for
0.0709615614	principled way
0.0709063739	probabilistic model for
0.0708408129	extended to
0.0708169500	methods such as
0.0707653052	better understand
0.0706925767	to stop
0.0706839898	an svm
0.0706344198	isable to
0.0706284019	do so
0.0706152407	created by
0.0706042145	guided by
0.0706032654	progress in
0.0705633434	bound on
0.0704917197	learning to learn
0.0704157535	an exhaustive
0.0703451519	a convolutionalneural network
0.0703358908	trained end
0.0702755361	generative model for
0.0702582082	accomplished by
0.0702456367	a non convex
0.0702437058	the first part
0.0701969473	other types of
0.0701670027	built on
0.0701134674	a deep convolutional neural
0.0701001373	most commonly used
0.0700519721	to explain
0.0700193263	believed to
0.0700005409	particularly useful
0.0699593903	several state of
0.0699437958	an energy
0.0699366493	a new framework for
0.0699239866	further improve
0.0698920914	an ideal
0.0698341184	art models
0.0697823804	work focuses on
0.0697296950	asked to
0.0696490947	the number of
0.0696212536	many applications
0.0696113673	used to
0.0696093816	negative matrix
0.0695731883	to take into account
0.0695382537	the source and target
0.0695098163	an exponential
0.0695057016	theanalysis of
0.0694159727	isapplied to
0.0693984611	constructed by
0.0693789674	embedded into
0.0693525454	proved to
0.0693307696	the effectiveness of theproposed
0.0692583843	impact of
0.0692274232	existing ones
0.0692094825	a significant
0.0691888155	coupled with
0.0691851312	function based on
0.0691667058	the most relevant
0.0691647935	two major
0.0691497265	the state of art
0.0691486872	three major
0.0691249796	identified by
0.0691212104	the curse
0.0690979687	examination of
0.0690369761	to preserve
0.0690051668	computed by
0.0689965701	effect of
0.0689559016	inferred from
0.0689481555	scope of
0.0688630631	to maintain
0.0687855711	performed by
0.0687831478	a simple and effective
0.0686827555	collection of
0.0686039932	role in
0.0685398504	the experiment results
0.0684552923	order to obtain
0.0684465657	the quality of
0.0683705573	a diverse set
0.0683650597	the new algorithm
0.0683513439	still not
0.0683438231	first contribution
0.0682529643	a family
0.0682448909	search algorithm for
0.0681537735	fail to
0.0681071466	development of
0.0680771843	key role in
0.0680491623	choice of
0.0679977828	empirical study of
0.0679291363	more discriminative
0.0679252428	to increase
0.0679140343	competitive with
0.0678538099	running time of
0.0678196021	direction method of
0.0678187760	techniques such as
0.0678174253	main goal of
0.0678161974	an intermediate
0.0677811424	recently become
0.0677477281	to monitor
0.0676776346	introduced by
0.0676601049	advantages over
0.0676173216	most appropriate
0.0675524132	domains such as
0.0675344198	asequence of
0.0674917978	clustering based on
0.0674832180	a simple yet
0.0674814356	paper aims at
0.0674752025	allowsus to
0.0674717376	effectiveness of
0.0672927053	two types of
0.0672550121	technique for
0.0672437969	thedistribution of
0.0672337929	compared with other
0.0671597846	many machine learning
0.0671478874	applicability of
0.0671066111	properties such as
0.0670772142	a novel model
0.0670746175	order to overcome
0.0670263301	offered by
0.0670151163	seen as
0.0670145025	search over
0.0669334666	perform well on
0.0669303192	encoded in
0.0669065627	used to perform
0.0668752849	a largenumber of
0.0667795622	a convolutional neural
0.0667669797	a modification
0.0666992410	the efficacy of
0.0666909241	required to
0.0666050997	controlled by
0.0665749539	driven by
0.0665731512	an approach based
0.0664769557	the number of samples
0.0664346270	unlike most
0.0663391992	over thestate of
0.0663092518	formalism for
0.0661881255	a key
0.0661399389	significantly more
0.0661354014	a custom
0.0661274288	very difficult
0.0661203991	problem of
0.0660906492	the value function
0.0660367366	system achieves
0.0660177671	to drive
0.0659867765	the performance
0.0659756703	the expressive power
0.0659440869	step towards
0.0657911191	a major
0.0657385785	various domains
0.0657194451	does not rely on
0.0657131013	and real data sets
0.0656302687	detailed analysis of
0.0654828508	in contrast to
0.0654804773	a wide class
0.0653555287	well in practice
0.0653349415	a linear
0.0653278499	faced with
0.0652932825	a fundamental
0.0652426141	the expected
0.0651960014	learned by
0.0651770860	a recurrent neural
0.0651727626	further improvement
0.0650933757	learning approach to
0.0650836436	the art methods on
0.0649877991	mapped to
0.0649453646	only if
0.0649452324	apart from
0.0647979423	a quantum
0.0647936828	different categories
0.0647477281	to realize
0.0646948734	the most efficient
0.0646017634	to support
0.0645228679	a multitude of
0.0645091936	to make
0.0644599546	obtained via
0.0644409833	amount of information
0.0643865377	used to classify
0.0643152873	inorder to
0.0643102464	used to identify
0.0642572941	parameterized by
0.0642449283	appearing in
0.0642341201	used to detect
0.0642250482	bounded by
0.0641670271	range of
0.0641354344	searching for
0.0640933496	best known
0.0640603884	majority of
0.0640392658	expressed in
0.0640308264	efficient way
0.0640274478	involved in
0.0640255058	to manage
0.0639643748	computed from
0.0639444415	to further improve
0.0638407368	recent work on
0.0638212589	perform better
0.0638098447	the current state of
0.0638022168	a case
0.0637841388	measures such as
0.0637347845	very useful
0.0637199666	the goal
0.0637141019	foundation for
0.0637099503	to infinity
0.0636175693	to accommodate
0.0636082866	convolutional neural network for
0.0635903096	estimated from
0.0635702555	an accurate
0.0635573867	3d convolutional
0.0634711482	the number of nodes
0.0634459996	to match
0.0634451170	achieve better
0.0634256030	for solving
0.0633985457	a powerful
0.0633585769	used to learn
0.0633556566	the field of
0.0633428602	estimation from
0.0633290289	to gather
0.0631975545	favor of
0.0631613654	operate on
0.0631450373	an integral
0.0630973158	the time complexity
0.0630461812	equal to
0.0630461812	added to
0.0629650208	does not need
0.0629466886	inability to
0.0629202315	unable to
0.0628969347	currently available
0.0628937522	a first order
0.0628050370	much better
0.0628036428	a consequence
0.0627932741	crucial role in
0.0626618221	an abstract
0.0626450675	main contribution of
0.0626037287	any additional
0.0625812828	an external
0.0625447418	inference over
0.0625222092	art performance in
0.0625110598	gathered from
0.0624956687	feasibility of
0.0624883517	a given input
0.0624790409	the same object
0.0624711482	the number of classes
0.0624317549	does not depend on
0.0624252428	to define
0.0624204185	used to solve
0.0623939728	various kinds of
0.0622511790	the impact of
0.0622445884	various applications
0.0622437969	thecase of
0.0622117911	implemented as
0.0621940664	inclusion of
0.0621832420	improves over
0.0621781148	converted to
0.0621737881	a given image
0.0621378488	the need for
0.0620706872	faced by
0.0620558483	various aspects
0.0620389447	good generalization
0.0620333712	the existence of
0.0620157055	an instance
0.0620117073	annotated with
0.0618637385	a small set of
0.0618587602	each instance
0.0618058756	suite of
0.0617760712	to bridge
0.0617734733	to discriminate
0.0617494992	drop in
0.0616668830	a trade off between
0.0616507868	a variety of applications
0.0616409097	further research
0.0616277554	with regard to
0.0616090000	depth from
0.0616061435	systems such as
0.0615984012	the development of
0.0615975096	superior performance in
0.0615606639	the bag of words
0.0614622044	a mathematical
0.0614157469	this report
0.0613254237	as well
0.0612762580	structure from
0.0612120294	a compact
0.0612016663	a meta
0.0611941818	used to determine
0.0611597968	in accordance
0.0611500070	very efficient
0.0611265579	this gap
0.0610979159	to modify
0.0610112979	lies in
0.0609897663	builds on
0.0609343801	to deal
0.0608779562	mainly focus on
0.0607662928	a detailed
0.0607487380	to break
0.0607273982	in theform of
0.0607036512	simple yet
0.0606960177	efficiency of
0.0606926442	a dynamic
0.0606863627	acquired from
0.0606832375	the minimum
0.0606657677	comparable or
0.0606603273	by adopting
0.0606406996	more effectively
0.0606030579	needed to
0.0605500140	the ability of
0.0605357598	a variational
0.0604888725	achieved through
0.0604723157	a semi
0.0603705114	implemented by
0.0603368982	withrespect to
0.0603305757	large dataset of
0.0602690535	the most suitable
0.0602511790	the effect of
0.0602477281	to promote
0.0602345346	an operator
0.0602245111	to investigate
0.0601584667	often requires
0.0600960779	even more
0.0600881170	also discussed
0.0600760294	theneed for
0.0600563738	a mixture of
0.0600258057	in terms of accuracy
0.0600207289	to compare
0.0599718449	employed to
0.0599514987	to verify
0.0598547922	in terms
0.0598542157	theprobability of
0.0596448387	performance in terms of
0.0596438737	further improved
0.0595969420	a plethora of
0.0595115015	different regions
0.0594745162	very close to
0.0593590672	theprocess of
0.0593494687	very competitive
0.0593438134	attempted to
0.0592500384	the arts
0.0591821464	does not depend
0.0591626897	the presence
0.0591623622	to write
0.0591617660	several variants
0.0590614374	by presenting
0.0590563738	the success of
0.0590263301	imposed by
0.0590135420	in contrast
0.0590010657	large corpus of
0.0589948652	utilization of
0.0589907368	this phenomenon
0.0589044658	dedicated to
0.0588467293	a novel deep learning
0.0587937879	the case of
0.0587877962	start with
0.0587641041	a few
0.0587247954	a varietyof
0.0587170363	good results
0.0586137256	paid to
0.0586101371	handled by
0.0585729659	canbe used to
0.0585387261	a service
0.0585165546	same class
0.0584151441	processed by
0.0583960384	correlated with
0.0583600244	selection via
0.0583320385	an easy
0.0583036572	the first
0.0582499181	to apply
0.0582267993	an introduction to
0.0582158484	addressed by
0.0581756016	experiment with
0.0581544658	intended to
0.0581515221	the whole image
0.0581122066	theestimation of
0.0581054408	runs in
0.0580549078	previous work on
0.0580535261	against state of
0.0579977281	to enforce
0.0579908545	realized by
0.0579800260	communicate with
0.0579395237	the case
0.0579317136	a state of
0.0578926034	the main contribution of
0.0578620685	linked to
0.0578376427	a general class
0.0578334153	a novel two
0.0578086784	begin by
0.0577922500	a new approach to
0.0576918917	proposed method on
0.0576657164	the number of features
0.0576241664	referred to
0.0576174036	much research
0.0575561069	but rather
0.0575454193	an appropriate
0.0575368837	an equivalent
0.0574706534	metrics such as
0.0574217879	an upper
0.0573680162	by showing
0.0572410835	recognition based on
0.0572271330	the sum of
0.0572069460	proliferation of
0.0572042787	embedded in
0.0571692654	beneficial to
0.0571683712	obtained through
0.0571532043	in many fields
0.0571321658	required by
0.0571256439	a method based
0.0570942278	shared by
0.0570669285	a constraint
0.0570214439	algorithms in terms
0.0570195732	arise from
0.0570052058	existing work
0.0569959236	continue to
0.0569944684	case study on
0.0569743450	experiments on two
0.0569725420	superior performance of
0.0569248388	particular case
0.0569189073	the most accurate
0.0569032501	a number
0.0567642575	exponentially with
0.0567382001	wealso show
0.0566628581	to characterize
0.0566439036	used to derive
0.0564900883	used for training
0.0564231987	toolkit for
0.0564149618	by taking
0.0564118855	well suited to
0.0563807651	a new technique
0.0563740119	to measure
0.0563443184	amount of
0.0563220332	the sample complexity of
0.0563148372	designed for
0.0562465049	part of
0.0562380917	a crucial
0.0562284414	the interplay between
0.0562087116	a class of
0.0562043689	wealth of
0.0561319129	available online
0.0561172088	very hard
0.0560708440	to adjust
0.0560284327	some interesting
0.0560034066	our findings
0.0559994401	do not scale
0.0559686364	architecture for
0.0559147645	the art on
0.0559039375	fields such as
0.0558111646	accurate than
0.0557720640	approaches such as
0.0557394025	success of
0.0557222399	the goal of
0.0556420791	usefulness of
0.0556192214	reduced by
0.0556177623	the ability to
0.0555265509	requires only
0.0555215500	to play
0.0555198229	encoded as
0.0554770676	the viability of
0.0554675330	demonstrated by
0.0554660191	transferred to
0.0554140667	the relationship between
0.0554135862	to embed
0.0554084621	differ from
0.0553896732	important task in
0.0553795051	the first algorithm
0.0553000456	to validate
0.0552926480	amount of data
0.0552843693	different kinds of
0.0552769082	inspired from
0.0552710792	captured from
0.0551587570	to cope with
0.0551556099	information such as
0.0551032015	used as
0.0550800134	beneficial for
0.0550076304	efficacy of
0.0549903537	to differentiate
0.0549715388	a variety of tasks
0.0549620702	the art performance in
0.0549017479	understood as
0.0548613152	arise in
0.0548319479	couple of
0.0547537447	to deploy
0.0547354755	more efficient than
0.0547346845	the main goal
0.0546233499	while providing
0.0546216097	performance across
0.0546187916	the efficiency of
0.0546166155	explained by
0.0545948073	the new method
0.0545748268	mainly due
0.0545180145	two decades
0.0544972423	computational model of
0.0544478199	a broad class of
0.0544336656	learnt from
0.0543491418	to decide
0.0542938419	to tune
0.0542713089	other domains
0.0542689964	a range of
0.0542661519	included in
0.0542477281	to uncover
0.0541975545	theoutput of
0.0541949967	different sizes
0.0541895357	superiority of
0.0541560221	a large variety of
0.0541553482	part ofthe
0.0541447023	by treating
0.0541106704	thetask of
0.0541010715	an excellent
0.0540713103	best result
0.0540682409	to simulate
0.0540665920	seek to
0.0540177205	used for classification
0.0540000424	to leverage
0.0539650939	acquired by
0.0538972949	areable to
0.0538322849	the recent success
0.0538266736	a framework for
0.0537928818	occurs in
0.0537752373	to derive
0.0537332887	thefield of
0.0537204248	than thestate of
0.0537131217	want to
0.0536739572	an array
0.0536626075	order to
0.0536560221	a finite number of
0.0535255058	to meet
0.0534887295	a greedy
0.0534334743	perform well in
0.0534173848	selected from
0.0533085694	to reason about
0.0531811523	a large set of
0.0531552519	query by
0.0531418698	often leads
0.0531393700	of magnitude
0.0531089093	together with
0.0530902997	prior work on
0.0530611827	to run
0.0530476459	each other
0.0530304406	a convex
0.0530044513	a method for
0.0529907364	order to evaluate
0.0529338812	rise to
0.0529183537	anextension of
0.0529094599	begins with
0.0528867410	the superiority of
0.0528599750	bound for
0.0528588392	to deliver
0.0528494865	different parts of
0.0527927378	to express
0.0527841874	to generalize
0.0527584772	to enrich
0.0527477281	to mimic
0.0527155382	to compensate for
0.0526989332	to combat
0.0526771764	in thisarticle
0.0526308413	the emergence of
0.0525833796	new formulation
0.0525615298	comes at
0.0525327244	to propagate
0.0525291953	an auxiliary
0.0524968773	validated on
0.0524843404	illustrated by
0.0524200935	a method for learning
0.0523846960	interacting with
0.0522641746	the validity of
0.0522345802	a given
0.0522335768	used to model
0.0522279467	used in
0.0521615973	in order to evaluate
0.0521446171	the choice of
0.0521421225	union of
0.0521289251	needed for
0.0521164882	a coarse
0.0520763825	the number of iterations
0.0520564591	important problem in
0.0520559953	selected by
0.0520468428	different levels of
0.0520384014	a trade off
0.0520271928	optimized by
0.0519681669	integrated with
0.0519530232	any time
0.0518549885	lie on
0.0518208214	an analysis of
0.0518149470	used for
0.0518002382	while simultaneously
0.0517928743	each variable
0.0517611105	to segment
0.0517477281	to convert
0.0517093940	a broad range of
0.0516900650	limited by
0.0516871991	a small amount
0.0516791373	posed as
0.0516780579	a large collection of
0.0516256153	aim to
0.0516211480	a wide
0.0516073510	methods in terms of
0.0515943555	a large class of
0.0514991259	evaluated by
0.0514828508	a lot of
0.0514728594	an optimization
0.0514699503	to compress
0.0514698313	extensiveexperiments on
0.0514134106	made available
0.0514003025	more likely
0.0513211152	a distributed
0.0512481368	unified framework for
0.0512468638	submission to
0.0512359135	different forms
0.0511675019	expressed by
0.0511588038	central role in
0.0511362887	experimental results on two
0.0511176394	for example
0.0511121899	a new task
0.0510947113	the most effective
0.0510769684	account for
0.0510381060	a small subset of
0.0510206045	coarse to
0.0509967440	very good
0.0509884135	a large amount
0.0509226154	to cover
0.0508867410	a collection of
0.0507856501	a wide class of
0.0507843990	fundamental problem in
0.0507667736	mainly due to
0.0507423888	calculated by
0.0506923883	operates on
0.0506866377	the best reported
0.0506661821	encoded by
0.0506283837	an approach to
0.0505994667	light on
0.0505363311	crucial for
0.0504741486	deployed in
0.0503662402	to approximate
0.0503496771	the possibility of
0.0503330938	comes from
0.0503311854	a subset of
0.0503190778	opportunity to
0.0502844160	conclude by
0.0502745889	over state of
0.0502084719	this setting
0.0501955613	to highlight
0.0501885975	the computational complexity of
0.0501862399	arises from
0.0501057182	improved by
0.0500480865	many state of
0.0499655753	this work wepropose
0.0499391992	the most widely used
0.0499010031	the form of
0.0498365320	validated by
0.0498350243	demonstrated on
0.0497659047	able to improve
0.0497458297	conclude with
0.0497245576	an internal
0.0496975607	three different
0.0496490947	the complexity of
0.0496362329	network model for
0.0496304845	to illustrate
0.0496189302	heavily on
0.0496188955	encountered in
0.0495463949	linearly with
0.0495297963	most state of
0.0495215390	not sufficient
0.0494652217	a special
0.0494644076	to utilize
0.0493724476	by fusing
0.0493328588	constrained by
0.0493273168	full use of
0.0492210467	most similar
0.0492068143	new measure
0.0491536726	the currentstate of
0.0491040709	need to
0.0490744343	field of
0.0490742265	favorably with
0.0490556384	posed by
0.0490474560	most important
0.0490279364	imposed on
0.0489967298	the most recent
0.0489383660	the possibility
0.0489273557	experiments on three
0.0488768083	a linear combination
0.0487552591	holds for
0.0487001098	new technique
0.0486708990	regard to
0.0486568925	different aspects
0.0486218339	to share
0.0485996957	in doing so
0.0485892472	by replacing
0.0485357521	this kind
0.0484820159	simple but
0.0484496771	the feasibility of
0.0484452531	different settings
0.0484211742	able to provide
0.0483613867	three types
0.0483414731	to connect
0.0483284454	order to solve
0.0482562207	more practical
0.0482228222	many situations
0.0481948422	agreement with
0.0481401502	than others
0.0481105520	the overall performance
0.0480175904	justification for
0.0479624303	a new framework
0.0479510564	the existence
0.0479389037	performance than
0.0479080023	to answer
0.0478639958	areas such as
0.0478495455	tested with
0.0478249563	the previous state of
0.0478105896	lie in
0.0478090389	challenging problem in
0.0477293522	the challenging problem of
0.0477261087	far from
0.0476480653	the notion of
0.0476455058	the first method
0.0476385550	an overview of
0.0475766839	a family of
0.0475681351	new state of
0.0475504018	both synthetic
0.0475313871	the bag of
0.0474618548	a finite
0.0474611941	required for
0.0474301842	10 dataset
0.0473984721	termed as
0.0473208538	start from
0.0472498616	the most likely
0.0472230308	different scenarios
0.0472163008	starts with
0.0471931665	ispossible to
0.0470849868	does not rely
0.0470644131	by extending
0.0470096913	augmented with
0.0469322860	second step
0.0468930368	the expected value
0.0468875742	to yield
0.0468688145	to employ
0.0468332221	two separate
0.0468329037	away from
0.0467506543	the last few
0.0467409702	in many real world
0.0466008431	a novel algorithm
0.0465627711	different strategies
0.0465155959	a new state of
0.0464122616	the first approach
0.0463711140	the most
0.0463611152	a multi
0.0463147769	experiments on several
0.0462800794	the efficacy
0.0462086997	left to
0.0461733441	in order to make
0.0460814742	held in
0.0460383161	performance against
0.0460349014	the application of
0.0460104987	a challenging
0.0459279861	extracted by
0.0459149324	the beginning of
0.0459076875	amount of time
0.0458585584	proposed approach on
0.0458146701	the proposed system
0.0457956124	with other state of
0.0457505985	identified as
0.0457430388	large amount
0.0457397632	origin of
0.0457155382	two kinds of
0.0457097435	the past few
0.0456962831	not yet
0.0456954642	enhanced by
0.0456762706	challenging task in
0.0456656301	overall performance
0.0456381829	a simplified
0.0456358085	to replace
0.0454640855	the new approach
0.0454430369	enabled by
0.0454338243	to bring
0.0454190820	a low
0.0452902023	a multimodal
0.0452830307	to add
0.0452708722	more easily
0.0451752124	the analysis of
0.0451619099	a broad
0.0450029081	a polynomial
0.0449736436	the art by
0.0449639884	show experimentally
0.0449620702	the art results in
0.0448814889	beused to
0.0448248507	more suitable
0.0448241774	a proxy
0.0447068697	to remedy
0.0446921725	to align
0.0446824110	the expense of
0.0446660403	an attempt
0.0445766839	a combination of
0.0445296724	amethod for
0.0444618309	to extend
0.0444570294	therepresentation of
0.0444542080	the absence
0.0444427593	to satisfy
0.0444363221	in addition to
0.0444299553	present here
0.0444125467	analgorithm for
0.0444018515	room for
0.0443390804	several examples
0.0443389005	comparison with
0.0442738248	a bag
0.0442481555	helpful for
0.0442341396	various techniques
0.0442080896	collected by
0.0441950164	by examining
0.0441029641	relate to
0.0440819512	more recently
0.0440422561	the creation of
0.0439470334	commonly used in
0.0439383473	several advantages
0.0439178507	to fit
0.0439143947	to decompose
0.0438980432	to succeed
0.0438768441	differ in
0.0438620138	a viable
0.0438393209	also provided
0.0437402764	show empirically
0.0435449577	known as
0.0435372601	an important part
0.0435332056	the usefulness of
0.0435076693	in order to provide
0.0434928618	the notion
0.0434699503	to reproduce
0.0434186310	the amount of data
0.0433988967	to attain
0.0433677798	the rest of
0.0431978075	tailored for
0.0431717226	the suitability of
0.0431709812	very close
0.0431542644	possibility of
0.0431540459	a new type of
0.0430278522	artperformance on
0.0429493940	various types of
0.0429095662	the relation between
0.0429024747	failed to
0.0428952154	a closed
0.0428816113	likely to
0.0428348423	than state of
0.0428167382	independently from
0.0427464811	on synthetic and real
0.0427149052	the effect
0.0426790990	the amount of
0.0425863049	to use
0.0424938869	little work
0.0424424324	to store
0.0423616425	impossible to
0.0423534015	aframework for
0.0423373718	come from
0.0422820752	best reported
0.0422399304	to establish
0.0422143542	to observe
0.0422032015	also known as
0.0421953080	not require
0.0421868960	last few
0.0421560221	a new family of
0.0421235239	taking into
0.0420891169	made publicly
0.0420525611	very well
0.0419590688	a coarse to
0.0419352156	other areas
0.0418465393	grounded in
0.0418347336	to fill
0.0417668784	to aggregate
0.0416907437	new metric
0.0416303680	a recurrent
0.0415817528	advent of
0.0415797649	a subset
0.0415659545	a long time
0.0415563642	many domains
0.0415069626	the previous state
0.0414355190	a sequential
0.0413880131	a variant of
0.0413178507	to guarantee
0.0412291743	past few
0.0412276482	the well known
0.0412021549	to achieve state of
0.0411996792	scales well
0.0411914421	more natural
0.0411146560	a smart
0.0410929392	learn about
0.0410714893	this paper focuses
0.0410424480	the last
0.0410298681	an important role in
0.0409835270	beused for
0.0409000939	to get
0.0408325742	all possible
0.0407727483	derived by
0.0407509337	while still
0.0406178766	to augment
0.0405931212	compensate for
0.0405594118	not directly
0.0405586413	a new class of
0.0404999737	the lack of
0.0404699144	the first one
0.0404503803	by observing
0.0404019451	the generalized
0.0403638305	the ability
0.0403434827	assist in
0.0403274071	an expressive
0.0402007440	as part of
0.0401896320	a diverse set of
0.0401578063	a well established
0.0401246807	an extra
0.0400968075	to merge
0.0400954057	theability to
0.0400935221	a limited number of
0.0399404343	useful tool
0.0399276958	the union of
0.0399199048	recovery from
0.0397850567	par with
0.0397792374	to addressthis
0.0397458759	the right
0.0397064299	multivariate time
0.0396960186	not fully
0.0396447641	the impact
0.0396411344	an improvement
0.0396381829	to carry
0.0396283837	a novel approach to
0.0396243918	new framework
0.0395972480	natural way
0.0395930016	beginning of
0.0395361637	used in practice
0.0394901595	the effectiveness
0.0394638305	a benchmark
0.0394290221	the same number
0.0394234881	a novel way
0.0393304942	to boost
0.0392139618	a corpus of
0.0391896442	this paper deals
0.0391054716	two types
0.0390917792	generality of
0.0390134543	other words
0.0389080750	to cope
0.0388704823	to gain
0.0388185068	best possible
0.0386898692	with thestate of
0.0386834804	to analyse
0.0385534169	an attempt to
0.0385332118	utilized to
0.0385144916	small amount
0.0383721357	a simple but
0.0383594924	the usefulness
0.0382164364	different kinds
0.0382031289	many computer vision
0.0380863882	taken from
0.0380476758	even without
0.0379911727	to draw
0.0379608536	better understanding
0.0378370494	contrast to
0.0377657610	done by
0.0377473534	effective way
0.0377233847	the total number of
0.0377032614	the curse of
0.0376855916	a connection
0.0376001185	effective way to
0.0375916326	on bothsynthetic and
0.0375757243	to balance
0.0375157444	a union of
0.0374924907	a novel approach for
0.0374734572	thecontext of
0.0374681462	five different
0.0373889108	at once
0.0373498129	the existing state of
0.0373282181	predicted by
0.0372758478	in many practical
0.0372464863	the class of
0.0372130479	to keep
0.0371838864	by considering
0.0371381116	a new algorithm for
0.0371211052	the combination of
0.0370485749	to account for
0.0369932997	corresponding to
0.0369911727	to evolve
0.0369892940	to aid
0.0369402465	the first work
0.0369398591	a novel method for
0.0368723767	this kind of
0.0368644784	approach does not
0.0368347336	seem to
0.0368244207	to follow
0.0366772457	the area under
0.0366734036	this type of
0.0366525452	a given set of
0.0366283837	a new method for
0.0365328006	all previous
0.0365037166	many applications such as
0.0364834309	several popular
0.0364772171	method against
0.0364690612	each pair of
0.0364553779	a given set
0.0364206828	four different
0.0364035441	a tool
0.0364024439	our results show
0.0363242352	an implementation of
0.0363197192	in comparison with
0.0362514517	an ensemble of
0.0361697621	more traditional
0.0361069132	the best possible
0.0360766603	described by
0.0359870911	to rank
0.0359813317	a methodology
0.0359727466	the main goal of
0.0359691348	a slight
0.0359427908	various types
0.0358470457	other hand
0.0358095480	an inherent
0.0358056077	the emergence
0.0357976116	the first to
0.0357815180	by removing
0.0357164880	most prominent
0.0356567360	previous work in
0.0356211189	to constrain
0.0356011724	more important
0.0355799748	owing to
0.0355435239	some ofthe
0.0355332118	hold for
0.0355287474	many practical
0.0355239061	the applicability of
0.0354460222	certain conditions
0.0353522962	the availability of
0.0352915084	any given
0.0352500273	the role
0.0352246475	to associate
0.0351371303	a bag of
0.0351355341	a collection
0.0351194054	most widely used
0.0351168995	to take advantage
0.0350220706	in thepresence of
0.0350199611	viability of
0.0349755382	a crucial role in
0.0349622625	used to find
0.0349308562	a general framework for
0.0348736440	a finite set of
0.0348573879	an algorithm based
0.0348099494	three types of
0.0348040801	in many real
0.0348035162	to reveal
0.0346979299	new family
0.0346251035	a new approach for
0.0344755022	to simplify
0.0344309058	this paper deals with
0.0344144270	an extension to
0.0343624614	a proof of
0.0343524217	most successful
0.0343274371	try to
0.0343177822	to act
0.0341678092	to formulate
0.0341367205	run on
0.0341349319	in contrast to previous
0.0341139850	by taking into
0.0341089107	to separate
0.0340424052	any prior
0.0340246661	at most
0.0340016925	known about
0.0339554857	to conduct
0.0338857939	occur in
0.0338789292	in order to find
0.0338745157	an adaptation of
0.0338699423	an algorithm for
0.0338179406	same object
0.0337555932	lot of
0.0337362692	by deriving
0.0337122980	the projected
0.0336330113	a well defined
0.0336273336	none of
0.0336238371	not explicitly
0.0335531881	the definition of
0.0335428257	the superiority
0.0335114385	or even
0.0335025292	to induce
0.0334873739	to ease
0.0334516646	the era of
0.0334511814	an application to
0.0333842609	to parse
0.0333770362	the feasibility
0.0333516254	some specific
0.0333512881	important part of
0.0333001281	built using
0.0332741954	the advent
0.0332367006	of magnitude faster than
0.0332213964	use of
0.0332034030	the viability
0.0331169334	to seek
0.0331103543	in particular
0.0330592226	a small amount of
0.0330121554	to operate
0.0330096610	incontrast to
0.0329755382	a key role in
0.0327446147	the similarity between
0.0327412892	methods do not
0.0327309431	recent work in
0.0325326015	to dealwith
0.0324374978	the alternating direction method of
0.0323580841	an implementation
0.0323132366	the input to
0.0322445707	used to describe
0.0321715213	most accurate
0.0321382719	our experiments show
0.0320792868	to mine
0.0320782100	any other
0.0320594848	then used to
0.0319550420	the number of training
0.0319304949	to decrease
0.0319198548	the nature of
0.0318851480	most recent
0.0318646932	the connection between
0.0318558515	a model for
0.0318325647	to summarize
0.0318287687	work focuses
0.0317648009	to hold
0.0317405423	a list
0.0316853066	the currentstate
0.0316751149	available datasets
0.0316593940	to do so
0.0315284150	a comparison of
0.0315265464	correctness of
0.0315238279	a discussion
0.0315206258	the majority of
0.0315133785	limited amount
0.0315019930	available at
0.0314634285	leveraged to
0.0314598432	the first two
0.0314383618	the most widely
0.0313929096	this perspective
0.0312506318	most commonly
0.0312394132	the expressive power of
0.0311394389	the scope
0.0311070129	the degree of
0.0310979810	to check
0.0310691635	quite different
0.0310427426	far more
0.0310026262	to converge to
0.0308791017	a special case of
0.0308745157	a high degree of
0.0308521045	the same as
0.0308127466	the necessity of
0.0307808871	most suitable
0.0306859970	the literature on
0.0306573936	takes into
0.0306522274	each pair
0.0306225830	to treat
0.0305965432	the success of deep
0.0305569957	suitability of
0.0305238279	a tight
0.0305112536	the scope of
0.0304616847	work addresses
0.0304188616	to devise
0.0304051875	a linear combination of
0.0303734057	our results indicate
0.0303598797	validity of
0.0302728415	the most commonly
0.0302501413	able to find
0.0301709058	this paper focuses on
0.0301681638	a central role in
0.0301304051	a new way
0.0300856979	work proposes
0.0300635148	adopted to
0.0300038130	a large amount of
0.0299887641	off between
0.0299840425	on bothsynthetic
0.0299836953	a toy
0.0299496393	the problem as
0.0299343953	an application of
0.0298558515	the need to
0.0298558515	the implementation of
0.0298088363	a new class
0.0298087574	a connection between
0.0297785209	a better understanding of
0.0297029462	the collection of
0.0296682192	the correlation between
0.0296203072	becomes more
0.0295795054	more appropriate
0.0295748160	the gap between
0.0295321379	the same way
0.0295167766	a notion of
0.0294753623	the large number of
0.0294713488	to specify
0.0293653059	first step
0.0293640693	a branch
0.0293362972	not hold
0.0293279040	the last two
0.0293256448	a problem of
0.0293042558	a novel framework for
0.0292996605	the beginning
0.0291729601	to ask
0.0291458973	a thorough
0.0291207279	a step towards
0.0290465515	to interact
0.0290220706	a modification of
0.0289987712	a case study in
0.0289961939	the advantage of
0.0288983043	an adaptation
0.0288429012	to take advantage of
0.0288099494	a fundamental problem in
0.0287775418	the absence of
0.0287395280	method does not
0.0287394132	an empirical evaluation of
0.0287394010	this approach to
0.0286444398	an approach for
0.0286140831	than existing
0.0285844299	model does not
0.0285524297	the end to
0.0284718029	a case study on
0.0284584750	becoming more
0.0284393427	to noise
0.0284384085	taken by
0.0283386790	any existing
0.0282523058	more than one
0.0281877026	the distance between
0.0280397530	new paradigm
0.0278942141	the problem into
0.0278916626	in termsof
0.0278824612	or better than
0.0278471507	to adapt to
0.0277826891	to compensate
0.0277502026	most previous
0.0276877604	a natural way
0.0276298562	the experimental results show
0.0275970486	theform of
0.0275411824	a suite of
0.0274677239	to give
0.0274583068	a bi
0.0274398432	the case for
0.0274325052	a method based on
0.0273909820	a fraction of
0.0273831501	an alternative to
0.0272523928	to impose
0.0272307286	to advance
0.0271610936	the human visual system
0.0268854754	most current
0.0268644818	an undirected
0.0268623945	as measured by
0.0268400044	to reflect
0.0268015186	in contrast with
0.0267978643	an improvement of
0.0267821316	an easy to
0.0267441860	to justify
0.0266610012	fill in
0.0265689049	no prior
0.0264636701	both theoretically and
0.0264359504	not seen
0.0263410937	a powerful tool for
0.0263345132	the problem in
0.0263146416	exploited to
0.0263126030	era of
0.0262575353	to interact with
0.0262364205	calls for
0.0261574978	to outperform
0.0261123664	theory and practice of
0.0260967194	a consequence of
0.0260534156	need for
0.0260211799	this problem as
0.0260131787	to belong
0.0259972340	the range of
0.0259839376	unlikely to
0.0259681638	this gap by
0.0258989578	time algorithm for
0.0258852633	the type of
0.0258733811	the problem by
0.0257730035	the difference between
0.0257682523	a comparative study of
0.0256719198	to serve
0.0256193945	most cases
0.0255514971	more suitable for
0.0255149153	novel technique
0.0254812248	a new technique for
0.0254071907	new type of
0.0253796717	to perform well
0.0253795516	to help
0.0253663039	comes with
0.0251678465	the proposed method on
0.0250596519	solved using
0.0250175457	expense of
0.0249293411	an evaluation of
0.0248607120	does not use
0.0248120876	well as
0.0247805472	areused to
0.0247717457	a mapping from
0.0247547296	the integration of
0.0247292553	to grow
0.0246598379	work aims
0.0244832352	necessity of
0.0244080389	a variant
0.0244039957	same time
0.0243927272	a form of
0.0243918569	improve over
0.0243831501	more robust to
0.0243497103	a novel algorithm for
0.0243493000	to move
0.0242837409	the best known
0.0242827109	a bound on
0.0242290754	the correctness of
0.0241747141	the superior performance of
0.0241738757	most widely
0.0241242762	soon as
0.0240787026	a lack of
0.0240304745	the generality of
0.0239714561	many fields
0.0239698649	an approach based on
0.0239088564	a general class of
0.0237961096	each type
0.0237949293	this paperintroduces
0.0237912286	effective than
0.0237295327	this class of
0.0236677603	to generalize to
0.0235760252	conference on
0.0235158044	to make use of
0.0233259955	many cases
0.0233180510	possible to
0.0232882997	to know
0.0232852459	an important problem in
0.0232346647	first attempt to
0.0232112823	to allow for
0.0232050171	a list of
0.0231954933	a sum of
0.0231515633	a review of
0.0231144486	as compared to
0.0230680031	the incorporation of
0.0230647288	each other in
0.0230004522	the determination of
0.0229855549	a generative model of
0.0229281147	made by
0.0229163417	wish to
0.0228683142	a type of
0.0228246796	this problem by
0.0227941236	to allow
0.0227458846	thepurpose of
0.0227447187	an efficient algorithm for
0.0227308593	new approach to
0.0227138157	a tool for
0.0226046132	a vital
0.0225726688	work presents
0.0225238544	most effective
0.0225229032	most significant
0.0224477819	a kind of
0.0224477819	a technique for
0.0224071907	new class of
0.0223943415	do not use
0.0223678159	the inclusion of
0.0223533811	the literature in
0.0223288542	a challenging task in
0.0223188882	the family of
0.0222816377	an example of
0.0220996495	to focus on
0.0220270135	an improvement in
0.0220220995	an important task in
0.0219716619	a new family
0.0219081124	a distribution over
0.0218587538	a unified framework for
0.0217812060	placed on
0.0217074458	the number of data
0.0216862912	more than two
0.0216592680	a part of
0.0216385619	specified by
0.0216247862	for dealing with
0.0215922347	the computational cost of
0.0215874104	this issue by
0.0215563614	the possibility to
0.0215370713	not able to
0.0215170713	a number of different
0.0214832352	beenproposed to
0.0214784428	a large dataset of
0.0214341214	a directed
0.0214185508	more often
0.0213533811	the case in
0.0213377561	the extension of
0.0213340610	to take into
0.0212304147	the best
0.0212176363	an algorithm based on
0.0211081094	a discussion of
0.0210996495	to learn from
0.0210029674	to shed
0.0209773704	for example in
0.0209255680	a need for
0.0209090059	both simulated and
0.0208482542	a bridge
0.0208383504	a methodology for
0.0207394010	a version of
0.0207110271	experiments on synthetic and
0.0206959049	taken into
0.0206718828	the interaction between
0.0205443182	to solvethe
0.0205386233	most general
0.0205162383	as well as on
0.0205091441	many applications such
0.0204023274	our experiments on
0.0203113103	a challenging problem in
0.0202634580	a bottom
0.0201384124	an instance of
0.0200790998	a new way to
0.0199877035	better understanding of
0.0199217256	tried to
0.0198175641	a new method to
0.0197753158	to take
0.0196005769	for future work
0.0195324892	the same number of
0.0195139043	new framework for
0.0194584446	change over
0.0192634019	the effectiveness and efficiency
0.0191877636	new perspective
0.0191859970	a new dataset of
0.0190890132	along with
0.0189807720	many areas
0.0189773704	in particular for
0.0188820609	an end
0.0187499636	the expense
0.0187010936	a theoretical analysis of
0.0186882490	the large amount
0.0186414380	both simulated
0.0184426445	both theoretical
0.0183710472	new approach for
0.0183479924	both synthetic and
0.0181718516	conjunction with
0.0181289851	often used to
0.0179941909	even better
0.0178972734	first attempt
0.0176342468	handful of
0.0175075077	new way to
0.0171567614	new method for
0.0171074458	widely used to
0.0170917662	at least as
0.0169604212	a novel method to
0.0169488780	our experimental results show
0.0168376085	this work provides
0.0168288542	an average of
0.0168265352	a benchmark for
0.0167990200	to account
0.0166846721	novel approach to
0.0165529374	an expectation
0.0163574187	then applied to
0.0161315690	new family of
0.0156857950	isused to
0.0156778639	any number
0.0156757237	most promising
0.0155234281	given set of
0.0155225305	this paper provides
0.0153659894	however due to
0.0153341214	to reason
0.0150388332	to occur
0.0149962383	to rely on
0.0149952956	order to make
0.0149401002	to lead
0.0148405710	first part of
0.0137231117	first step in
0.0136720284	novel method for
0.0136629049	the experimental results on
0.0136479264	this paperproposes
0.0136175419	in many computer
0.0130023003	order to find
0.0121265739	novel framework for
0.0120358757	on three different
0.0118968918	to improvethe
0.0103461106	same number of
0.0095592452	novel approach for
